[
  {
    "objectID": "analysis3_1_dates.html",
    "href": "analysis3_1_dates.html",
    "title": "Dates & Times",
    "section": "",
    "text": "Show code\nlibrary(tidyverse)\nlibrary(reshape2)\nlibrary(reactable)\nlibrary(nycflights13)\n\npalette1 <- c(\"#648fff\",\"#785ef0\",\"#dc267f\",\"#fe6100\",\"#ffb000\",\"#000000\",\"#ffffff\")\npalette2 <- c(\"#CC79A7\",\"#D55E00\",\"#0072B2\",\"#F0E442\",\"#009E73\",\"#56B4E9\",\"#E69F00\",\"#000000\")"
  },
  {
    "objectID": "analysis3_1_dates.html#datumsobjekte-und-zeiten",
    "href": "analysis3_1_dates.html#datumsobjekte-und-zeiten",
    "title": "Dates & Times",
    "section": "Datumsobjekte und Zeiten",
    "text": "Datumsobjekte und Zeiten\nNatürlich stellt sich zunächst die Frage, was Datumsobjekte sind und wie sie sich von anderen Datentypen unterscheiden. Neben character, numeric oder factor gibt es auch Date & POSIXct. Die Funktionen today() und now() können uns das aktuelle Datum bzw. die Zeit geben und demonstrieren uns dabei die beiden Datentypen:\n\n# Date:\ntoday()\n\n[1] \"2023-05-16\"\n\n# PosixCT:\nnow()\n\n[1] \"2023-05-16 16:26:21 CEST\"\n\n\n\nFrage: Wofür steht CEST?\n\nCEST ist eine Zeitzone - Zeitzonen spielen bei Datumsobjekten eine Rolle, aber dazu später mehr.\n\nDate\nIhr habt bereits die Funktion as.Date() verwendet. Damit könnt ihr einen String in ein Datumsobjekt umwandeln, welches dann das Datum enthält:\n\nas.Date(\"1969-07-20\")\n\n[1] \"1969-07-20\"\n\n\n\n\nPOSIXct\nWenn wir jetzt auch die Uhrzeit abspeichern wollten, müssten wir ein POSIXct Objekt erzeugen, da R keinen eigenen Objekttyp für Zeiten hat:\n\nas.POSIXct(\"1969-07-20 20:17:40\")\n\n[1] \"1969-07-20 20:17:40 CET\"\n\n\nSuper! Wir haben Datumsobjekte kennengelernt und erfolgreich angewendet. Jetzt stellt sich nur eine Frage:\n\nWelchen Vorteil haben Datumsobjekte? Wofür brauchen wir so etwas?\n\nIm Gegensatz zu Zeiten, die bspw. als character gespeichert sind, können wir mit Datumsobjekten eine Vielzahl an Dingen machen. So können wir bspw. Zeiträume berechnen, Tage/Monate oder Wochentage extrahieren, und ebenso Zeitzonen umrechnen. Großartig!\nOb das ganze wirklich so einfach wie gedacht ist, und wann lubridate ins Spiel kommt, das lernen wir jetzt."
  },
  {
    "objectID": "analysis3_1_dates.html#datumsformate",
    "href": "analysis3_1_dates.html#datumsformate",
    "title": "Dates & Times",
    "section": "Datumsformate",
    "text": "Datumsformate\nFangen wir also damit an, den angepassten flights Datensatz aus dem Moodle zu importieren:\n\ndataset <- read.csv(\"assets/datasets/flights.csv\")\n\nZunächst wollen wir eine Idee davon bekommen:\n\nhead(dataset)\n\n\n\n  \n\n\n\n\nFrage: Welche sind die für uns relevanten, zeitbezogenen Spalten? Was für Probleme könnte es geben?\n\n\ndataset %>% \n  mutate(departure = as.Date(departure))\n\nR schmeißt uns hier den Fehler:\n\ncharacter string is not in a standard unambiguous format\nFrage: Wie könnten wir diesen Fehler lösen?\n\nGenau, erstens handelt es sich nicht nur um ein Datum, sondern um ein Datum mit Zeit, weshalb sich hier POSIXct besser eignet. Damit R aber unser Datumsobjekt auch als solches versteht, müssen wir auch das entsprechende Format des Datumsobjekts klarmachen. Wie auf dem Titelbild oben zu sehen, gibt es eine Vielzahl verschiedener Möglichkeiten, Daten aufzuschreiben, etwa:\n\n\"1969/07/20 20:17:40\"\n\"1969-07-20 20 Uhr\"\n\"20.7.1969 20:17:40\"\n\"20.07.1969 20:17:40\"\n\"20. Juli 1969 um 20:17:40\"\netc.\n\n\nCodes\nDas heißt, wir müssen irgendwie das Format definieren, in dem das Datum “geparsed” werden soll. Dazu gibt es verschiedene Codes. So können wir etwa über %d das Datum definieren, oder über %m den Monat. Hier eine (unvollständige) Liste der Codes, übernommen von hier:\n\n\n\nType\nCode\nMeaning\nExample\n\n\n\n\nYear\n%Y\n4 digit year\n2021\n\n\n\n%y\n2 digit year\n21\n\n\nMonth\n%m\nNumber\n2\n\n\n\n%b\nAbbreviated name\nFeb\n\n\n\n%B\nFull name\nFebruary\n\n\nDay\n%d\nTwo digits\n02\n\n\n\n%e\nOne or two digits\n2\n\n\nTime\n%H\n24-hour hour\n13\n\n\n\n%I\n12-hour hour\n1\n\n\n\n%p\nAM/PM\npm\n\n\n\n%M\nMinutes\n35\n\n\n\n%S\nSeconds\n45\n\n\n\n%Z\nTime zone name\nAmerica/Chicago\n\n\n\n%z\nOffset from UTC\n+0800\n\n\n\nWir können nun aus diesen Codes das Datum parsen. Manchmal enthalten Daten noch zusätzliche Zeichen, wie etwa / oder :. Diese können wir einfach kombinieren mit den Codes:\n\nas.POSIXct(\"69/07/20-20:17:40\", format = \"%y/%m/%d-%H:%M:%S\")\n\n[1] \"1969-07-20 20:17:40 CET\"\n\n\n\nFrage: Was fällt auf?\n\nWie wir sehen können, haben wir den String \"69/07/20-20:17:40\" erfolgreich “geparsed”. Das hat nur funktioniert, weil wir das Format über format = \"%y/%m/%d-%H:%M:%S\" definiert haben."
  },
  {
    "objectID": "analysis3_1_dates.html#flights-datensatz",
    "href": "analysis3_1_dates.html#flights-datensatz",
    "title": "Dates & Times",
    "section": "Flights Datensatz",
    "text": "Flights Datensatz\nBetrachten wir nun also wieder unseren Datensatz dataset, und die darin enthaltenen Zeitspalten.\n\nWie können wir diese in ein richtiges Zeitobjekt überführen?\n\nAm besten wie oben, nur angepasst auf das neue Format:\n\ndataset %>% \n  mutate(departure = as.POSIXct(departure, format = \"%d.%m.%Y %H:%M:%S\"),\n         scheduled_departure = as.POSIXct(departure, format = \"%d.%m.%Y %H:%M:%S\"),\n         arrival = as.POSIXct(arrival, format = \"%d.%m.%Y %H:%M:%S\"),\n         scheduled_arrival = as.POSIXct(scheduled_arrival, format = \"%d.%m.%Y %H:%M:%S\")) %>% \n  head()\n\n\n\n  \n\n\n\nHätten wir das gleiche mit as.Date() gemacht, würden wir natürlich keine Uhrzeit mit dabei haben:\n\ndataset %>% \n  mutate(departure = as.Date(departure, format = \"%d.%m.%Y\"),\n         scheduled_departure = as.Date(departure, format = \"%d.%m.%Y\"),\n         arrival = as.Date(arrival, format = \"%d.%m.%Y\"),\n         scheduled_arrival = as.Date(scheduled_arrival, format = \"%d.%m.%Y\")) %>% \n  head()\n\n\n\n  \n\n\n\nSuper! Nun wisst ihr, wie wie Spalten, in denen das Datum als character gespeichert ist, in ein richtiges Datumsobjekt überführt. Dennoch stellt sich die Frage, warum wir das ganze überhaupt machen - schließlich hätten wir die Spalten auch als einzelne “Faktoren” belassen können. Dazu kommen wir jetzt, denn die Datumsobjekte bringen eine Vielzahl sehr praktischer Funktionen mit sich, und dabei hilft uns am meisten das Package lubridate."
  },
  {
    "objectID": "analysis3_1_dates.html#lubridate-und-datumsobjekte",
    "href": "analysis3_1_dates.html#lubridate-und-datumsobjekte",
    "title": "Dates & Times",
    "section": "Lubridate und Datumsobjekte",
    "text": "Lubridate und Datumsobjekte\nWir könnten meinen, dass die obige Vorgehensweise etwas umständlich ist und erfordert, dass wir wissen, was die einzelnen Codes bedeuten. Lubridate bringt die Funktionen ymd_hms() und weitere mit:\n\nymd_hms(\"69/07/20-20:17:40\")\n\n[1] \"1969-07-20 20:17:40 UTC\"\n\n\nWir sehen, dass ymd_hms() das Datum auch ohne die Angabe des Formats parsed.\n\nFrage: Was wäre, wenn das Datum 20/7/1969 20:17:40 gelautet hätte?\n\nLubridate bringt wie bereits erwähnt eine Vielzahl von Funktionen mit. Das heißt, wenn wir den obigen String parsen wollten, könnten wir einfach dmy_hms() benutzen:\n\ndmy_hms(\"20/7/1969 20:17:40\")\n\n[1] \"1969-07-20 20:17:40 UTC\"\n\n\nEbenso gibt es mdy() für Daten wie \"7-20-1969\" oder mdy_hm() für \"07-20-1969 20:17\":\n\nmdy(\"7-20-1969\")\n\n[1] \"1969-07-20\"\n\n\nWir wissen nun also, wie wir bequem Datumsobjekte richtig umwandeln können. Aber was kann lubridate noch?\n\nKomponenten\nSobald wir ein Datumsobjekt haben, können wir allerlei Informationen daraus ziehen. Also bspw. den Tag…\n\ndate <- dmy_hms(\"20/7/1969 20:17:40\")\n\nday(date)\n\n[1] 20\n\n\n…den Monat…\n\nmonth(date)\n\n[1] 7\n\n\noder das Jahr…\n\nyear(date)\n\n[1] 1969\n\n\n…oder die Kalenderwoche:\n\nweek(date)\n\n[1] 29\n\n\nManchmal kann es auch hilfreich sein, das Quartal zu extrahieren:\n\nquarter(date)\n\n[1] 3\n\n\nSuper! Wir wissen nun also, wie wir aus einem funktionierenden Datumsobjekt Komponenten herausziehen können. Versuchen wir das doch mal mit unserem Datensatz dataset.\n\nWie würden wir vorgehen, um die Datumsspalten mit lubridate richtig zu formatieren?\n\n\ndataset %>% \n  mutate(departure = dmy_hms(departure),\n         scheduled_departure = dmy_hms(scheduled_departure),\n         arrival = dmy_hms(arrival),\n         scheduled_arrival = dmy_hms(scheduled_arrival))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `arrival = dmy_hms(arrival)`.\nCaused by warning:\n!  12 failed to parse.\n\n\n\n\n  \n\n\n\nIn der Warnmeldung sehen wir, dass 12 Zeilen nicht geparsed werden konnten. Offensichtlich gibt es 12 Zeilen, in denen es Probleme gibt. Da wir knapp 10,000 Observations haben, sind uns diese 12 Sonderfälle egal, und wir ignorieren den Fehler. Sollten wir aber einen kleineren Datensatz/kleinere Stichprobe haben, so wäre es schon relevant sich die betreffenden Zeilen anzuschauen. Wir müssen jetzt das ganze noch abspeichern:\n\ndataset <- dataset %>% \n              mutate(departure = dmy_hms(departure),\n                     scheduled_departure = dmy_hms(scheduled_departure),\n                     arrival = dmy_hms(arrival),\n                     scheduled_arrival = dmy_hms(scheduled_arrival))\n\nSicherlich erinnern wir uns noch an den flights Datensatz, und dass wir dort Verspätungen berechnet haben. Nun wollen wir das gleiche mit unserem Datensatz dataset machen. Das könnten wir etwa aus arrival und scheduled_arrival machen.\n\nFrage: Wie würden wir vorgehen?\n\nIm originalen flights Datensatz gab es einzelne Spalten für die Minuten und Stunden, sodass wir einfach die Verspätung als Zahl berechnen konnten (in dem Fall als Minuten). Nun, da wir richtige Datumsobjekte haben, können wir direkt mit ihnen arithmetische Operationen durchführen, was dann zu sogenannten difftime Objekten führt.\n\n\nDifftime\nBeachten wir den Output des folgenden Codes:\n\ndate1 <- dmy_hms(\"20/7/1969 20:17:40\")\ndate2 <- dmy_hms(\"21/7/1969 17:54:00\")\n\ndate2-date1\n\nTime difference of 21.60556 hours\n\n\nR gibt uns als Ergebnis der Subtraktion einen Satz mit der Zeitdifferenz aus. Wir sehen, dass der Datentyp difftime ist:\n\nclass(date2-date1)\n\n[1] \"difftime\"\n\n\nDifftime Objekte geben uns die Differenz intern in Sekunden wieder. Wir können die Differenz aber auch explizit berechnen, indem wir die difftime() Funktion benutzen. Hier müssen wir dann lediglich die zwei Zeitpunkte mit einem Komma (,) getrennt übergeben:\n\ndifftime(date2,date1)\n\nTime difference of 21.60556 hours\n\n\nNun können wir auch explizit machen, welche Einheit wir wollen:\n\ndifftime(date2,date1,units = \"mins\")\n\nTime difference of 1296.333 mins\n\n\nWir wissen also nun, wie wir Zeitdifferenzen berechnen. Wenden wir das nun also auf unseren Datensatz dataset an, und berechnen die Variable late:\n\ndataset <- dataset %>% \n              mutate(late = difftime(arrival,scheduled_arrival,units = \"mins\"))\n\nNun wollen wir late visualisieren, um eine Idee über die durchschnittlichen Verspätungen zu bekommen:\n\ndataset %>% \n  ggplot(aes(x=late))+\n  geom_bar()+\n  theme_classic()\n\nDon't know how to automatically pick scale for object of type <difftime>.\nDefaulting to continuous.\n\n\nWarning: Removed 12 rows containing non-finite values (`stat_count()`).\n\n\n\n\n\nWir bekommen leider kein richtiges Ergebnis angezeigt.\n\nWoran könnte das liegen?\n\nEin kleiner Blick auf die Daten könnte uns helfen. Dazu müssen wir kurz die Variable mit as.numeric() umwandeln:\n\nsummary(as.numeric(dataset$late))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n-3682851      -18       -6     7947       13  3682855       12 \n\n\nWir sehen, dass wir Werte haben, die extrem von der Mitte abweichen. Diese Ausreißer sollten wir entfernen. Wir wir das machen, lernt ihr gleich.\nBis dahin können wir aber hingehen und alles mit mehr als 120 Minuten Verspätung (oder zu früh) ausschließen:\n\ndataset %>% \n  filter(late < 120 & late > -120) %>% \n  ggplot(aes(x=late))+\n  geom_bar(fill=palette1[3])+\n  geom_vline(xintercept=0,linetype=\"dashed\")+\n  theme_classic()\n\nDon't know how to automatically pick scale for object of type <difftime>.\nDefaulting to continuous.\n\n\n\n\n\nWie zu sehen, kommt der Großteil der Flüge früher an als geplant, im Schnitt 0.48 Minuten.\nAngenommen, wir haben eine Studie mit 150 Proband:innen, und wollen den Datensatz in drei Gruppen aufteilen, je nach Uhrzeit: morgens, mittags und abends.\n\nWie könnten wir vorgehen? Wir haben POSIXct, Date und Difftime kennengelernt.\n\nAußer difftime, welches eine zeitliche Differenz wiedergibt, enthalten die beiden Typen POSXIct und Date immer auch das Datum. Was das für Implikationen hat, sehen wir, wenn wir die verschiedenen Daten im Folgenden betrachten:\n\nlanding_times <- c(dmy_hms(\"20/7/1969 20:17:40\"),\n                   dmy_hms(\"19/11/1969 06:54:35\"),\n                   dmy_hms(\"5/2/1971 09:18:11\"),\n                   dmy_hms(\"30/7/1971 22:16:29\"),\n                   dmy_hms(\"21/4/1972 02:23:35\"),\n                   dmy_hms(\"11/12/1972 19:54:58\")) %>% \n  as.data.frame() %>% \n  setNames(c(\"time\"))\n\nlanding_times\n\n\n\n  \n\n\n\nlanding_times zeigt uns verschiedene Zeitpunkte, die teilweise Jahre voneinander entfernt sind. Wenn uns jetzt aber nur die Uhrzeit interessiert, wollen wir das Datum gar nicht betrachten.\n\nFrage: Wie können wir das lösen?\n\nEine Möglichkeit wäre es, über hour() und minute() zu arbeiten, etwa so:\n\nlanding_times %>% \n  mutate(hour=hour(time),minute=minute(time))\n\n\n\n  \n\n\n\nWenn wir allerdings eine Spalte haben wollen, in der beides enthalten ist (also nur die Zeit, nach dem Prinzip h:m:s, dann kommen wir mit base-R nicht weiter, dann hilft uns das gleichnamige Package hms, über welches wir Zeiten abspeichern können."
  },
  {
    "objectID": "analysis3_1_dates.html#hms",
    "href": "analysis3_1_dates.html#hms",
    "title": "Dates & Times",
    "section": "HMS::",
    "text": "HMS::\nWir wollen an dieser Stelle gar nicht zu tief in ein weiteres Package einsteigen, daher konzentrieren wir uns auf die für uns relevante Kernfunktionalität von hms: Zeiten abspeichern. Über den Befehl as_hms() können wir also aus einem Datumsobjekt die Zeit herausziehen, und das Datum loswerden:\n\nlibrary(hms)\n\n\nAttaching package: 'hms'\n\n\nThe following object is masked from 'package:lubridate':\n\n    hms\n\nlanding_times %>% \n  mutate(time_only = as_hms(time))\n\n\n\n  \n\n\n\nPerfekt! Ein neuer Versuch, unsere landing_times zu plotten:\n\nlanding_times %>% \n  mutate(time_only = as_hms(time)) %>% \n  ggplot(aes(x=time_only))+\n  geom_density(fill=\"grey\")+\n  theme_classic()\n\n\n\n\nPerfekt! Wir sehen, dass es deskriptiv eine leichte Tendenz nach hinten gibt, d.h., die Zeiten sind eher in den späten Stunden.\n\nFrage: Wofür könnte das eben Erlente im Komedia Kontext hilfreich sein?\n\nBei Fragebogenstudien, sowie im Kontext von Learning Analytics können Zeiten und Datumsobjekte überaus hilfreich sein. So könnte man etwa die Tageszeit bei Fragebögen/Studien mit in die Analyse aufnehmen, oder die Beteiligung in Online-Foren im zeitlichen Verlauf bzw. in Abhängigkeit der Tageszeit. Das können wir nun - super!\n\n\n\n\n\n\nFrage: Wo könnte es noch Probleme geben?\n\nZeitzonen und wie sie abgespeichert sind ist ein überaus komplexes Thema, welches wir hier ebenfalls nicht komplett elaborieren wollen. Was aber wichtig ist, dass natürlich jedes Zeitobjekt einer Zeitzone zugewiesen wird. Standardmäßig wird bei den Lubridate Funktionen wie etwa dmy_hms() die Zeitzone UTC (Universal Coordinated Time) zugewiesen.\nSolange wir nicht Daten aus verschiedenen Quellen haben (vor allem aus verschiedenen Zeitzonen), sollte uns das erstmal nicht betreffen. Wenn wir allerdings in unserem Mondlandungsbeispiel bleiben, dann könnte uns durchaus interessieren, um wieviel Uhr hier in Duisburg die Astronauten jeweils ihren Fuß auf den Mond gesetzt haben. Wir wissen aus sicherer Quelle, dass die Zeitzone bereits UTC ist. Die Funktion tz() kann uns dabei helfen:\n\nlanding_times %>% \n  mutate(tz = tz(time))\n\n\n\n  \n\n\n\nWenn wir nun zwei neue Spalten time_duisburg und tz_duisburg hinzufügen wollen, kann uns die Funktion with_tz() helfen, welche die Zeitzone in eine angegbene umrechnet:\n\nlanding_times %>% \n  mutate(tz=tz(time),\n         time_duisburg = with_tz(time,tzone=\"EST\"),\n         tz_duisburg = tz(time_duisburg))\n\n\n\n  \n\n\n\nSuper! Nun wissen wir auch die Zeiten in Duisburg, zu denen die Mondlandungen geschehen sind.\n\nlanding_times <- c(dmy_hms(\"20/7/1969 20:17:40\", tz=\"UTC\"),\n                   dmy_hms(\"19/11/1969 06:54:35\", tz=\"UTC\"),\n                   dmy_hms(\"5/2/1971 09:18:11\", tz=\"UTC\"),\n                   dmy_hms(\"30/7/1971 22:16:29\", tz=\"UTC\"),\n                   dmy_hms(\"21/4/1972 02:23:35\", tz=\"UTC\"),\n                   dmy_hms(\"11/12/1972 19:54:58\", tz=\"UTC\")) %>% \n  as.data.frame() %>% \n  setNames(c(\"time\"))\n\nlanding_times %>% \n  mutate(tz = tz(time),\n         time_duisburg = with_tz(time,tzone=\"EST\"),\n         tz_duisburg = tz(time_duisburg))\n\n\n\n  \n\n\n\nSuper! Wir kennen nun dates, times, datetimes, POSIXct & times. Und noch Zeitzonen! Wenn ihr mehr über Datumsobjekte erfahren wollt, schaut gerne hier vorbei:\nR for Data Science"
  },
  {
    "objectID": "datawrangling_dplyr.html",
    "href": "datawrangling_dplyr.html",
    "title": "Wrangling mit dplyr",
    "section": "",
    "text": "Show code\nlibrary(tidyverse)\nlibrary(reshape2)\nlibrary(reactable)\n\npalette1 <- c(\"#648fff\",\"#785ef0\",\"#dc267f\",\"#fe6100\",\"#ffb000\",\"#000000\",\"#ffffff\")\npalette2 <- c(\"#CC79A7\",\"#D55E00\",\"#0072B2\",\"#F0E442\",\"#009E73\",\"#56B4E9\",\"#E69F00\",\"#000000\")\n\ndataset <- read.csv(\"assets/datasets/iris.csv\")\ndataset$Species <- as.factor(dataset$Species)"
  },
  {
    "objectID": "datawrangling_dplyr.html#hintergrund",
    "href": "datawrangling_dplyr.html#hintergrund",
    "title": "Wrangling mit dplyr",
    "section": "Hintergrund",
    "text": "Hintergrund\nViele verschiedene Objekte im Environment sind nur eine Motivation, dplyr zu benutzen. Vielmehr bietet es “eine einfache Grammatik der Datenmanipulation, welche einfach zu lernen & anzuwenden ist” (vgl. tidyverse.com) und viel Flexibilität bietet - daher gehört es auch zu den beliebtesten R-Packages, besonders wenn es um Datenanalyse geht."
  },
  {
    "objectID": "datawrangling_dplyr.html#der-pipe-operator",
    "href": "datawrangling_dplyr.html#der-pipe-operator",
    "title": "Wrangling mit dplyr",
    "section": "Der Pipe-Operator",
    "text": "Der Pipe-Operator\nIm Zentrum von dplyr steht der sogenannte Pipe-Operator %>% (ursprünglich aus dem Magrittr Package). Dieser schaut zunächst etwas ungewohnt aus, daran gewöhnt man sich aber schnell:\n\nobject %>% \n  do_something(parameters = \"xyz\")\n\nMithilfe des Pipe Operators können wir verschiedene Befehler aneinanderketten. Der Output des jeweiligen Befehls wird sozusagen als Input in die nächste Zeile “gepiped”. Das heißt, wenn der Input ein Dataframe ist, mit dem dann entsprechend Anpassungen vorgenommen werden, so ist der Output für die nächste Zeile wieder ein Dataframe - welches diese dann weiterverarbeitet.\nFür den Operator gibt es natürlich eine Tastenkombination, mithilfe derer wir ihn schnell eingeben können. Diese lautet COMMAND + SHIFT + M für Mac-User, und STRG + SHIFT + M für Windows User.\nWir erinnern uns, dass der obige Befehl üblicherweise so lauten würde:\n\ndo_something(object, parameters = \"xyz\")\n\nHier müssen wir das betreffende Objekt als Parameter übergeben, und anschließend den Output entweder direkt verwerten, oder über <- speichern. Wenn wir nun eine zweite Funktion do_something_else für das gleiche Objekt verwenden wollten, müssten wir dies erneut speichern, oder überspeichern:\n\nresult <- do_something(object, parameters = \"xyz\")\n\ndo_something_else(result, parameters = \"abc\")\n\nMithilfe des Pipe Operators %>% können wir uns das sparen. Wir starten mit dem ursprünglichen Objekt, und geben dann die bearbeiteten Versionen in die jeweilige Zeile weiter:\n\nobject %>% \n  do_something(parameters = \"xyz\") %>% \n  do_something_else(parameters = \"abc\")\n\nDer Output von do_something() wird also zum Input von do_something_else() ."
  },
  {
    "objectID": "datawrangling_dplyr.html#funktionen",
    "href": "datawrangling_dplyr.html#funktionen",
    "title": "Wrangling mit dplyr",
    "section": "Funktionen",
    "text": "Funktionen\ndplyr ist aber natürlich nicht nur wegen des Pipe-Operators so praktisch. Es bringt auch viele Funktionen mit sich, die bei typischen Datenanalyse-Aufgaben relevant sind. Diese Funktionen ermöglichen ähnliche Dinge wie bspw. das auf der vorigen Seite gezeigte subset(), sind dabei jedoch etwas weniger umständlich und intuitiver. Im Folgenden wollen wir euch die wichtigsten davon vorstellen:\n\nfilter()\nBeschreibung von tidyverse.org:\n\nThe filter() function is used to subset a data frame, retaining all rows that satisfy your conditions. To be retained, the row must produce a value of TRUE for all conditions. Note that when a condition evaluates to NA the row will be dropped, unlike base subsetting with [.\n\n\nMit der filter() Funktion können wir, wie der Name schon sagt, Datensätze auf Basis gewisser Kriterien filtern. Angenommen, wir wollen wieder nach der Spezies setosa filtern. Dies geschieht mit dplyr wie folgt:\n\ndataset %>% \n  filter(Species == \"setosa\")\n\n\n\n  \n\n\n\nEbenso können wir mehrere Bedingungen kombinieren:\n\ndataset %>% \n  filter(Species == \"setosa\" & Sepal.Length < 5)\n\n\n\n  \n\n\n\nDas besondere an dplyr ist, dass wir dies nun aber auch in zwei Schritten machen könnten:\n\ndataset %>% \n  filter(Species == \"setosa\") %>% \n  filter(Sepal.Length < 5)\n\n\n\n  \n\n\n\nWie zu sehen ist, sind die resultierenden Dataframes dieselben.\n\n\nselect()\nBeschreibung von tidyverse.org:\n\nSelect (and optionally rename) variables in a data frame, using a concise mini-language that makes it easy to refer to variables based on their name (e.g. a:f selects all columns from a on the left to f on the right) or type (e.g. where(is.numeric) selects all numeric columns).\n\n\nEbenso wie mit select() in subset() können wir hiermit verschiedene Spalten auswählen. Die Syntax ist wie folgt:\n\ndataset %>% \n  select(Sepal.Length,Sepal.Width)\n\n\n\n  \n\n\n\nDamit wählen wir die Spalten Sepal.Length und Sepal.Width aus. Wir könnten auch sagen, wir wollen alle Spalten außer Species:\n\ndataset %>% \n  select(-Species)\n\n\n\n  \n\n\n\nOft kommt es vor, dass wir bspw. durch Limesurvey wissen, dass alle Variablen eines Fragebogens mit “SQ..” anfangen. Sollten wir diese alle schnell auswählen wollen (bspw. zum Berechnen von Summenscores), kann uns dplyr auch dabei helfen, mithilfe von starts_with().\n\ndataset %>% \n  select(starts_with(\"Se\"))\n\n\n\n  \n\n\n\n\n\nmutate()\nBeschreibung von tidyverse.org:\n\nmutate() creates new columns that are functions of existing variables. It can also modify (if the name is the same as an existing column) and delete columns (by setting their value to NULL).\n\n\nApropos Summenscores - mutate() klingt gruseliger, als es ist. Mit mutate() können wir neue Variablen kreieren, oder bestehende modifizieren. Angenommen, die Blätter der Iris-Blume wären rechteckig, und wir könnten die Fläche Petal.Square einfach in Quadratzentimeter berechnen:\n\ndataset %>% \n  mutate(Petal.Square = Petal.Length*Petal.Width)\n\n\n\n  \n\n\n\nWir sehen, die Syntax ist mutate(neue_variable = berechnungen). Wir können auch mehrere Variablen in einem Zug erzeugen:\n\ndataset %>% \n  mutate(Petal.Square = Petal.Length*Petal.Width,\n         Sepal.Square = Sepal.Length*Sepal.Width)\n\n\n\n  \n\n\n\nWenn der neue_variable Name gleich wie der alte ist, überschreiben wir die Spalte:\n\ndataset %>% \n  mutate(Sepal.Length = Sepal.Length/10)\n\n\n\n  \n\n\n\n\n\narrange()\nBeschreibung von tidyverse.org:\n\narrange() orders the rows of a data frame by the values of selected columns. Unlike other dplyr verbs, arrange() largely ignores grouping; you need to explicitly mention grouping variables (or use  .by_group = TRUE) in order to group by them, and functions of variables are evaluated once per data frame, not once per group.\n\n\nManchmal kommt es vor, dass wir einen Datensatz sortieren wollen, um einen besseren Überblick über die Daten zu bekommen. Dazu bietet sich arrange() an:\n\ndataset %>% \n  arrange(Sepal.Length)\n\n\n\n  \n\n\n\nWir sehen, die Spalte Sepal.Length wird in aufsteigender Reihenfole sortiert. Wollen wir es in absteigender Reihenfolge haben, müssen wir die Funktion desc() dazunehmen:\n\ndataset %>% \n  arrange(desc(Sepal.Length))\n\n\n\n  \n\n\n\n\n\ngroup_by()\nBeschreibung von tidyverse.org:\n\nMost data operations are done on groups defined by variables. group_by() takes an existing tbl and converts it into a grouped tbl where operations are performed “by group”. ungroup() removes grouping.\n\n\nEine der hilfreichsten Funktionen von dplyr ist die group_by() Funktion. Hier ist allerdings eine etwas genauere Erklärung erforderlich. Die generelle Syntax ist die folgende:\n\nobject %>% \n  group_by(variable) %>% \n  do_something()\n\nWir übergeben der group_by() Funktion als Parameter eine Variable, nach der diese den Datensatz gruppieren soll. Hier ist es wichtig zu verstehen, dass diese Gruppierung für alle nachfolgenden Schritte gilt, selbst aber keinen Effekt hat. Was heißt das genau? Betrachten wir folgenden Code:\n\ndataset %>% \n  group_by(Species)\n\n\n\n  \n\n\n\nAußer der Info Groups: Species [3] hat sich nichts verändert. Nehmen wir allerdings in der nächsten Zeile eine Funktion hinzu (bspw. mutate()), so verändert das, wie diese sich verhält. Normalerweise würde folgender Code die Variable Sepal.Length_mean erzeugen, welche den Mittelwert von Sepal.Length enthält:\n\ndataset %>% \n  mutate(Sepal.Length_mean = mean(Sepal.Length))\n\n\n\n  \n\n\n\nDa dieser natürlich für den gesamten Datensatz berechnet wird, beträgt er für jede Messung 5.84. Wenn wir vorher allerdings group_by() eingeben, sieht das anders aus:\n\ndataset %>% \n  group_by(Species) %>% \n  mutate(Sepal.Length_mean = mean(Sepal.Length))\n\n\n\n  \n\n\n\nWir sehen, dass mutate() den Wert pro Gruppe berechnet hat. Dies kann sehr hilfreich für verschiedenste Anwendungen sein. Ebenso können wir nach mehreren Variablen gruppieren. Dazu aber gleich mehr.\n\n\nsummarise()\nBeschreibung von tidyverse.org:\n\nsummarise() creates a new data frame. It returns one row for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarising all observations in the input. It will contain one column for each grouping variable and one column for each of the summary statistics that you have specified. summarise() and summarize() are synonyms.\n\n\nVielleicht wirkt es etwas merkwürdig, dass wir oben die Variable Sepal.Length_mean für jeden Datenpunkt einzeln berechnet haben, obwohl diese natürlich immer gleich ist. Meistens wollen wir solche Werte für die gesamte Stichprobe haben. Dabei hilft uns summarise():\n\ndataset %>% \n  summarise(Sepal.Length_mean = mean(Sepal.Length))\n\n\n\n  \n\n\n\nWir sehen, dass die Syntax dieselbe wie die von mutate() ist, nur dass der Output eben nur eine Zahl (bzw. eine Zeile ist) - eben die Summary unseres Datensatzes unter den gegebenen Bedingungen. Besonders hilfreich ist summarise() in Kombination mit group_by():\n\ndataset %>% \n  group_by(Species) %>% \n  summarise(Sepal.Length_mean = mean(Sepal.Length))\n\n\n\n  \n\n\n\nSo können wir mit drei Zeilen die Mittelwerte für die jeweilige Gruppe sehen. Wir können natürlich auch gleich die Standardabweichung mitberechnen:\n\ndataset %>% \n  group_by(Species) %>% \n  summarise(Sepal.Length_mean = mean(Sepal.Length),\n            Sepal.Length_sd = sd(Sepal.Length))\n\n\n\n  \n\n\n\n\nFür Expert:innen:\nOben wurde kurz erwähnt, dass wir auch nach mehreren Variablen gruppieren können. Angenommen, es gäbe noch zusätzlich die Variable color in den Ausprägungen purple, blue und white für jeden Datenpunkt, d.h., jede Blüte kann auch eine dieser verschiedenen Farben haben.\n\n\nShow code\ndataset %>% \n  mutate(color = as.factor(sample(rep(c(\"purple\",\"blue\",\"white\"),50)))) -> dataset\n\n\nWenn wir nun nach den Variablen Species und color gruppieren wollen, müssen wir diese beiden Variablen als Parameter übergeben:\n\ndataset %>% \n  group_by(Species, color) %>% \n  summarise(Sepal.Length_mean = mean(Sepal.Length),\n            Sepal.Length_sd = sd(Sepal.Length))\n\n\n\n  \n\n\n\nNun sehen wir die Mittelwerte für die verschiedenen Kombinationen der Variablen (also bspw. setosa & blue). Hier kann es hilfreich sein, die Anzahl an Zeilen pro Gruppe zu bekommen. Hierfür eignet sich die n() Funktion:\n\ndataset %>% \n  group_by(Species, color) %>% \n  summarise(Sepal.Length_mean = mean(Sepal.Length),\n            Sepal.Length_sd = sd(Sepal.Length),\n            n = n())\n\n\n\n  \n\n\n\nNun haben wir die Mittelwerte für jede Kombination aus Species und color, sowie die Anzahl n der jeweiligen Zeilen bzw. Messungen."
  },
  {
    "objectID": "datawrangling_dplyr.html#temporär-oder-speichern",
    "href": "datawrangling_dplyr.html#temporär-oder-speichern",
    "title": "Wrangling mit dplyr",
    "section": "Temporär oder Speichern?",
    "text": "Temporär oder Speichern?\nSicherlich ist aufgefallen, dass alle unsere gezeigten “Pipes” nur zu einem Output im Chunk geführt haben, wir diese allerdings nicht in unserem Environment gespeichert haben. Natürlich können wir dies tun, indem wir unsere Pipe einem Objekt über den Zuweisungsoperator <- zuweisen:\n\nobject <- object %>% \n            select(variableA,variableB,variableC) %>% \n            mutate(variableA*100)\n\nNatürlich ist uns selbst überlassen, wie wir mit den Daten umgehen. Manchmal wird es Fälle geben, in denen Speichern Sinn ergibt (bspw. wenn sonst immer wieder die gleiche Transformation anstünde), oft wird es aber auch ausreichend sein, die Daten einfach im Output des jeweiligen Chunks anzuschauen, bzw. etwaige Transformationen unmittelbar vor weiteren Schritten zu machen (bspw. beim Erzeugen von Plots).\nAchtung: Wenn wir mit summarise() einen Output bekommen, so ist dieser natürlich nicht mehr das “ursprüngliche” Dataframe. Diesen würden wir uns also üblicherweise im Chunk-Output anschauen."
  },
  {
    "objectID": "datawrangling_dplyr.html#fazit",
    "href": "datawrangling_dplyr.html#fazit",
    "title": "Wrangling mit dplyr",
    "section": "Fazit",
    "text": "Fazit\ndplyr bringt eine einfach verständliche Grammatik mit sich, die uns hilft, uns auf das wesentliche zu konzentrieren: die Daten. Im nächsten Kapitel werden wir lernen, wie wir diese Daten visualiseren können, um sie noch besser zu verstehen."
  },
  {
    "objectID": "datawrangling_base.html",
    "href": "datawrangling_base.html",
    "title": "Base R",
    "section": "",
    "text": "Show code\nlibrary(tidyverse)\nlibrary(reshape2)\nlibrary(reactable)\n\npalette1 <- c(\"#648fff\",\"#785ef0\",\"#dc267f\",\"#fe6100\",\"#ffb000\",\"#000000\",\"#ffffff\")\npalette2 <- c(\"#CC79A7\",\"#D55E00\",\"#0072B2\",\"#F0E442\",\"#009E73\",\"#56B4E9\",\"#E69F00\",\"#000000\")"
  },
  {
    "objectID": "datawrangling_base.html#iris-datensatz",
    "href": "datawrangling_base.html#iris-datensatz",
    "title": "Base R",
    "section": "Iris Datensatz",
    "text": "Iris Datensatz\nBeginnen wir also zunächst damit, Daten in R einzulesen. Wir werden heute mit dem iris Datensatz arbeiten, einem berühmten Datensatz von R.A. Fisher aus dem Jahr 1936, in dem verschiedene Eigenschaften in Bezug auf die Blütengröße verschiedener Spezies der Blume Iris enthalten sind (siehe hier).\n\n\n\nSources: Wikipedia; Danielle Langlois, Денис Анисимов & Eric Hunt\n\n\nIm Zentrum des Datensatzes steht die Größe der verschiedenen Blatttypen für die jeweilige Spezies. Dabei gibt es die Blatttypen Sepal und Petal. Die folgende Grafik verdeutlicht das etwas:\n\n\n\n\n\nDer Datensatz ist direkt im Package datasets integriert. Um allerdings zu lernen, wie wir Daten bspw. aus .csv einlesen, haben wir den Datensatz nochmal als einzelne Datei vorbereitet. Diese können wir hier herunterladen:\n Iris Datensatz \n\nExkurs: Trennzeichengetrennte Textdateien & CSV\n.csv ist ein typisches Datenformat, aus dem wir mit R Daten einlesen können. Wenn wir die Datei mit einem Textbearbeitungsprogramm öffnen, sehen wir, wie die Daten dort strukturiert sind:\n\n\n\n.csv in einem Texteditor (hier Sublime Text)\n\n\nWir sehen, dass die Daten einfach hintereinander per Text in der Datei gespeichert sind, und mit einem Trennzeichen (in diesem Fall das ,) voneinander getrennt sind. Strings werden mit \" abgespeichert. Die erste Zeile beschreibt die Spaltennamen."
  },
  {
    "objectID": "datawrangling_base.html#daten-einlesen",
    "href": "datawrangling_base.html#daten-einlesen",
    "title": "Base R",
    "section": "Daten einlesen",
    "text": "Daten einlesen\nNun wissen wir also, wie .csv Dateien funktionieren. Fangen wir also an, die Daten in R-einzulesen:\n\ndataset <- read.csv(\"assets/datasets/iris.csv\")\n\nDie Funktion read.csv liest die Dateien ein. Als Parameter müssen wir lediglich den Pfad zur Datei angeben. In unserem Fall handelt es sich um einen relativen Pfad - absolute Pfade würden auch funktionieren. In unserem Fall ist die Datei wir wir wissen mit Komma (,) getrennt, daher brauchen wir nichts weiter als Parameter zu übergeben. Achtung: Das deutsche Excel speichert Dateien bei .csv oft mit Semikolon (;) als Trennzeichen ab (aufrund des Kommas als Dezimaltrennzeichen).\n\nMac User:innen aufgepasst: Wenn ihr im Finder mit der rechten Maustaste auf eine Datei klickt, und dann alt bzw. option drückt, könnt ihr direkt den Pfad zur Datei in eure Zwischenablage kopieren. Das eignet sich hier bspw. besonders gut.\n\nWenn alles geklappt hat, sollte dataset rechts in unserem Environment auftauchen. Schauen wir es uns also an. Hierzu können wir entweder rechts oben auf das Objekt klicken, damit es sich im Viewer öffnet, oder wir schauen es uns hier an:\n\ndataset\n\n\n\n  \n\n\n\nWie wir sehen gibt es mehrere Spalten für die Länge und Breite der Blätter (e.g., Sepal.Length), sowie eine Spalte mit der Spezies (Species).\nWerfen wir nun einen genaueren Blick in die Daten."
  },
  {
    "objectID": "datawrangling_base.html#daten-anschauen",
    "href": "datawrangling_base.html#daten-anschauen",
    "title": "Base R",
    "section": "Daten anschauen",
    "text": "Daten anschauen\nUm schnell einen Überblick über die Daten zu bekommen, eignet sich die summary() Funktion:\n\nsummary(dataset)\n\n       X           Sepal.Length    Sepal.Width     Petal.Length  \n Min.   :  1.00   Min.   :4.300   Min.   :2.000   Min.   :1.000  \n 1st Qu.: 38.25   1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600  \n Median : 75.50   Median :5.800   Median :3.000   Median :4.350  \n Mean   : 75.50   Mean   :5.843   Mean   :3.057   Mean   :3.758  \n 3rd Qu.:112.75   3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100  \n Max.   :150.00   Max.   :7.900   Max.   :4.400   Max.   :6.900  \n  Petal.Width      Species         \n Min.   :0.100   Length:150        \n 1st Qu.:0.300   Class :character  \n Median :1.300   Mode  :character  \n Mean   :1.199                     \n 3rd Qu.:1.800                     \n Max.   :2.500                     \n\n\nHier sehen wir direkt deskriptive Werte wie Minimum, Maximum oder Median für jede einzelne Spalte. Für die Spalte Species sehen wir allerdings lediglich, dass diese als character gespeichert ist, wenngleich es sich eigentlich um einen Faktor handelt, da wir ja bereits wissen, dass es drei Spezies gibt. Das können wir ändern:\n\ndataset$Species <- as.factor(dataset$Species)\n\nFühren wir nun den Code erneut aus, sehen wir die entsprechenden Faktorlevel bzw. Ausprägungen und die jeweiligen Datenpunkte pro Ausprägung:\n\nsummary(dataset)\n\n       X           Sepal.Length    Sepal.Width     Petal.Length  \n Min.   :  1.00   Min.   :4.300   Min.   :2.000   Min.   :1.000  \n 1st Qu.: 38.25   1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600  \n Median : 75.50   Median :5.800   Median :3.000   Median :4.350  \n Mean   : 75.50   Mean   :5.843   Mean   :3.057   Mean   :3.758  \n 3rd Qu.:112.75   3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100  \n Max.   :150.00   Max.   :7.900   Max.   :4.400   Max.   :6.900  \n  Petal.Width          Species  \n Min.   :0.100   setosa    :50  \n 1st Qu.:0.300   versicolor:50  \n Median :1.300   virginica :50  \n Mean   :1.199                  \n 3rd Qu.:1.800                  \n Max.   :2.500                  \n\n\nDas Gleiche würden wir sehen, wenn wir mit der Funktion levels die Ausprägungen für die entsprechende Spalte abfragen:\n\nlevels(dataset$Species)\n\n[1] \"setosa\"     \"versicolor\" \"virginica\""
  },
  {
    "objectID": "datawrangling_base.html#deskriptive-analysen",
    "href": "datawrangling_base.html#deskriptive-analysen",
    "title": "Base R",
    "section": "Deskriptive Analysen",
    "text": "Deskriptive Analysen\nStrenggenommen hat uns die summary() Funktion bereits viele interessante Werte für alle Spalten gegeben. Falls wir dies aber für eine einzelne Spalte machen wollen, können wir dies natürlich auch tun:\n\nmean(dataset$Sepal.Length)\n\n[1] 5.843333\n\n\nWie wir sehen, kann die mean() Funktion mit einer Spalte eines Dataframes (über den $-Operator ausgewählt) arbeiten, und gibt uns den entsprechenden Mittelwert aus. Gleich verhält es sich mit der Standardabweichung:\n\nsd(dataset$Sepal.Length)\n\n[1] 0.8280661\n\n\nWie wir in der ersten Seminarstunde bereits gesagt haben, bietet R als Statistiksoftware natürlich eine Vielzahl eingebauter Standard-Funktionen. So können wir etwa auch gleich die Korrelation zwischen Sepal.Width und Sepal.Length berechnen…\n\ncor(dataset$Sepal.Length,dataset$Sepal.Width, method = \"pearson\")\n\n[1] -0.1175698\n\n\n… oder die Quantile ausgeben lassen:\n\nquantile(dataset$Sepal.Length)\n\n  0%  25%  50%  75% 100% \n 4.3  5.1  5.8  6.4  7.9 \n\n\nDas sind natürlich alles nur Beispiele der im base Package enthaltenen Funktionen - alle weiteren können wir bspw. in der Dokumentation oder über help(base) finden."
  },
  {
    "objectID": "datawrangling_base.html#datensatz-aufteilen",
    "href": "datawrangling_base.html#datensatz-aufteilen",
    "title": "Base R",
    "section": "Datensatz aufteilen",
    "text": "Datensatz aufteilen\nAngenommen, wir wollen die Spalte Species von den anderen Spalten trennen, und in einem separaten Aufgabenblatt abspeichern. Dies können wir über die $ und <- Operatoren machen:\n\ndataset_species <- dataset$Species\n\nDies speichert die entsprechende Spalte als Objekt dataset_species in unserem Environment. Dies geschieht als Vektor des Typs factor. Die Länge des Vektors entspricht natürlich der Anzahl an Observations:\n\nlength(dataset_species)\n\n[1] 150\n\n\nNun wollen wir einen Schritt weitergehen, und nicht nur einzelne Spalten extrahieren, sondern unseren Datensatz auf Basis gewisser Kriterien filtern, bzw. ein Subset bilden. Base R bietet dafür die subset() Funktion. Deren Syntax ist folgendermaßen:\n\ndataset_filtered <- subset(dataset, Sepal.Width > 3)\n\nWir definieren das betreffende Objekt dataset, sowie die Spalte, nach der wir filtern wollen (Sepal.Width) und unsere Bedingung (> 3).\nWir wollen aber noch weiter spezifizieren. Angenommen, wir wollen, dass zusätzlich nur Blumen der Spezies setosa im Datensatz enthalten sind, und wir nur die Spalten Sepal.Width und Sepal.Length betrachten wollen:\n\ndataset_filtered2 <- subset(dataset, Sepal.Width > 3 & Species == \"setosa\",select = c(Sepal.Width,Sepal.Length,Species))\n\nDer obige Befehl hat das Objekt dataset_filtered2 erzeugt, welches die entsprechenden Filterungen beinhaltet.\nWir haben nun also gelernt, wie wir sogenannte Subsets von Dataframes auf Basis von Variablenausprägungen und Spalten erzeugen können, und diese enstprechend in unserem Environment speichern können.\nWenn wir nun allerdings einen Blick in unser Environment werfen, und uns vorstellen, wie dieses nach einer R-Session aussehen könnte, stellt sich vielleicht eine Frage: Wie können wir den Überblick behalten?\n\n\n\nHow the author imagines Environment overload\n\n\nDabei, sowie bei vielen anderen Dingen die mit Data Wrangling zu tun haben, hilft uns das dplyr package. Dazu auf der nächsten Seite mehr."
  },
  {
    "objectID": "5join.html#aufgabe",
    "href": "5join.html#aufgabe",
    "title": "Zusammenhängende Dataframes",
    "section": "Aufgabe",
    "text": "Aufgabe\nJetzt, wo ihr die Theorie von left_join() kennengelernt habt, könnt ihr es auch selbst anwenden für unsere dataframes.\n\nWMC_vals <- WMC_vals %>%\n  mutate(subject.id = as.numeric(subject.id))\n\ndata.joined <- dataset %>% \n  left_join(WMC_vals, by = c(\"id\"= \"subject.id\"))\n\n\n\ndata.joined <- WMC_vals %>% \n  mutate(subject.id = as.numeric(subject.id)) %>% \n  right_join(dataset, by = c(\"subject.id\"= \"id\"))"
  }
]