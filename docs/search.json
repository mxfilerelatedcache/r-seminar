[
  {
    "objectID": "5factorplot.html",
    "href": "5factorplot.html",
    "title": "Umgang mit Faktoren in Plots",
    "section": "",
    "text": "Umgang mit Faktoren in Plots\nIn diesem Abschnitt tauchen wir etwas tiefer ein in den Umgang mit Faktoren in Plots.\nWenn man nur kurz für einen Überblick einen Plot erstellt, ist die Lesbarkeit und Ästhetik weniger wichtig, aber wenn Plots für eine Abgabe oder Publikation gedacht sind, ist das Verhalten von Faktoren in Plots manchmal zum Haare raufen. Daher schauen wir uns schon mal präventiv an, wie man die Darstellung unter Kontrolle bekommt.\n\ndataset %>% \n  ggplot()+\n  geom_col(aes(dmLivingSit, gewissenhaftigkeit), fill = \"#537188\")+\n  theme_minimal()\n\n\n\n\nStandardmäßig sind die Faktoren alphabetisch sortiert - das ist häufig okay, kann aber auch nerven:\n\nManche Faktoren haben eine sinnvollere Reihenfolge (z.B. Monate)\nManchmal sind solche Graphen einfacher zu lesen, wenn die Balken nach Größe sortiert sind (z.B. Beliebheit aller Bundestagsmitglieder)\nDie Kategorie “Sonstiges” hätten wir hier lieber als letztes in der Reihe\n\nUm den Plot für uns schöner zu gestalten, können wir entweder direkt die Daten bearbeiten oder nur im Kontext vom Plot die Daten vorübergehend umändern.\n\nFaktorstufen umbenennen\nAuch wenn es für das Datenhandling in und außerhalb von R besser ist, keine Leerzeichen oder Sonderzeichen in den Faktoren und Faktorstufen einzubauen, wollen wir unseren Plot natürlich lesbar haben.\nÜber fct_recode() können wir die Faktorstufen nach dem Prinzip neu = alt umbenennen. Nicht genannte Stufen werden einfach beibehalten.\nEin netter Trick ist außerdem, lange Labels mit \\n mit einem Zeilensprung vom Überlappen abzuhalten. Dabei kann man \\n einfach “dreist” mitten in den neuen Namen einfügen.\n\n# Welche Stufen gibt es noch mal?\nlevels(factor(dataset$dmLivingSit))\n\n[1] \"eigeneWohnung\"      \"Elternhaus\"         \"PartnerKind\"       \n[4] \"Sonstiges\"          \"WG\"                 \"WohnheimAppartment\"\n[7] \"WohnheimZimmer\"    \n\n# Plot mit ordentlich benannten Faktorstufen\ndataset %>% \n  mutate(dmLivingSit = fct_recode(dmLivingSit,\n                                   \"eigene Wohnung\" = \"eigeneWohnung\",\n                                   \"Partner/Kind\" = \"PartnerKind\",\n                                   \"Wohnheim-\\nAppartment\" = \"WohnheimAppartment\",\n                                   \"Wohnheim-\\nZimmer\" = \"WohnheimZimmer\")) %>% \n  ggplot+\n  geom_bar(aes(dmLivingSit), fill = \"#537188\")+\n  theme_minimal()\n\n\n\n\n\n\nFaktorstufen zusammenfassen\nHäufig möchte man die Datenerfassung besonders sorgfältig umsetzen und fragt viele potenzielle Faktorstufen ab. In der Realität gibt es vielleicht nur wenige Teilnehmende, auf die eine bestimmte Faktorstufe zutrifft, sodass es sich anbietet, einige ähnliche Faktorstufen zusammenzulegen.\n\nIn Abhängigkeit vom Inhalt\nHier setzen wir das mithilfe von fct_collapse() für die beiden Wohnheim-Ausprägungen WohnheimAppartment und WohnheimZimmer um.\n\ndataset %>% \n  mutate(dmLivingSit = fct_collapse(dmLivingSit,\n                                    Wohnheim = c(\"WohnheimAppartment\", \"WohnheimZimmer\"))) %>% \n  ggplot+\n  geom_bar(aes(dmLivingSit), fill = \"#537188\")+\n  theme_minimal()\n\n\n\n\n\n\nIn Abhängigkeit von der Ausprägung\nFür die Idee von “die kleinsten Reste zusammenfassen” gibt es verschiedene fct_lump-Varianten. In dem Beispiel hier bietet es sich eigentlich nicht an, wir demonstrieren es trotzdem.\nBei fct_lump_min() lässt sich über das Argument min = spezifizieren, wie häufig eine bestimmte Ausprägung mindestens vorkommen muss, um durch einen eigenen Balken dargestellt zu werden. Der Rest wird in einem Balken zusammengefasst und standardmäßig als “Other” bezeichnet, was sich über other_level = \"..\" ändern lässt.\n\nsummary(factor(dataset$dmLivingSit))\n\n     eigeneWohnung         Elternhaus        PartnerKind          Sonstiges \n                10                 61                 12                  1 \n                WG WohnheimAppartment     WohnheimZimmer \n                22                  3                  7 \n\ndataset %>% \n  mutate(dmLivingSit = fct_lump_min(dmLivingSit, min = 20, other_level= \"Sonstiges\")) %>% \n  ggplot+\n  geom_bar(aes(dmLivingSit), fill = \"#537188\")+\n  theme_minimal()+\n  ggtitle(\"Ausprägungen ab n>=20\")\n\n\n\n\nÄhnlich fasst fct_lump_prop() die Kategorien nach prozentualen Anteilen zusammen.\n\ndataset %>% \n  mutate(dmLivingSit = fct_lump_prop(dmLivingSit, prop = 0.1, other_level= \"Sonstiges\")) %>% \n  ggplot+\n  geom_bar(aes(dmLivingSit), fill = \"#537188\")+\n  theme_minimal()+\n  ggtitle(\"Ausprägungen ab Anteil von 10%\")\n\n\n\n\nAndersherum können wir mit fct_lump_n() spezifizieren, wie viele Balken wir sehen wollen, also ab dem wievielten der Rest zusammengefasst werden soll (n+1). Dabei werden natürlich nicht einfach die n alphabetisch ersten Balken gewählt, sondern die höchsten.\n\ndataset %>% \n  mutate(dmLivingSit = fct_lump_n(dmLivingSit, n = 4, other_level= \"Sonstiges\")) %>% \n  ggplot+\n  geom_bar(aes(dmLivingSit), fill = \"#537188\")+\n  theme_minimal()+\n  ggtitle(\"Reste zusammenfassen nach k=4\")\n\n\n\n\nZuguterletzt fasst fct_lump_lowfreq() die Ausprägungen mit den geringsten Häufigkeiten automatisch so zusammen, dass “Other” nicht größer als der nächstgröße Balken wird. Wenn man also viele Ausprägungen und nicht das Bedürfnis hat, die Anzahl der Balken genau festzulegen, ist das eine bequeme Option.\n\ndataset %>% \n  mutate(dmLivingSit = fct_lump_lowfreq(dmLivingSit, other_level= \"Sonstiges\")) %>% \n  ggplot+\n  geom_bar(aes(dmLivingSit), fill = \"#537188\")+\n  theme_minimal()+\n  ggtitle(\"Reste so lange zusammenfassen, wie es der niedrigste Balken bleibt\")\n\n\n\n\nAber Vorsicht: Wenn eine Ausprägung so häufig ist, dass die Summe der restlichen immer noch geringer ist, macht diese Funktion wenig Sinn, wie auch in unserem Plot zu erkennen ist.\nDie Beispiele hier haben wir anhand von Balkendiagrammen und der Häufigkeit gezeigt, es funktioniert aber natürlich auch mit anderen geoms und anderen kontinuierlichen Variablen auf der Y-Achse:\n\ndataset %>% \n  mutate(dmLivingSit = fct_lump_prop(dmLivingSit, prop = 0.1, other_level= \"Sonstiges\")) %>% \n  group_by(dmLivingSit) %>% \n  summarize(mean.gw = mean(gewissenhaftigkeit),\n            sd.gw = sd(gewissenhaftigkeit)) %>% \n  ggplot(aes(dmLivingSit, mean.gw))+\n  geom_point(color = \"#537188\", size=3)+\n  geom_errorbar(aes(ymin=mean.gw-sd.gw, ymax=mean.gw+sd.gw), width=0.1, alpha=0.3)+\n  theme_minimal()+\n  ggtitle(\"Ausprägungen ab Anteil von 10%\")\n\n\n\n\n\n\n\nFaktoren umsortieren\nWenn wir mit der Anzahl der Faktoren zufrieden sind, könnte uns aber immer noch die angezeigte, alphabetische Reihenfolge stören. Hier haben wir mehrere Möglichkeiten.\n\nSortieren nach Höhe\nDie Funktion fct_infreq() ist vor allem für Barplots gedacht und sortiert diese nach Höhe.\n\ndataset %>% \n  mutate(dmLivingSit = fct_infreq(dmLivingSit)) %>% \n  ggplot+\n  geom_bar(aes(dmLivingSit), fill = \"#537188\")+\n  theme_minimal()+\n  ggtitle(\"Sortierung nach Höhe, absteigend\")\n\n\n\n\nFür den Fall, dass wir mit der niedrigsten Ausprägung anfangen wollen, können wir ein fct_rev() ergänzen.\n\ndataset %>% \n  mutate(dmLivingSit = fct_rev(fct_infreq(dmLivingSit))) %>% \n  ggplot+\n  geom_bar(aes(dmLivingSit), fill = \"#537188\")+\n  theme_minimal()+\n  ggtitle(\"Sortierung nach Höhe, aufsteigend\")\n\n\n\n\n\n\nIndividuell Sortieren\nMit dem Sortieren nach der Größe haben wir zufällig auch erreicht, dass die “Sonstiges”-Ausprägung endlich am Ende landet. Aber das lässt sich auch gezielt umsetzen, indem wir mittels fct_relevel() genau spezifizieren, in welcher Reihenfolge die Faktorstufen angezeigt werden sollen.\nDabei werden als Argumente neben dem Faktor alle Faktorstufen in der Reihenfolge genannt, in der sie auftauchen sollen: fct_relevel(dmLivingSit, \"Elternhaus\", \"eigeneWohnung\", ...). Über das Argument after= lassen sich alternativ eine oder mehrere Ausprägungen an eine gezielte Stelle platzieren.\n\ndataset %>% \n  mutate(dmLivingSit = fct_relevel(dmLivingSit, \"Sonstiges\", after=Inf)) %>% \n  ggplot+\n  geom_bar(aes(dmLivingSit), fill = \"#537188\")+\n  theme_minimal()+\n  ggtitle(\"Sonstiges am Ende platziert\")\n\n\n\n\n\n\nSortieren nach Ausprägung einer weiteren Variable\nEs ist zwar praktisch, die Faktorstufen individuell sortieren zu können, aber abgesehen von Sonderkategorieren doch eher aufwendig. Besonders umständlich wäre es, wenn wir die Stufen der Größe nach sortieren wollen, aber es bei der Größe um die Ausprägung einer anderen Variable gehen soll. Da müssten wir die Variable erst pro Faktorstufe berechnen, nach Größe sortieren und dann die Reihenfolge übernehmen.\nGlücklicherweise gibt es mit fct_reorder() eine einfachere Möglichkeit: Hier können wir eine Variable spezifizieren, deren Ausprägung die Reihenfolge vorgeben soll.\n\ndataset %>% \n  group_by(dmLivingSit) %>% \n  summarize(mean.gw = mean(gewissenhaftigkeit),\n            sd.gw = sd(gewissenhaftigkeit)) %>% \n  ggplot(aes(x=fct_rev(fct_reorder(dmLivingSit, mean.gw)), mean.gw))+\n  geom_point(color = \"#537188\", size=3)+\n  geom_errorbar(aes(ymin=mean.gw-sd.gw, ymax=mean.gw+sd.gw), width=0.1, alpha=0.3)+\n  theme_minimal()+\n  ggtitle(\"Sortierung in Abhängigkeit von Gewissenhaftigkeit\")\n\n\n\n\nHier in dem Fall taucht die fct-Funktion erst im Plot auf, da nach mean.gw sortiert werden soll und diese Variable erst mal im Kontext von summarize entsteht. Es funktioniert genauso, aber wenn man die Möglichkeit hat, ist es übersichtlicher, sie bei mutate auszuführen."
  },
  {
    "objectID": "5join.html#aufgabe",
    "href": "5join.html#aufgabe",
    "title": "Zusammenhängende Dataframes",
    "section": "Aufgabe",
    "text": "Aufgabe\nJetzt, wo ihr die Theorie von left_join() kennengelernt habt, könnt ihr es auch selbst anwenden für unsere dataframes.\n\nWMC_vals <- WMC_vals %>%\n  mutate(subject.id = as.numeric(subject.id))\n\ndata.joined <- dataset %>% \n  left_join(WMC_vals, by = c(\"id\"= \"subject.id\"))\n\n\n\ndata.joined <- WMC_vals %>% \n  mutate(subject.id = as.numeric(subject.id)) %>% \n  right_join(dataset, by = c(\"subject.id\"= \"id\"))"
  },
  {
    "objectID": "5outlier.html#grafische-ausreißersuche",
    "href": "5outlier.html#grafische-ausreißersuche",
    "title": "Ausreißeranalyse",
    "section": "Grafische Ausreißersuche",
    "text": "Grafische Ausreißersuche\nHier bieten sich zwei Plots an, die ihr schon kennt: Histogramme und Boxplots\n\n# Base\nhist(dataset$dmAge)\n\nboxplot(dataset$dmAge)\n\n\n# ggplot\nggplot(dataset)+\n  geom_histogram(aes(dmAge))\n\nggplot(dataset)+\n  geom_boxplot(aes(dmAge))\n\nIm Boxplot werden potentielle Ausreißer mit den Punkten dargestellt, wenn sie das “IQR Kriterium” überschreiten. IQR steht für interquartile range, also Abstand zwischen dem ersten und zweiten Quantil. Das erste Quantil ist die Zahl, unter dem 25% der Werte liegen und das dritte Quantil die Zahl, über der 25% der Werte liegen. Von diesen Quantilgrenzen aus geht man noch mal weiter um 1.5*IQR in beide Richtungen. Die Werte, die dann immer noch nicht im Bereich liegen, sind die potentiellen Ausreißer."
  },
  {
    "objectID": "5outlier.html#ausreißersuche-mit-base-r",
    "href": "5outlier.html#ausreißersuche-mit-base-r",
    "title": "Ausreißeranalyse",
    "section": "Ausreißersuche mit base R",
    "text": "Ausreißersuche mit base R\nDas IQR-Kriterium lässt sich natürlich nicht nur im Boxplot anwenden, sondern auch selbst berechnen.\n\n# Grenzen bestimmen\nlow <- quantile(dataset$dmAge)[2]-1.5*IQR(dataset$dmAge)\nhigh <- quantile(dataset$dmAge)[4]+1.5*IQR(dataset$dmAge) \n\n# Probanden identifizieren, die Grenzen überschreiten\ndataset %>% \n  filter(dmAge < low | dmAge > high) %>% \n  select(dmAge, everything())\n\nNetterweise gibt es auch eine Funktion zu den Daten hinter einem Boxplot, mit der wir die Werte der potentiellen Ausreißer direkt genannt bekommen:\n\nboxplot.stats(dataset$dmAge)\n\nout <- boxplot.stats(dataset$dmAge)$out\n\nHier werden uns nur leider nicht direkt die Probanden-IDs oder Zeilennummern mitgeliefert. Dabei kann uns aber die Funktion which() und der %in%-Operator helfen, die auch allgemein nützlich sind, immer wenn es darum geht, bestimmte Werte in einer Tabelle wiederzufinden.\n\n# Welche Elemente des Alter-Vektors entsprechen einem der Elemente aus dem out-Vektor?\ndataset$dmAge %in% out\n\n# Welche Zeilen-Indices entsprechen diese Übereinstimmungen?\nwhich(dataset$dmAge %in% out)\n# Ähnlich wie match(): Indices!\n\n# Indices im Datensatz anwenden\ndataset[which(dataset$dmAge %in% out),]\n\nEin weiterer Ansatz kann sein, die Daten mit scale() zu standardisieren und dann zu überprüfen, welche Werte die Grenzen überschreiten, welche gemäß der Standardnormalverteilung wie folgt definiert sind:\n\naußerhalb von -2 oder 2: selten\naußerhalb von -3 oder 3: sehr selten\n\n\ndataset %>% \n  mutate(age.sc = scale(dmAge)) %>%\n  filter(age.sc > 3 | age.sc < -3) %>% \n  select(dmAge, everything())"
  },
  {
    "objectID": "5outlier.html#ausreißersuche-mit-dafür-designten-packages",
    "href": "5outlier.html#ausreißersuche-mit-dafür-designten-packages",
    "title": "Ausreißeranalyse",
    "section": "Ausreißersuche mit dafür designten Packages",
    "text": "Ausreißersuche mit dafür designten Packages\nNeben diesen deskriptiven Ansätzen gibt es auch Signifikanz-Tests für die Ausreißeranalyse, die uns angeben, ob der extremste Wert ein Ausreißer ist oder nicht. Die hier vorgestellten basieren alle auf der Grundannahme, dass die Variable (abgesehen von den Ausreißern) normalverteilt ist.\nDer grubbs.test() kommt aus dem outlier-Package.\n\n# höchsten Wert testen\ngrubbs.test(dataset$dmAge)\n\n# niedrigsten Wert testen\ngrubbs.test(dataset$dmAge, opposite = T)\n\nDer dixon.test() ebenso. Er eignet sich besonders gut für kleine samples mit maximal n = 30.\n\ndata.small <- dataset[1:20,]\n\n# höchsten Wert testen\ndixon.test(data.small$dmAge)\n\n# niedrigsten Wert testen\ndixon.test(data.small$dmAge, opposite=T)\n\nWie man sieht, testen beide Tests immer nur einen Wert auf einmal. Allerdings konnten wir unserem Boxplot ja entnehmen, dass nicht nur der höchste Wert ein potentieller Ausreißer ist. Die Lösung dafür ist entweder, den Test zu wiederholen mit einem Datensatz, in dem der bereits identifizierte Ausreißer entfernt wurde, oder einen alternativen Test zu benutzen: rosnerTest() aus dem EnvStats-Package.\nWichtig zu beachten ist dabei, dass dieser Test sich eher für große Samples (n>20) eignet. Außerdem müssen wir spezifizieren, wie viele potentielle Ausreißer wir erwarten. Standardmäßig werden drei Werte getestet. Wir können uns gut am Boxplot von vorhin orientieren.\n\nrosnerTest(dataset$dmAge, k=7)\n\nrosnerTest(dataset$dmAge, k=7)$all.stats"
  },
  {
    "objectID": "5outlier.html#aufgabe",
    "href": "5outlier.html#aufgabe",
    "title": "Ausreißeranalyse",
    "section": "Aufgabe",
    "text": "Aufgabe\nFührt selbst eine Ausreißeranalyse für eine beliebige andere kontinuierliche Variable aus unserem Datensatz durch.\nZur Übersicht:\n\nPlots mit boxplot, geom_boxplot, hist, geom_histogram\nWertebereich definieren mit quantile, IQR, boxplots.stats\nAusreißer testen mit grubb.test, dixon.test, rosnerTest\nevtl. unterstützende Funktionen: which, %in%"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "analysis1.html",
    "href": "analysis1.html",
    "title": "Skalen mit across()",
    "section": "",
    "text": "Show code\nlibrary(tidyverse)\nlibrary(reshape2)\nlibrary(reactable)\n\npalette1 <- c(\"#648fff\",\"#785ef0\",\"#dc267f\",\"#fe6100\",\"#ffb000\",\"#000000\",\"#ffffff\")\npalette2 <- c(\"#CC79A7\",\"#D55E00\",\"#0072B2\",\"#F0E442\",\"#009E73\",\"#56B4E9\",\"#E69F00\",\"#000000\")"
  },
  {
    "objectID": "analysis1.html#überblick",
    "href": "analysis1.html#überblick",
    "title": "Skalen mit across()",
    "section": "Überblick",
    "text": "Überblick\n\n\n\n\n\nIn der letzten Woche habt ihr kennengelernt, wie wir mit R ganz einfach statistische Verfahren anwenden können, um relevante Fragestellungen zu beantworten. Dabei habt ihr sowohl gelernt, wie ihr einfache deskriptive Analysen macht, als auch komplexere, inferenzstatistische Methoden anwendet. Zuletzt habt ihr dann noch gelernt, wie ihr eure Ergebnisse in einem Bericht “publizierfähig” speichern könnt.\nIn dieser Woche soll es darum gehen, diese Fähigkeiten mit denen aus den vorigen Wochen zu verknüpfen, und gemeinsam an einem konkreten Beispiel zu arbeiten. Dabei wollen wir mit einem Datensatz arbeiten, wie ihr ihn auch erhalten könntet, wenn ihr bspw. mit Limesurvey arbeitet."
  },
  {
    "objectID": "analysis1.html#hintergrund",
    "href": "analysis1.html#hintergrund",
    "title": "Skalen mit across()",
    "section": "Hintergrund",
    "text": "Hintergrund\nZunächst soll es hier aber ein wenig um den Datensatz bzw. die Studie gehen, die wir untersuchen. Der Datensatz ist ein Auszug aus einer Studie, die im Rahmen einer Abschlussarbeit erhoben wurde. Dabei ging es darum, den Einfluss verschiedener Faktoren auf das selbstregulierte Lernen (SRL) während der Corona-Pandemie zu untersuchen.\n\n\n\n\n\nKonkret ging es darum, die empfundenen Herausforderungen beim selbstregulierten Lernen entsprechend zu erfassen und herauszufinden, inwiefern diese durch individuelle Faktorenwie Gewissenhaftigkeit bzw. situative Faktoren wie die Lernumgebung beeinflusst werden.\nBei der Studie handelte es sich um eine Fragebogenstudie mit kleineren Tests, und die abhängige Variable (SRL-Herausforderungen) sowie die unabhängigen Variablen bzw. Prädiktoren (Gewissenhaftigkeit/Lernumgebung) wurden mittels Fragebögen erhoben. Diese wollen wir uns nun kurz anschauen."
  },
  {
    "objectID": "analysis1.html#instrumente",
    "href": "analysis1.html#instrumente",
    "title": "Skalen mit across()",
    "section": "Instrumente",
    "text": "Instrumente\nIm Rahmen der Studie wurden einige Instrumente verwendet, über die es gut ist, ein wenig zu erfahren, bevor wir mit der Auswertung beginnen.\n\nSRL-Herausforderungen\nDer Fragebogen basierte auf einem angepassten Fragebogen zu Herausforderungen beim kollaborativen Lernen Spang, Fett, and Greisel (2020). Er umfasst 96 Items mit 32 Facetten, abgebildet auf einer 5-stufigen Likert Skala von (1 = kein Problem bis 5 = großes Problem). Hier sieht man ein paar Beispiele sowie im Hintergrund die Facetten:\n\n\n\n\n\n\n\nGewissenhaftigkeit\nGewissenhaftigkeit (engl. conscientiousness) wurde erfasst mittels einer Subskala des deutschen Big-Five Inventorys (BFI-2) Danner et al. (2019). Hier gab es 12 Items, die auf einer 6-Punkte Likert Skala von (1 = nicht wahr, 6 = wahr) beantwortet werden konnten. Beispielitems waren:\n\n„Ich bleibe an einer Aufgabe dran, bis sie erledigt ist.”\n„Ich bin eher der chaotische Typ, mache selten sauber.”\n\n\n\nSuboptimale Lernumgebung\nHier bezieht sich Lernumgebung nicht auf eine digitale Lernumgebung wie bspw. Moodle, sondern konkret auf die physische Lernumgebung, die die Lernenden bei sich zuhause haben. Dazu wurden 7 Items entsprechend 3 Dimensionen entwickelt, mit Items wie:\n\nAn meinem Lernplatz bin ich von vielen Umgebungsgeräuschen (bspw. Staßenlärm, Gespräche) umgeben.\nIch habe einen festen Ort, an dem ich lerne.\nDie Umgebungstemperatur an meinem Lernplatz ist oft zu kalt oder zu warm.\n\nWir werden ab jetzt den Fragebogen als suboptimal_study bezeichnen."
  },
  {
    "objectID": "analysis1.html#materialien-methoden",
    "href": "analysis1.html#materialien-methoden",
    "title": "Skalen mit across()",
    "section": "Materialien & Methoden",
    "text": "Materialien & Methoden\nBeginnen wir also mit der Auswertung. Ladet euch dazu im Moodle den Datensatz dataset_srl_challenges.csv herunter und ladet ihn entsprechend in R.\n\ndataset <- read.csv(\"dataset_srl_challenges.csv\")\n\nNun schauen wir uns die Daten an, um ein Gefühl dafür zu bekommen. Mit 238 Spalten bzw. Variablen ist der Datensatz nicht klein, und mag auf den ersten Blick unübersichtlich wirken.\n\nFrage: Welche könnten die relevanten Variablen (siehe oben) sein?\n\nMit etwas Scharfsinn können wir sicherlich ermitteln, welche die für uns relevanten Variablen in diesem Dschungel aus Spalten sind. Generell empfiehlt es sich hier allerdings immer, mit einem Codebook zu arbeiten. Wenn wir mit Limesurvey erheben, ist die einfachste Art und Weise, an ein Codebook zu kommen folgende:\n\n\n\n\n\nDort bekommt ihr eine genaue Übersicht über die in eurem Datensatz auftauchenden Variablen, sowie die dazugehörigen Ausprägungen etc. Hier sei besonders erwähnt, dass Datensätze üblicherweise nicht so eine schöne Form haben, wie wir es bis jetzt gewohnt waren (bspw. numerische Ausprägungen von 1 bis 5). Vielmehr kann es passieren, dass ihr in eurem Datensatz für eine Variable mit einer 5-stufigen Likert Skala folgende Ausprägungen habt:\nDa stellt sich natürlich direkt die Frage, wie die Polarität ist, und wie wir die Variable so umformatieren können, dass wir damit normal rechnen können. Dabei kann die Umfragelogikdatei bzw. ein Codebook sehr helfen. In unserem Fall haben wir leider keine Umfrage-Logikdatei, sondern ein Codebook. Dieses findet ihr ebenfalls im Moodle als codebook.xlsx.\n\nNAs eliminieren\nBevor wir aber an die einzelnen Skalen gehen, wollen wir NAs eliminieren, sofern diese in unserem Datensatz existieren.\n\nFrage: Wie würden wir da vorgehen?\n\n\ndataset <- dataset %>% \n  filter(!is.na(submitdate))\n\n\n\nItems umkodieren\n\nFrage: Welche Items könnten uns noch Probleme bereiten bzw. müssen umkodiert werden, bevor wir starten können?\n\nEin kurzer Blick in die Daten über view() zeigt, dass alle Items des Gewissenhaftigkeitsfragebogens (gh_ etc.) unschön kodiert sind. Um das zu ändern, bieten sich einige Optionen an. Wir wollen hier mit dplyr und case_when arbeiten. Die Syntax sieht wie folgt aus:\n\ndf %>% \n  mutate(new_variable = case_when(new_variable == condition1 ~ new_val1,\n                                  new_variable == condition2 ~ new_val2))\n\nWir sehen, wie wir für jede einzelne Ausprägung von new_variable entscheiden können, wie new_val dann aussehen soll. Hier kann man auch mehrere Bedingungen verknüpfen, bspw. zusätzlich noch Ausprägungen in anderen Variablen berücksichtigen und dann mit & arbeiten.\n\nFrage: Wenn wir dies nun für unsere Variablen machen wollen, wie würden wir vorgehen?\n\nDies für jede einzelne Variable zu machen ist natürlich ganz schön aufwändig. Glücklicherweise fangen alle betreffenden Items mit gh_ an, sodass wir diese über across() und starts_with() auswählen können. Das across() hilft uns hier, mehrere Spalten auszuwählen, in Abhängigkeit von Kriterien (in diesem Fall der Name). Doch was genau macht across()?\n\n\n\n\n\nIm Gegensatz zu mutate() (ohne across) und summarise(), welche quasi Werte über Zeilen hinweg aggregieren, können wir mittels mutate(across()) Werte über Spalten hinweg aggregieren. Das kann sehr hilfreich sein, wenn wir Summenscores bilden wollen. Schaut euch die Syntax hier am besten mal an, und übernehmt sie in eurem Code:\n\ndataset %>% \n  mutate(across(starts_with(\"gh_\"),function(x) case_when(x == \"A01\" ~ 1,\n                                                         x == \"A02\" ~ 2,\n                                                         x == \"A03\" ~ 3,\n                                                         x == \"A04\" ~ 4,\n                                                         x == \"A05\" ~ 5)))\n\nJetzt müssen wir das natürlich noch über <- im Datensatz speichern.\n\n\nShow code\ndataset <- dataset %>% \n  mutate(across(starts_with(\"gh_\"),function(x) case_when(x == \"A01\" ~ 1,\n                                                         x == \"A02\" ~ 2,\n                                                         x == \"A03\" ~ 3,\n                                                         x == \"A04\" ~ 4,\n                                                         x == \"A05\" ~ 5)))\n\n\n\n\nFragebögen prüfen\nBevor wir die Fragebögen verwenden können, müssen wir natürlich erst deren Güte prüfen. Starten wir mit unserem Fragebogen für die Lernumgebung. Die packages psych und sjPlot bieten einige praktische Funktionen, die wir hier direkt anwenden können. Über psych::alpha() bekommen wir für eine Auswahl an Spalten Cronbach’s Αlpha berechnet. Über select() können wir die betreffenden Spalten vorher auswählen.\n\nFrage: Berechnet Cronbachs Alpha für die 7 Items des Lernumgebungsfragebogens (sl_SQ001 etc.). Was fällt auf?\n\n\ndataset %>%\n select(sl_SQ001,\n        sl_SQ002,\n        sl_SQ003,\n        sl_SQ004,\n        sl_SQ005,\n        sl_SQ006,\n        sl_SQ007)%>% psych::alpha()\n\nOh nein! Ein genauerer Blick auf die Items (oder entsprechendes Wissen) hätte verraten, dass manche Items negativ kodiert sind. Das zerschießt uns natürlich jegliches Cronbach’s Alpha. Auch da kann dplyr und case_when helfen.\n\nFrage: Kodiert die Variablen sl_SQ_001 und sl_SQ006 entsprechend um. Die Werte (min-max) findet ihr bspw. über summary(dataset$sl_SQ001) heraus. Kodiert so um, dass auf 1 eine 6 wird, aus 2 eine 5 usw. Hier nochmal die Syntax, wie ihr mittels across() und case_when einzelne Spalten auswählt. Achtet vor allem auf die Klammern!\n\n\ndf %>% \n  mutate(across(c(variable1,variable2), function(x) case_when(x==condition ~ newval)))\n\n\ndataset <- dataset %>% \n  mutate(across(c(sl_SQ001,sl_SQ006),function(x) case_when(x==1 ~ 6,\n                                                         x==2 ~ 5,\n                                                         x==3 ~ 4,\n                                                         x==4 ~ 3,\n                                                         x==5 ~ 2,\n                                                         x==6 ~ 1)))\n\nEin neuer Blick auf Cronbachs α verrät, dass unser Umkodieren etwas bewirkt hat, die Fehlermeldung ist verschwunden und der Wert hat sich verbessert.\n\ndataset %>%\n select(sl_SQ001,\n        sl_SQ002,\n        sl_SQ003,\n        sl_SQ004,\n        sl_SQ005,\n        sl_SQ006,\n        sl_SQ007)%>% psych::alpha()\n\n\n\nExkurs: Itemschwierigkeit etc.\nUm hier nicht zu tief in Test- und Fragebogenkonstruktion abzutauchen, hier nur als kleiner Exkurs: Das package sjPlot kann uns dabei helfen, wichtige Eigenschaften über die Items zu gewinnen, bevor wir Cronbachs α berechnen. Dazu gehören die Itemschwierigkeit (item difficulty) und die Trennschärfe der Items (item discrimination). Der einfache Befehl tab_itemscale() gibt uns gleich ein publizierfähiges Plot:\ndataset %>%\n select(sl_SQ001,\n        sl_SQ002,\n        sl_SQ003,\n        sl_SQ004,\n        sl_SQ005,\n        sl_SQ006,\n        sl_SQ007)%>% sjPlot::tab_itemscale()\n\n\n\nComponent 1\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nα if deleted\n\n\nsl_SQ001\n0.00 %\n2.03\n1.28\n1.41\n0.34\n0.28\n0.55\n\n\nsl_SQ002\n0.00 %\n2.7\n1.4\n0.59\n0.45\n0.33\n0.53\n\n\nsl_SQ003\n0.00 %\n3.91\n1.43\n-0.08\n0.65\n0.40\n0.51\n\n\nsl_SQ004\n0.00 %\n2.93\n1.53\n0.5\n0.49\n0.23\n0.57\n\n\nsl_SQ005\n0.00 %\n2.75\n1.44\n0.63\n0.46\n0.41\n0.50\n\n\nsl_SQ006\n0.00 %\n4.87\n1.7\n-1.3\n0.81\n0.19\n0.59\n\n\nsl_SQ007\n0.00 %\n2.22\n1.4\n1.11\n0.37\n0.30\n0.54\n\n\nMean inter-item-correlation=0.170 · Cronbach's α=0.581\n\n\n\n\nWir können sehen, dass die Itemschwierigkeit bei den meisten Items nahe 0.50 ist, und die Trennschärfe auch nicht zu klein (bis auf vereinzelte Items wie sl_sq006.\nDurch eine exploratorische Faktoranalyse, welche wir hier nicht rechnen werden, wurde herausgefunden, dass nicht alle Items unser Konzept suboptimale Lernumgebung gut vorhersagen. Daher haben wir uns entschieden, die folgenden Items zu nehmen: sl_SQ002, sl_SQ003, sl_SQ005 und sl_SQ007. Diese bilden einen Faktor ab, weshalb es nun auch Sinn ergibt hierfür Cronbachs α zu berechnen:\n\ndataset %>% \n  select(sl_SQ002,\n  sl_SQ003,\n  sl_SQ005,\n  sl_SQ007)%>%\npsych::alpha()\n\nDieses ist mit 0.58 zwar immer noch nicht sonderlich gut, im Rahmen dieser Zufallsstichprobe allerdings OK. Wir wollen diese Items nun zusammenfassen, da wir lieber mit einem Wert für suboptimal_study arbeiten wollen. Dazu gibt es zwei Wege:\n\n\nSumscores mit across() und rowMeans()\nAcross() habt ihr oben bereits kennengelernt. In der Kombination mit der base-R rowMeans() Funktion können wir es nutzen, um Summen bzw. Meanscores für unsere Items zu bilden:\n\ndf %>% \n  mutate(new_variable = rowMeans(across(c(variable1,variable2))))\n\nKonkret sieht das dann so aus:\n\ndataset <- dataset %>% \n  mutate(suboptimal_study = rowMeans(across(c(sl_SQ002,sl_SQ003,sl_SQ005,sl_SQ007))))\n\n\n\nSumscores mit rowwise() und mean()\nUm Funktionen nicht mehr per Spalte, sondern per Zeile anzuwenden, bietet sich auch die Kombination aus rowwise() und Funktionen wie bspw. mean() an. Dabei sieht die Syntax wie folgt aus:\n\ndf %>% \n  rowwise()\n  mutate(new_variable = mean(c(variable1, variable2)))\n\nWenden wir das also auf unseren Datensatz an:\n\ndataset %>% \n  rowwise() %>% \n  mutate(suboptimal_study = mean(c(sl_SQ002,sl_SQ003,sl_SQ005,sl_SQ007))) %>% select(suboptimal_study)\n\n\n\n  \n\n\n\nWie zu sehen, erhalten wir dieselben Mittelwerte für unsere Skala, wie oben."
  },
  {
    "objectID": "analysis1.html#aufgabe-subskalen-für-gewissenhaftigkeit",
    "href": "analysis1.html#aufgabe-subskalen-für-gewissenhaftigkeit",
    "title": "Skalen mit across()",
    "section": "Aufgabe: Subskalen für Gewissenhaftigkeit",
    "text": "Aufgabe: Subskalen für Gewissenhaftigkeit\nNun wollen wir auch die Subskalen nach dem oben gezeigten Prinzip für unser Konstrukt Gewissenhaftigkeit prüfen. Dazu haben wir alles was wir brauchen:\n\nDatensatz\nCodebook mit Variablennnamen und Labels\nLiteraturangabe Danner et al. (2019)\n\nMit Items sowie dazugehörigen Dimensionen\nNamen der negativ-kodierten Items\n\n\nNutzt also die Informationen und eure neu-erlenten Skills, um nun die entsprechenden Subskalen (plot-twist: es sind mehr als eine) zu berechnen (als Mittelwerte). Gebt auch Cronbach’s α für die einzelnen Subskalen an.\n\nwrite.csv(dataset, \"dataset_srl_challenges_wrangled.csv\")"
  },
  {
    "objectID": "analysis2.html",
    "href": "analysis2.html",
    "title": "Wide/long Format",
    "section": "",
    "text": "Show code\nlibrary(tidyverse)\nlibrary(reshape2)\nlibrary(reactable)\n\npalette1 <- c(\"#648fff\",\"#785ef0\",\"#dc267f\",\"#fe6100\",\"#ffb000\",\"#000000\",\"#ffffff\")\npalette2 <- c(\"#CC79A7\",\"#D55E00\",\"#0072B2\",\"#F0E442\",\"#009E73\",\"#56B4E9\",\"#E69F00\",\"#000000\")\n\ndataset <- read.csv(\"dataset_srl_challenges_wrangled.csv\")"
  },
  {
    "objectID": "analysis2.html#deskriptive-analysen",
    "href": "analysis2.html#deskriptive-analysen",
    "title": "Wide/long Format",
    "section": "Deskriptive Analysen",
    "text": "Deskriptive Analysen\nWir haben nun also den Datensatz bereinigt, Items zu Summen- bzw. meanscores zusammengefasst, sowie deren interne Konsistenz berichtet. Super! Der nächste Schritt in unserer Analyse wäre üblicherweise, mittels deskriptiver Analysen eine Idee von unseren Daten zu bekommen. Das machen wir gleich mal:\n\nStichprobe\nNatürlich haben wir die Basis-Informationen zu unserer Stichprobe, wie Alter, Geschlecht, etc.\n\nFrage: Wie können wir das gut berichten?\n\n\ndataset %>% \n  group_by(dmSex) %>% \n  summarise(age=mean(dmAge),n=n()) %>% \n  mutate(rel=paste0(round(n/sum(n)*100,2),\"%\"))\n\n\n\n  \n\n\n\nFalls wir unseren Output direkt in einem publikationsfähigen Table haben wollen, können wir dies mit stargazer() tun:\ndataset %>% \n  group_by(dmSex) %>% \n  summarise(age=round(mean(dmAge)),n=n()) %>% \n  stargazer::stargazer(type = \"html\",summary = F)\n\n\n\n\n\n\n\n\n\n\ndmSex\n\n\nage\n\n\nn\n\n\n\n\n\n\n\n\n1\n\n\ndivers\n\n\n18\n\n\n1\n\n\n\n\n2\n\n\nfemale\n\n\n22\n\n\n82\n\n\n\n\n3\n\n\nmale\n\n\n23\n\n\n33\n\n\n\n\n\n\n\n\n\n\nFragebögen\nIn wissenschaftlichen Arbeiten ist es üblich, immer auch gleich die mean() bzw. sd() values mitzuberichten. Machen wir das also für unsere eben gebildeten Skalen.\n\n\nSuboptimal Study\n\ndataset %>% \n  summarise(across(c(sl_SQ002,\n                     sl_SQ003,\n                     sl_SQ005,\n                     sl_SQ007),\n                   list(mean=mean,sd=sd)),\n            total_mean=mean(suboptimal_study),\n            total_sd=sd(suboptimal_study))\n\n\n\n  \n\n\n\n\n\nGewissenhaftigkeit\n\ndataset %>% \n  summarise(across(c(gh_ordnungsliebe,\n  gh_fleiss,\n  gh_verlaesslichkeit),list(mean=mean,sd=sd)),total_m=mean(gewissenhaftigkeit),total_sd=sd(gewissenhaftigkeit))\n\n\n\n  \n\n\n\n\n\nSRL-Probleme\nUnser 90-Items Fragebogen, zu dem wir glücklicherweise schon die Meanscores haben, bietet sich aufgrund der Größe eher weniger zur numerischen Darstellung an. Hier wollen wir lieber visuell die Werte zeigen. Doch wie machen wir das? Ein kurzer Blick in den Datensatz verrät, dass es einige Subskalen gibt:\n\ndataset %>% \n  select(starts_with(\"P\",ignore.case=F),-Probleme_gesamt)\n\n\n\n  \n\n\n\nAngenommen, wir wollten aber nun die Mittelwerte sowie Standardabweichungen (bspw. über geom_bar() und geom_errorbar()) für alle diese Spalten berechnen - wie würden wir da vorgehen? Das leitet zu einem weiteren wichtigen Konzept über: Das long vs. wide Format von Daten."
  },
  {
    "objectID": "analysis2.html#reshaping-data",
    "href": "analysis2.html#reshaping-data",
    "title": "Wide/long Format",
    "section": "Reshaping data",
    "text": "Reshaping data\nNormalerweise sind Daten in einem Format angeordnet, das dem folgendem entspricht:\n\n\n\n\n  \n\n\n\n\nFrage: Wenn wir nun allerdings die Variablen variable1, variable2 und variable3 in einem Plot darstellen wollten, was wäre das Problem?\n\n\ndf %>% \n  ggplot()+\n  geom_bar(aes(x=?,y=?))\n\nDas Problem ist, dass die Variablen als einzelne Spalten vorhanden sind, das hilft uns bei der Visualisierung mit ggplot nicht weiter. Die Daten befinden sich im long Format. Für viele Analysen bietet sich aber das wide Format viel besser an. Hier ein Beispiel, wie das wide Format im oben genannten Dataframe aussieht:\n\n\n\n\n  \n\n\n\nSomit können wir viel einfacher mit ggplot weiterarbeiten:\n\ndf %>% \n  tidyr::pivot_longer(cols=variable1:variable3) %>% \n  ggplot() +\n  geom_boxplot(aes(x=name,y=value,fill=name))+\n  scale_fill_manual(values=palette1)+\n  theme_classic()\n\nDieses sogenannte “schmelzen” des Dataframes ermöglicht uns die Funktion pivot_longer aus dem tidyr Package. Dessen Syntax ist relativ einfach:\n\ndf %>% \n  tidyr::pivot_longer(cols=variable1:variable3)\n\nHierbei bleibt die Variable, die nicht bei cols= übergeben wird, automatisch als ID Variable.\nMachen wir das also mit unserem Datensatz, um die verschiedenen Subskalen für Probleme zu visualisieren:\n\ndataset %>% \n  select(id,starts_with(\"P\",ignore.case=F),-Probleme_gesamt) %>% \n  tidyr::pivot_longer(cols=P_Verständnis:P_Ressouren_Technik)\n\n\n\n  \n\n\n\nNun können wir ganz einfach weiterarbeiten mit ggplot.\n\nFrage: Probieren wir doch mal ein Violin-Plot aus!\n\n\ndataset %>% \n  select(starts_with(\"P\",ignore.case=F),-Probleme_gesamt) %>% \n  tidyr::pivot_longer(cols=P_Verständnis:P_Ressouren_Technik) %>% \n  ggplot()+\n  geom_violin(aes(x=name,y=value,fill=name))+\n  theme_classic()+\n  scale_fill_manual(values=palette2)+\n  scale_x_discrete(guide = guide_axis(angle = 45))"
  },
  {
    "objectID": "analysis2_1.html",
    "href": "analysis2_1.html",
    "title": "Apply & Multiple data sources",
    "section": "",
    "text": "library(tidyverse)\n\ndataset <- read.csv(\"dataset_srl_challenges_wrangled.csv\")"
  },
  {
    "objectID": "analysis2_1.html#working-memory-operation-span-ospan",
    "href": "analysis2_1.html#working-memory-operation-span-ospan",
    "title": "Apply & Multiple data sources",
    "section": "Working Memory Operation Span (OSPAN)",
    "text": "Working Memory Operation Span (OSPAN)\nMit dem OSPAN Test können wir die Arbeitsgedächtnisleistung von Proband:innen testen. Dazu gibt es ein Tool von Forscher:innen aus Indiana, welches über HTML & Javascript läuft und womit wir die Arbeitsgedächtnisleistung testen können. Der Test sieht wie folgt aus:\n\n\n\n\n\nEr läuft im Browser, und nach Abschluss der Studie haben wir die Ergebnisse der einzelnen Proband:innen als einzelne .csv Dateien auf dem Server:\n\n\n\n\n\nKopiert also nun die aus Moodle heruntergeladenen 40 .csv Dateien in den Ordner assets/datasets/ospan/, wobei euer dataset_srl_challenges_wrangled.csv auf der obersten Ebene liegt (also auf der gleichen Ebene wie der Ordner assets)."
  },
  {
    "objectID": "analysis2_1.html#mit-multiplen-daten-umgehen",
    "href": "analysis2_1.html#mit-multiplen-daten-umgehen",
    "title": "Apply & Multiple data sources",
    "section": "Mit multiplen Daten umgehen",
    "text": "Mit multiplen Daten umgehen\nWir beginnen also damit, uns einen Überblick über die einzelnen Dateien zu machen, welche sich unter assets/datasets/ospan/ befinden. Natürlich müssen wir diese Dateien nicht einzeln als Liste übertragen, sondern können uns diese ganz einfach auflisten mit list.files().\n\ntests_list <- list.files(\"assets/datasets/ospan/\")\n\nWir sehen nun, wenn wir uns das erzeugte Objekt tests_list anschauen, dass dieses unsere 40 Tests enthält:\n\ntests_list\n\n [1] \"WM_operation_span_101_1612186377273.csv\"\n [2] \"WM_operation_span_106_1612359256107.csv\"\n [3] \"WM_operation_span_107_1612524224258.csv\"\n [4] \"WM_operation_span_15_1610368013780.csv\" \n [5] \"WM_operation_span_20_1610447300494.csv\" \n [6] \"WM_operation_span_23_1610461666072.csv\" \n [7] \"WM_operation_span_24_1610465426790.csv\" \n [8] \"WM_operation_span_27_1610465354156.csv\" \n [9] \"WM_operation_span_31_1610465797553.csv\" \n[10] \"WM_operation_span_36_1610536583499.csv\" \n[11] \"WM_operation_span_39_1610544645563.csv\" \n[12] \"WM_operation_span_40_1610544829023.csv\" \n[13] \"WM_operation_span_43_1610551510486.csv\" \n[14] \"WM_operation_span_47_1610623434127.csv\" \n[15] \"WM_operation_span_48_1610623346034.csv\" \n[16] \"WM_operation_span_5_1610357167520.csv\"  \n[17] \"WM_operation_span_51_1610623724516.csv\" \n[18] \"WM_operation_span_52_1610630827066.csv\" \n[19] \"WM_operation_span_54_1610637379465.csv\" \n[20] \"WM_operation_span_55_1610638115446.csv\" \n[21] \"WM_operation_span_6_1610357158395.csv\"  \n[22] \"WM_operation_span_62_1610703127236.csv\" \n[23] \"WM_operation_span_64_1610713861384.csv\" \n[24] \"WM_operation_span_65_1610714043568.csv\" \n[25] \"WM_operation_span_66_1610714141398.csv\" \n[26] \"WM_operation_span_67_1610717257175.csv\" \n[27] \"WM_operation_span_68_1610716883738.csv\" \n[28] \"WM_operation_span_70_1610724891439.csv\" \n[29] \"WM_operation_span_71_1610962127977.csv\" \n[30] \"WM_operation_span_73_1610965288157.csv\" \n[31] \"WM_operation_span_76_1611048167607.csv\" \n[32] \"WM_operation_span_78_1611048577537.csv\" \n[33] \"WM_operation_span_79_1611055497570.csv\" \n[34] \"WM_operation_span_80_1611055678962.csv\" \n[35] \"WM_operation_span_84_1611134539309.csv\" \n[36] \"WM_operation_span_90_1611228773344.csv\" \n[37] \"WM_operation_span_91_1611228344338.csv\" \n[38] \"WM_operation_span_94_1611570780638.csv\" \n[39] \"WM_operation_span_96_1611581289527.csv\" \n[40] \"WM_operation_span_97_1611653179186.csv\" \n\n\n\nFrage: Wie würden wir nun vorgehen? Wie können wir alle Dateien aus dieser Liste auf einmal importieren?\n\nSicherlich kommen der einen oder dem anderen Informatiker:in nun Schleifen in den Sinn, die wir vielleicht aus anderen Programmiersprachen kennengelernt haben. Dazu gehören etwa while oder for . Solche Schleifen spielen im normalen R-Gebrauch nicht so eine große Rolle, daher werden wir hier nicht auf die genaue Syntax eingehen. Hier nur kurz, um es zu zeigen:\n\nfor (test in tests_list){\n  print(test)\n}\n\nNun könnten wir natürlich auf die Idee kommen, über eine for Schleife alle Dateien aus der Liste einzulesen, und diese als einzelne Objekte in unserem Environment zu speichern:\n\nfor (test in tests_list){\n  test <- read.csv(paste0(\"assets/datasets/ospan/\",test))\n}\n\n\nFrage: Was passiert nun? Klappt der Code?\n\nZwar sieht der obige Code aus, als würde er theoretisch klappen, allerdings tut er das nicht. Es werden nicht dynamisch neue Objekte im Environment erzeugt, sondern lediglich eines, welches test heißt und immer wieder überschrieben wird."
  },
  {
    "objectID": "analysis2_1.html#sind-loops-eine-gute-idee",
    "href": "analysis2_1.html#sind-loops-eine-gute-idee",
    "title": "Apply & Multiple data sources",
    "section": "Sind Loops eine gute Idee?",
    "text": "Sind Loops eine gute Idee?\nGenerell bietet es sich beim Programmieren natürlich immer an, Loops zu verwenden. Doch bei R sieht das etwas anders aus. Loops führen dort zu schlechter lesbaren Code, zu Variablen, die im globalen Environment gespeichert werden und so weiter. Doch wie lautet die Lösung? apply!"
  },
  {
    "objectID": "analysis2_1.html#listen-erstellen-mit-lapply",
    "href": "analysis2_1.html#listen-erstellen-mit-lapply",
    "title": "Apply & Multiple data sources",
    "section": "Listen erstellen mit lapply()",
    "text": "Listen erstellen mit lapply()\nWenn wir also nicht so leicht einzelne Objekte im Environment erzeugen können oder wollen, bietet es sich an, diese in einer Liste zu speichern. Da kommt lapply() sehr gelegen. Wir übergeben der Funktion eine Liste oder einen Vektor, und bekommen eine Liste der gleichen Länge zurück.\n\nRepetition: Listen\n\nFrage: Was waren nochmal Listen und wie unterscheiden sie sich von Vektoren?\n\nGenau, in Listen können wir mehr Datentypen, sowie auch verschiedene Datentypen unterbringen. Während folgender Code zwar funktioniert (aber alle übergebenen Elemente in character umwandelt):\n\nvec <- c(\"Das\",1,\"geht so\",1.4,\"aber nicht.\")\n\nWürde folgender Code die Elemente in ihrem usprünglichen Datencode belassen:\n\nsample_list <- list(1,\"f\",12,TRUE)\n\nsample_list[4]\n\n[[1]]\n[1] TRUE\n\n\nDas Resultat sind unterschiedliche Datentypen:\n\nprint(paste0(\"vec is \", typeof(vec),\" and sample_list is \",typeof(sample_list)))\n\n[1] \"vec is character and sample_list is list\"\n\n\n\nWas ist, wenn ich nun auf das zweite Objekt der Liste zugreifen will?\n\nÜber [] bekommen wir das Objekt an der Stelle, aber als Liste. Das Objekt selbst bekommen wir über [[]]. Beachte: Bei R fangen wir nicht bei 0, sondern bei 1 an zu zählen!\n\nsample_list[[3]]\n\n[1] 12\n\n\nWie hilft uns das jetzt bei unserer Fragestellung? Schauen wir uns folgende zwei Beispiel-Dataframes erneut an:\n\nrandom_dataframe1 <- data.frame(\n  ID = seq(1, 10),\n  Age = sample(18:65, 10, replace = TRUE),\n  Score = runif(10, min = 0, max = 100)\n)\nrandom_dataframe2 <- data.frame(\n  ID = seq(1, 10),\n  Age = sample(18:65, 10, replace = TRUE),\n  Score = runif(10, min = 0, max = 100)\n)\n\nlist_of_dfs <- list(random_dataframe1,random_dataframe2)\n\nWir können beide dataframes in einer Liste (list_of_dfs) zusammenfassen!\n\nlist_of_dfs\n\n[[1]]\n   ID Age    Score\n1   1  52 75.84595\n2   2  25 21.64079\n3   3  43 31.81810\n4   4  24 23.16258\n5   5  59 14.28000\n6   6  26 41.45463\n7   7  36 41.37243\n8   8  53 36.88455\n9   9  31 15.24447\n10 10  34 13.88061\n\n[[2]]\n   ID Age     Score\n1   1  58 37.446278\n2   2  27 66.511519\n3   3  40  9.484066\n4   4  44 38.396964\n5   5  24 27.438364\n6   6  44 81.464004\n7   7  49 44.851634\n8   8  55 81.006435\n9   9  42 81.238951\n10 10  51 79.434232\n\n\nDas heißt, wir könnten mit lapply() die Liste aus Variablennamen nutzen, um diese zu importieren. Dabei hilft uns paste0, um den Pfad entsprechend anzupassen:\n\ntests_list_with_path <- paste0(\"assets/datasets/ospan/\",tests_list)\n\nhead(tests_list_with_path)\n\n[1] \"assets/datasets/ospan/WM_operation_span_101_1612186377273.csv\"\n[2] \"assets/datasets/ospan/WM_operation_span_106_1612359256107.csv\"\n[3] \"assets/datasets/ospan/WM_operation_span_107_1612524224258.csv\"\n[4] \"assets/datasets/ospan/WM_operation_span_15_1610368013780.csv\" \n[5] \"assets/datasets/ospan/WM_operation_span_20_1610447300494.csv\" \n[6] \"assets/datasets/ospan/WM_operation_span_23_1610461666072.csv\" \n\n\nUnd nun könnten wir eine Liste mit den eingelesenen Dataframes erstellen:\n\nospan_tests <- lapply(tests_list_with_path, read.csv)\n\nWow! Das erspart uns eine Menge Arbeit."
  },
  {
    "objectID": "analysis2_1.html#funktionen-und-lapply",
    "href": "analysis2_1.html#funktionen-und-lapply",
    "title": "Apply & Multiple data sources",
    "section": "Funktionen und lapply()",
    "text": "Funktionen und lapply()\nWir haben soeben mit lapply und der read.csv() Funktionen alle 40 Dataframes des OSPAN Tests eingelesen und in der Liste ospan_tests gespeichert.\n\nWas fällt bei einem Blick auf die einzelnen Dataframes auf?\n\nEin Blick auf die 40 einzelnen Dataframes zeigt, dass diese immer gleich aufgebaut sind. Bei jedem Dataframe sind eventuell noch weitere Bearbeitungsschritte notwendig, und uns fehlt auch noch eine Zuordnung über die Proband:innen IDs. Das von uns verwendete read.csv() ist eine built-in Funktion. Hier bietet sich also eine Funktion an, bei der wir selbst entscheiden können, was mit den jeweiligen Dataframes passiert. Das geht natürlich auch mit R! Doch wie genau funktionieren überhaupt Funktionen in R?\nFunktionen sind in R nach folgendem Prinzip aufgebaut:\n\nexample_function <- function(value){\n  output <- do_something(value)\n  return(output)\n}\n\nWir definieren sie also normal wie auch Objekte in unserem Environment, definieren wie sie mit erhaltenen Objekten umgeht (value), und was sie damit machen soll (do_something()). Über return(output) können wir den output übergeben. Die Funktionsweise wird am besten mit Hilfe eines minimal-working examples deutlich:\n\ncustom_function <- function(object){\n  output <- object*55\n  return(output)\n}\n\n\ncustom_function(10)\n\n[1] 550\n\n\nSo weit so gut! Alles, was wir also in function() drinstehen haben, können wir also beliebig oft wiederholen.\nSchauen wir uns also an, wie wir mit diesem neuen Wissen unser Problem weiter angehen können. Für eine komplette Lösung unseres Working-Memory Input Problems haben wir zwei (bzw. drei) Challenges:\n\nDaten importieren\nZuordnung zu Proband:innen erreichen\nDatentransformationen (i.e., wmc_PCU erstellen)\n\nEine Sache, die wir noch lernen müssen, hat mit der Zuordnung der Proband:innen und dem Dateinamen zu tun.\n\nWo könnte sich dieser verstecken und was kann dabei helfen?\n\nGenau, unser WMC-Test Tool hat die Datei entsprechend mit dem Zuordnungscode abgespeichert, bspw. so: WM_operation_span_101_1612186377273.csv. Der Teil, der uns interessiert, ist 101.\n\nWie könnte man daran kommen?\n\nDabei kann uns str_split() aus stringr:: helfen (ist in tidyverse). Dessen Syntax sieht wie folgt aus:\n\nstr_split(c(\"split_this_string\",\"split_this_string\"),\"_\")[[1]][3]\n\nWobei wir zuerst den String übergeben, und dann das Zeichen bzw. Muster, nach dem gesplittet werden soll. In unserem Fall ist es das _:\n\nstr_split(\"WM_operation_span_101_1612186377273.csv\",\"_\")\n\n[[1]]\n[1] \"WM\"                \"operation\"         \"span\"             \n[4] \"101\"               \"1612186377273.csv\"\n\n\nLeider übergibt str_split() eine Liste von Vektoren. Das kann manchmal verwirren, ist aber nicht weiter schlimm. Ein Blick auf das Datenobjekt kann immer helfen:\n\n\n\n\n\nÜber [[1]] müssen wir also zunächst auf das erste Objekt der Liste zugreifen.\n\nstr_split(tests_list[1],\"_\")[[1]][4]\n\n[1] \"101\"\n\n\nDann können wir direkt danach das vierte Objekt des darin enthaltenen Vektors auswählen, also so:\n\nstr_split(\"WM_operation_span_101_1612186377273.csv\",\"_\")[[1]][4]\n\n[1] \"101\"\n\n\nNun haben wir den Zuordnungscode! Wir können nun alles vorbereiten, um unseren wmc_PCU Wert aus dem Dataframe zu extrahieren. Fangen wir also zunächst mit einem einzelnen Dataframe an, und überführen das dann später in eine Funktion und wenden das mit lapply() auf alles an (so eine Vorgehensweise ist immer ratsam).\n\nsample_filename <- tests_list[1]\nsubject_id <- str_split(sample_filename,\"_\")[[1]][4]\nfilename_with_path <- paste0(\"assets/datasets/ospan/\",sample_filename)\n\n\ndf <- read.csv(filename_with_path)\n\ndf <- df %>%\n      filter(!is.na(set_size)) %>%\n      filter(!is.na(accuracy)) %>%\n      mutate(accuracy = as.numeric(accuracy)) %>% \n      slice(6:17) %>%\n      mutate(wmc_PCU = accuracy/set_size) %>% \n      summarise(wmc_PCU = mean(wmc_PCU))\n\ndf <- cbind(subject_id,df)\n\ndf\n\n\n\n  \n\n\n\nPerfekt! Wir haben nun ein Dataframe, das den wmc_PCU Mittelwert, sowie die dazugehörige subject_id enthält. Jetzt müssen wir nur noch den entsprechenden Code in eine Funktion packen, und schön können wir es mit lapply() auf unsere gesamte Liste anwenden:\n\nreadAndWrangleWMCs <- function(object){\n  filename <- object\n  subject_id <- str_split(filename,\"_\")[[1]][4]\n  filename_with_path <- paste0(\"assets/datasets/ospan/\",filename)\n\n  df <- read.csv(filename_with_path)\n\n  df <- df %>%\n        filter(!is.na(set_size)) %>%\n        filter(!is.na(accuracy)) %>%\n        mutate(accuracy = as.numeric(accuracy)) %>% \n        slice(6:17)%>%\n        mutate(wmc_PCU = accuracy/set_size) %>%\n        summarise(wmc_PCU = mean(wmc_PCU))\n  \n  df <- cbind(subject_id,df)\n\n  return(df)\n}\n\nMachen wir nun einen kleinen Test mit dem ersten Objekt aus tests_list:\n\nreadAndWrangleWMCs(tests_list[1])\n\n\n\n  \n\n\n\nPerfekt, das scheint zu funktionieren! Nun können wir also die Funktion auf den ganzen Vektor aus Dateinamen tests_list anwenden mit lapply(). Zurück bekommen wir dann eine Liste:\n\ndfs_list <- lapply(tests_list,readAndWrangleWMCs)\n\nNun haben wir noch das Problem, dass wir eine Liste mit 40 Dataframes haben, obwohl wir am liebsten ein großes Dateframe mit den entsprechenden Werten hätten. Dabei kann uns dplyr’s bind_rows() helfen.\n\nbind_rows(dfs_list)\n\n\n\n  \n\n\n\nNun noch einem Objekt zuweisen, und wir sind bereit, es mit unserem vorhandenen Datensatz zu verbinden!\n\nWMC_vals <- bind_rows(dfs_list)\n\nWie das geht, erfahrt ihr nun."
  },
  {
    "objectID": "analysis3.html",
    "href": "analysis3.html",
    "title": "Inferenzstatistische Analyse",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggpubr)\nlibrary(rstatix)\n\ndataset <- read.csv(\"dataset_srl_challenges_wrangled.csv\")"
  },
  {
    "objectID": "analysis3.html#inferenzstatistische-analyse",
    "href": "analysis3.html#inferenzstatistische-analyse",
    "title": "Inferenzstatistische Analyse",
    "section": "Inferenzstatistische Analyse",
    "text": "Inferenzstatistische Analyse\nJetzt, wo die Daten vorbereitet sind und wir sie besser kennen, können wir uns mit unserer eigentlichen Fragestellung beschäftigen: Werden die empfundenen Herausforderungen von Persönlichkeits- und Umgebungsfaktoren beeinflusst?\nIm Detail handelt es sich bei den Herauforderungen um:\n\nMetakognition P_Metakognition\nMotivation P_Motivation\nVerständnis P_Verständnis\nProbleme insgesamt Probleme_gesamt\n\nAls Persönlichkeitsfaktor schauen wir uns die Gewissenhaftigkeit der Big Five an.\nFür die Umgebungsfaktoren haben wir einen Gesamtscore berechnet, der Ablenkbarkeit, Getrenntheit und Komfort berücksichtigt.\nKonkret wollen wir untersuchen, inwiefern die einzelnen Herausforderungen von Gewissenhaftigkeit, Umgebung und deren Interaktion beeinflusst werden.\nWas rechnen wir dafür?\nWelche Voraussetzungen müssen wir testen?\n\nFührt die Regressionsanalysen durch und testet vorher auch die Voraussetzungen.\n\n\nVoraussetzung Multikollinearität\nBei Regressionen ist es wichtig, dass die einzelnen Prädiktoren nicht zu hoch miteinander korrelieren.\n\ndataset %>% \n  cor_test(suboptimal_study, gewissenhaftigkeit)\n\n# A tibble: 1 × 8\n  var1             var2           cor statistic      p conf.low conf.high method\n  <chr>            <chr>        <dbl>     <dbl>  <dbl>    <dbl>     <dbl> <chr> \n1 suboptimal_study gewissenhaf… -0.22     -2.39 0.0187   -0.385   -0.0372 Pears…\n\n\n\n\nRegressionen\n\n# AV 1\nlm1 <- lm(P_Metakognition ~ suboptimal_study*gewissenhaftigkeit, dataset)\nsummary(lm1)\n\n\nCall:\nlm(formula = P_Metakognition ~ suboptimal_study * gewissenhaftigkeit, \n    data = dataset)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.12256 -0.43710 -0.06016  0.38035  1.40455 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                          3.59869    0.79162   4.546 1.39e-05 ***\nsuboptimal_study                     0.11141    0.23430   0.475   0.6354    \ngewissenhaftigkeit                  -0.58172    0.22579  -2.576   0.0113 *  \nsuboptimal_study:gewissenhaftigkeit  0.02056    0.06884   0.299   0.7657    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5476 on 112 degrees of freedom\nMultiple R-squared:  0.3611,    Adjusted R-squared:  0.3439 \nF-statistic:  21.1 on 3 and 112 DF,  p-value: 6.621e-11\n\n# AV 2\nlm2 <- lm(P_Motivation ~ suboptimal_study*gewissenhaftigkeit, dataset)\nsummary(lm2)\n\n\nCall:\nlm(formula = P_Motivation ~ suboptimal_study * gewissenhaftigkeit, \n    data = dataset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3312 -0.7064 -0.1931  0.5885  2.6584 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(>|t|)  \n(Intercept)                          2.70995    1.27394   2.127   0.0356 *\nsuboptimal_study                     0.02345    0.37706   0.062   0.9505  \ngewissenhaftigkeit                  -0.31227    0.36336  -0.859   0.3920  \nsuboptimal_study:gewissenhaftigkeit  0.03558    0.11079   0.321   0.7487  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8812 on 112 degrees of freedom\nMultiple R-squared:  0.05395,   Adjusted R-squared:  0.02861 \nF-statistic: 2.129 on 3 and 112 DF,  p-value: 0.1006\n\n# AV 3\nlm3 <- lm(P_Verständnis ~ suboptimal_study*gewissenhaftigkeit, dataset)\nsummary(lm3)\n\n\nCall:\nlm(formula = P_Verständnis ~ suboptimal_study * gewissenhaftigkeit, \n    data = dataset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5728 -0.6467 -0.1868  0.6031  2.1289 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                           5.6631     1.2232   4.630 9.92e-06 ***\nsuboptimal_study                     -0.8490     0.3621  -2.345  0.02078 *  \ngewissenhaftigkeit                   -1.0957     0.3489  -3.140  0.00216 ** \nsuboptimal_study:gewissenhaftigkeit   0.3121     0.1064   2.934  0.00406 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8461 on 112 degrees of freedom\nMultiple R-squared:  0.1236,    Adjusted R-squared:  0.1002 \nF-statistic: 5.267 on 3 and 112 DF,  p-value: 0.001953\n\n# AV 4\nlm4 <- lm(Probleme_gesamt ~ suboptimal_study*gewissenhaftigkeit, dataset)\nsummary(lm4)\n\n\nCall:\nlm(formula = Probleme_gesamt ~ suboptimal_study * gewissenhaftigkeit, \n    data = dataset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9410 -0.3921 -0.1479  0.3906  1.6183 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                          3.312658   0.779327   4.251 4.43e-05 ***\nsuboptimal_study                    -0.001691   0.230666  -0.007   0.9942    \ngewissenhaftigkeit                  -0.498589   0.222286  -2.243   0.0269 *  \nsuboptimal_study:gewissenhaftigkeit  0.070165   0.067773   1.035   0.3028    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5391 on 112 degrees of freedom\nMultiple R-squared:  0.2621,    Adjusted R-squared:  0.2423 \nF-statistic: 13.26 on 3 and 112 DF,  p-value: 1.806e-07\n\n\n\n\nVoraussetzung Normalverteilung\nBei einer Regression muss nicht die Normalverteilung der einzelnen Variablen, sondern der Fehler der Regression getestet werden. Es geht also um die Abweichung der wahren Werte von unseren vorhergesagten Werten. Wenn diese normalverteilt sind,\n\nshapiro.test(lm1$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  lm1$residuals\nW = 0.97975, p-value = 0.07666\n\nshapiro.test(lm2$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  lm2$residuals\nW = 0.92752, p-value = 9.359e-06\n\nshapiro.test(lm3$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  lm3$residuals\nW = 0.96932, p-value = 0.009209\n\nshapiro.test(lm4$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  lm4$residuals\nW = 0.9584, p-value = 0.00119"
  },
  {
    "objectID": "analysis3.html#interaktionsdiagramme",
    "href": "analysis3.html#interaktionsdiagramme",
    "title": "Inferenzstatistische Analyse",
    "section": "Interaktionsdiagramme",
    "text": "Interaktionsdiagramme\nWir wollen die Zusammenhänge jetzt noch visuell darstellen, indem wir Interaktionsdiagramme erstellen. Da wir zwei kontinuierliche Prädiktoren verwendet haben, können wir für eine bessere Visualisierung mithilfe eines Mediansplit einen der Prädiktoren umwandeln in eine kategoriale Variable: Werte, die geringer sind als der Median, haben die Ausprägung “niedrig”; Werte, die höher sind als der Median, haben die Ausprägung “hoch”.\n\nBerechnet eine neue Variable, die entsprechend dem Median zwei Faktorstufen hatx\nErstellt Interaktionsplots zu jeder der Regressionen\nBenutzt ggarrange() aus ggpubr, um alle vier Plots in einem Plot darzustellen\n\n\nVariable umwandeln\n\ndataset$gewiss.median <- ifelse(dataset$gewissenhaftigkeit <= median(dataset$gewissenhaftigkeit, na.rm=T), \"low\", \"high\")\n\n\n\nSeparate Plots erstellen\n\nplot1 <- ggplot(dataset, aes(gh_SQ007,hsf1_KIZ01, color=gewiss.median))+\n  geom_jitter()+\n  geom_smooth(method=\"lm\", se=F)+\n  theme_minimal()\n\nplot2 <- ggplot(dataset, aes(gh_SQ007,hsf1_KIZ01, color=gewiss.median))+\n  geom_jitter()+\n  geom_smooth(method=\"lm\", se=F)+\n  theme_minimal()\n\nplot3 <- ggplot(dataset, aes(gh_SQ007,hsf1_KIZ01, color=gewiss.median))+\n  geom_jitter()+\n  geom_smooth(method=\"lm\", se=F)+\n  theme_minimal()\n\nplot4 <- ggplot(dataset, aes(gh_SQ007,hsf1_KIZ01, color=gewiss.median))+\n  geom_jitter()+\n  geom_smooth(method=\"lm\", se=F)+\n  theme_minimal()\n\n\n\nPlots zusammenfügen\nWenn wir mehrere Plots in der gleichen Grafik darstellen wollen, können wir das mit ggarrange() aus dem ggpubr Package umsetzen.\n\nggarrange(plot1, plot2, plot3, plot4, \n          ncol=2, nrow=2,\n          common.legend =T)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nTheoretisch hätten wir hier auch die schon erwähnten facet_wrap() oder facet_grid() verwenden können. Dazu hätten wir unsere Daten allerdings umstrukturieren müssen, sodass wir anhand einer kategoriellen Variable spezifizieren können, welche Daten in welchen Plot gehören. Diese Alternative funktioniert allerdings auch nur solange, wie wir den gleichen Plot-Type und die gleichen Variablen für alle Subplots verwenden wollen. Sobald wir verschiedene Plots darstellen wollen, müssen wir auf ggarrange zurückgreifen. In vielen Fällen spricht allerdings auch nichts dagegen, die Plots einfach separat zu lassen."
  },
  {
    "objectID": "assignments/assignment2.html",
    "href": "assignments/assignment2.html",
    "title": "Hausaufgabe 2: Deskriptives",
    "section": "",
    "text": "Beschreibung\nDiese Woche habt ihr euch im Seminar mit dem Einlesen von Daten, dem Zurechtschnibbeln von Dataframes, dem Aufbereiten deskriptiver Ergebnisse und dem Erstellen von Plots beschäftigt. Wie letzte Woche wollen wir euch hier einiges wiederholen, kombinieren und weiterdenken lassen.\nFalls ihr an einer Stelle mal nicht weiterkommen solltet, sodass die folgenden Aufgaben nicht mehr lösbar sind, meldet euch bitte bei uns.\n\nPunkte\nFür jede Aufgabe bzw. Teilaufgabe gibt es Punkte - diese sind entsprechend kenntlich gemacht. Insgesamt gibt es für jedes Aufgabenblatt 10 Punkte. Aufgaben, die als [Bonus] markiert sind, geben keine Punkte.\n\n\nFür Experten\nManche Aufgaben erfordern Transferwissen, welches ihr im heutigen Seminar so nicht direkt gelernt hat. Diese Aufgaben sind als [Für Experten] markiert - hier berücksichtigen wir nicht nur die Lösung, sondern schon eure Lösungsversuche (falls ihr auf keine Lösung kommt).\n\n\nAbgabe\nUm eure Hausaufgabe abzugeben, ladet diese bitte als .qmd Datei im Moodle hoch. Etwaige anderer Dateien (.html, assets-Ordner) müsst ihr nicht hochladen. Bitte benennt die Dateien entsprechend (bspw. hausaufgabe2_simon_krukowski.qmd, und schreibt hier in der Datei oben bei author euren Namen rein.\n\n\n\nAufgaben\n\nAufgabe 1 [1 Punkt]\nAm Anfang von fast jedem R-Code aktiviert man erst mal die notwendigen Packages und lädt den relevanten Datensatz. Wir schauen uns hier einen Datensatz aus dem Package nycflights13 an. Installiert und aktiviert dementsprechend die Packages nycflights13, dplyr und ggplot2. (Denkt auch daran, welche Packages ihr installieren und welche vielleicht nur aktivieren müsst..)\n\n# Hier Lösung einfügen\n\nDer Datensatz, mit dem wir nun arbeiten, nennt sich flights. Unter diesem Namen könnt ihr ihn nach erfolgreicher Aktivierung von nycflights13 finden. Er enthält alle Flüge, die im Jahr 2013 in New York losgeflogen sind. Mehr Informationen zu den Variablen könnt ihr über die Hilfefunktion bekommen.\nHinweis: Speichert euch bei den folgenden Aufgaben eure Änderungen an flights in Objekten mit einem anderen Namen, damit ihr - falls etwas schief geht - immer wieder auf das Original zurückgreifen könnt.\n\n# Nur mal zum Anschauen:\nflights\n\n\n\nAufgabe 2 [1 Punkt]\nErgänzt flights um eine weitere Spalte, in der die Verspätungen bei Ankunft und Abflug (arr_delay und dep_delay) miteinander verrechnet werden. Gestaltet den Code so, dass ihr auch bei späteren Aufgaben auf die Spalte zugreifen könntet.\nVerwendet die Pipe und die euch bekannten dplyr-Befehle.\n\n# Hier Code zur Lösung eintragen\n\n\n\nAufgabe 3 [1 Punkt]\nSortiert flights so, dass ihr einmal die geringste und einmal die größte Verspätung bei Ankunftszeit entnehmen könnt.\n\n# Hier Code zur Lösung eintragen\n\n\n\nAufgabe 4 [1 Punkt]\nWie interpretiert ihr die Ergebnisse aus Aufgabe 3?\nAntwort hier:\n\n\nAufgabe 5 [1 Punkt]\nFür welche Variablen eignet sich group_by typischerweise bei einer “klassischen” Komedia-Studie; wofür hättet ihr sie beispielsweise bei eurer Bachelorarbeit verwenden können?\nAntwort hier:\n\n\nAufgabe 6 [1 Punkt]\nErstellt mittels summarize eine Tabelle, in der für jeden Monat der die durchschnittliche Distanz (Spalte distance) der Flüge und ihre Standardabweichungen angegeben werden.\nVerwendet die Pipe und die euch bekannten dplyr-Befehle.\n\n# Hier Code als Antwort eintragen\n\n\n\nAufgabe 7 [1 Punkt]\nVerwendet die in Aufgabe 6 erstellte Tabelle, um mit ggplot und geom_point einen Plot zu erstellen, in dem auf der x-Achse die Monate und auf der y-Achse die durchschnittliche Distanz zu sehen ist.\n\n# Hier Code als Antwort eintragen\n\n\n\nAufgabe 8 [1 Punkt]\n[für Experten]\nFügt dem Plot aus Aufgabe 7 außerdem Standardabweichungen mithilfe von geom_errorbar() hinzu. (Hinweis: Alles, was ihr für den Input bei geom_errobar() benötigt, befindet sich bereits in der Tabelle von Aufgabe 6)\n\n# Hier Code als Antwort eintragen\n\n\n\nAufgabe 9 [1 Punkt]\nGestaltet den Plot aus Aufgabe 7 oder 8 ästhetischer, indem ihr\n\neinen Titel hinzufügt\ndie Farbe der Punkte zu eurer Lieblingsfarbe verändert\ndie Achsen beschriftet\n\n\n# Hier Code als Antwort einfügen\n\n\n\nAufgabe 10 [1 Punkt]\nRecherchiert geom_jitter und beschreibt in eigenen Worten, was die Funktion macht und in welchen Fällen sie besonders nützlich ist.\nAntwort hier:"
  },
  {
    "objectID": "assignments/assignment3.html",
    "href": "assignments/assignment3.html",
    "title": "Hausaufgabe 2: Deskriptives",
    "section": "",
    "text": "Beschreibung\nDiese Woche habt ihr euch im Seminar mit dem Einlesen von Daten, dem Zurechtschnibbeln von Dataframes, dem Aufbereiten deskriptiver Ergebnisse und dem Erstellen von Plots beschäftigt. Wie letzte Woche wollen wir euch hier einiges wiederholen, kombinieren und weiterdenken lassen.\nFalls ihr an einer Stelle mal nicht weiterkommen solltet, sodass die folgenden Aufgaben nicht mehr lösbar sind, meldet euch bitte bei uns.\n\nPunkte\nFür jede Aufgabe bzw. Teilaufgabe gibt es Punkte - diese sind entsprechend kenntlich gemacht. Insgesamt gibt es für jedes Aufgabenblatt 10 Punkte. Aufgaben, die als [Bonus] markiert sind, geben keine Punkte.\n\n\nFür Experten\nManche Aufgaben erfordern Transferwissen, welches ihr im heutigen Seminar so nicht direkt gelernt hat. Diese Aufgaben sind als [Für Experten] markiert - hier berücksichtigen wir nicht nur die Lösung, sondern schon eure Lösungsversuche (falls ihr auf keine Lösung kommt).\n\n\nAbgabe [+ .html]\nUm eure Hausaufgabe abzugeben, ladet diese bitte als .qmd Datei im Moodle hoch. Etwaige anderer Dateien (.html, assets-Ordner) müsst ihr nicht hochladen. Bitte benennt die Dateien entsprechend (bspw. hausaufgabe2_simon_krukowski.qmd, und schreibt hier in der Datei oben bei author euren Namen rein.\n\n\n\nAufgaben\n\nAufgabe 1 [1 Punkt]\nIn der letzten Seminarstunde habt ihr einiges über statistische Verfahren in R gelernt. In dieser Hausaufgabe geht es darum, dieses Wissenanzuwenden. In der Stunde habt ihr außerdem gelernt, warum es wichtig und sinnvoll ist, zu Beginn eines jeden Quarto-Dokuments einen “Info-Chunk” zu erstellen, in dem ihr alle benötigten Packages aktiviert etc.\nErstellt nun einen solchen Chunk und nennt ihn preliminaries. Ladet darin das Package datarium (installiert es vorher ggf. über die Konsole), sowie alle weiteren Packages, die ihr benötigt. Speichert das darin enthaltene Dataset genderweight als dataset.\nAußerdem kann es hilfreich sein, dort direkt eine Farbpalette (bspw. mit 4 Farben, definiert als hex codes) in einem Vektor zu speichern. Holt euch bspw. bei colorhunt.co Inspiration und speichert eine solche Palette darin ab.\nStellt außerdem sicher, dass der Chunk keine Warnungen anzeigt beim rendern (hint: chunk-options).\n\n\n\n\n\nAufgabe 2 [1 Punkt]\nSchaut euch den Datensatz an, um ein Gefühl dafür zu bekommen. Gebt euch dazu wahlweise den Datensatz selbst, oder bspw. gruppierte Outputs über dplyr an. Wir wollen, dass die Tabellenoutputs nicht im default Format dargestellt werden. Stellt das entsprechend im YAML Header ein.\n\n\n\n\n\nAufgabe 3 [1 Punkt]\nPrüft nun die Variable weight auf Normalverteilung. Macht das wahlweise über einen Test, oder ein Diagramm:\n\n\n\nIst die Variable normalverteilt? Begründe kurz:\nAntwort hier\n\n\nAufgabe 4 [1 Punkt]\nUnabhängig davon, was wir gerade herausgefunden haben, entscheiden wir uns dazu, einen t-Test zu rechnen, und die Mittelwerte von weight miteinander zu vergleichen. Wir haben die Vermutung, dass Männer ein höheres Gewicht haben. Berücksichtigt das und rechnet den Test (hint: gerichtetes Testen). Was kommt heraus?\n\n\n\nAntwort hier\n\n\nBonus:\nWas hat die Überschrift zu bedeuten und wie könnte das mit Voraussetzungen zu tun haben?\nAntwort hier\n\n\nAufgabe 5 [1 Punkt]\nAngenommen, wir hätten in der Zwischenzeit herausgefunden, dass die Variable weight nicht normalverteilt ist. Welcher Test bietet sich dann stattdessen an? Rechnet diesen.\n\n\n\n\n\nAufgabe 6 [1 Punkt]\n-> Anova mit CO2 und Messwiederholung -> Long/Wide?\n\n\n\n\n\nAufgabe 7 [1 Punkt]\n-> Boxplot? -> Palette verwenden\n\n\nAufgabe 8 [1 Punkt]\nWir interessieren uns nun auch für Zusammenhänge zwischen Daten. Dazu schauen wir uns wieder das bekannte iris data set an. Primär interessiert uns, ob Sepal.Width und Sepal.Height zusammenhängen. Um also zunächst eine Idee über die Verteilung der Daten zu bekommen, sollt ihr mit ggplot ein Diagramm erstellen. Darin sollen mehrere Sachen enthalten sein:\n\nBivariates Streudiagramm (hint: ihr wollt Punkte plotten)\nEine Regressionsgrade (hint: geom_smooth() ist euer Freund)\n\n\n\n\nRechnet auch eine Pearson-Korrelation zwischen den beiden Variablen:\n\n\n\nWas lässt sich über den Zusammenhang der beiden Variablen sagen?\nAntwort hier\n\n\nAufgabe 9 [1 Punkt]\nWir haben nun die Annahme, dass die Spezies der Pflanzen außerdem einen Einfluss auf den Zusammenhang zwischen den beiden Variablen hat. Dazu sollt ihr zunächst die Punkte in Abhängigkeit von Species einfärben:\n\n\n\nWir haben nun das Gefühl, dass ein Zusammenhang zwischen den beiden Variablen besteht und uns dazu entschieden, ein Regressionsmodell zu berechnen, in dem wir Sepal.Length als abhängige Variable haben, sowie Species und Sepal.Width als Prädiktoren. Berechnet ein solches Modell und speichert es als model:\n\n\n\n\n\nAufgabe 10 [1 Punkt]\nNun wollen wir die Ergebnisse der Regressionsanalyse entsprechend berichten. Aus Übungsgründen sollt ihr dabei auf Packages wie stargazer() verzichten. Ihr habt im Seminar inline-Code gelernt. Verwendet diesen, um die Ergebnisse entsprechend zu berichten. Dazu bietet es sich an, den Output von summary() in einem Objekt zu speichern, und dann auf die Werte zuzugreifen. Denkt dran: über den $ Operator sowie ['variable'] könnt ihr auf einzelne Werte zugreifen.\n\n\n\nErgänzt also die F-Statistik in folgendem Satz:\n\nDie Regression mit Sepal.Width als abhängige und Sepal.Length und Species als erklärende Variablen ist signifikant, F (3,146) = antwort hier, p < .001.\n\n\n\nAufgabe 11 [Bonus]\nWas macht die paste0 Funktion und könnte man die obige Lösung damit vereinfachen?"
  },
  {
    "objectID": "Auswertung_survey.html",
    "href": "Auswertung_survey.html",
    "title": "analysis_priorknowledge",
    "section": "",
    "text": "library(dplyr, warn.conflicts = F)\nlibrary(ggplot2)\nlibrary(reshape2)\n\npalette1 <- c(\"#648fff\",\"#785ef0\",\"#dc267f\",\"#fe6100\",\"#ffb000\",\"#000000\",\"#ffffff\")\npalette2 <- c(\"#CC79A7\",\"#D55E00\",\"#0072B2\",\"#F0E442\",\"#009E73\",\"#56B4E9\",\"#E69F00\",\"#000000\")\n\nThis is the corresponding quarto file for our analyses regarding the prior-existing knowledge in our R-course.\nFirst we want to import the data:\n\ndataset <- read.csv(\"/Users/simonkrukowski/Downloads/results-survey368965-6.csv\")\n\nOf course we’re dissatisfied with the variable names as provided by LimeSurvey, so we have to take care of that as well:\n\ndataset %>% \n  rename_with(., ~ gsub(\".\",\"_\",.,fixed = T)) -> dataset\n\n\ndataset %>% \n  # rename columns\n  rename(age = G01Q02,\n         r_knowledge = G01Q03,\n         r_problems = G01Q09,\n         programming_knowledge = G01Q05,\n         programming_c = G01Q06_c_,\n         programming_javascript = G01Q06_javascript_,\n         programming_python = G01Q06_python_,\n         programming_java = G01Q06_Java_,\n         programming_sql = G01Q06_sql_,\n         programming_julia = G01Q06_julia_,\n         programming_other = G01Q06_other_) %>% \n  # mutate (create new/change old columns to account for limesurvey artifacts)\n  mutate(across(everything() & where(is.character), ~na_if(., \"\")),\n         across(everything() & where(is.numeric), ~na_if(., 0)),\n         sex = case_when(G01Q01_female_ == \"Y\" ~ \"female\",\n                         G01Q01_male_ == \"Y\" ~ \"male\",\n                         G01Q01_diverse_ == \"Y\" ~ \"diverse\"),\n         r_knowledge_amount = case_when(G01Q04_priorknow_ == \"AO01\" ~ 1,\n                                        G01Q04_priorknow_ == \"AO02\" ~ 2,\n                                        G01Q04_priorknow_ == \"AO03\" ~ 3,\n                                        G01Q04_priorknow_ == \"AO04\" ~ 4,\n                                        G01Q04_priorknow_ == \"AO05\" ~ 5),\n         hopes_base_knowledge = case_when(G01Q10_SQ001_ == \"Y\" ~ \"Y\"),\n         hopes_improve_knowledge = case_when(G01Q10_SQ002_ == \"Y\" ~ \"Y\"),\n         hopes_dataviz = case_when(G01Q10_SQ003_ == \"Y\" ~ \"Y\"),\n         hopes_stat_analyses = case_when(G01Q10_SQ004_ == \"Y\" ~ \"Y\"),\n         hopes_websites = case_when(G01Q10_SQ005_ == \"Y\" ~ \"Y\")\n         ) %>% \n  # remove all columns that start with G0 (all limesurvey artifacts)\n  select(-starts_with(\"G0\")) %>% \n  # replace all Ys with bool values and replace NAs with FALSE for multiple choice items\n  mutate(across(everything(),  ~replace(., . ==  \"Y\" , TRUE)),\n         across(starts_with(c(\"programming_\",\"hopes_\")),~replace(., is.na(.), FALSE)),\n         across(c(starts_with(c(\"hopes\",\"programming\")),r_knowledge),as.logical)) %>% \n  mutate(r_knowledge = case_when(is.na(r_knowledge) ~ FALSE,\n                                 !is.na(r_knowledge) ~ TRUE)) -> dataset\n\nNow we want to delete the first four rows because these were test data.\n\ndataset %>% \n  filter(row_number() > 4) -> dataset\n\nOkay now it comes to visualising. Let’s skip the descriptive data for our second assignment and let’s instead focus on prior knowledge.\n\ndataset %>% \n  mutate(r_knowledge = case_when(r_knowledge == T ~\"Vorwissen\",\n                                 r_knowledge == F ~\"kein Vorwissen\")) %>% \n  ggplot(aes(x=r_knowledge))+\n  geom_bar(aes(fill=r_knowledge),color=\"#2b2b2b\")+\n  geom_label(aes(label = paste0(after_stat(count)/sum(after_stat(count))*100, \"%\")),\n            stat = \"count\",\n            colour = \"black\",\n            position = position_dodge(width=0.9),vjust = 0.5) +\n  scale_fill_manual(values=palette1[2:3])+\n  theme_classic()+\n  ylab(\"Anzahl\")+xlab(\"Vorwissen\")+labs(fill=\"Vorwissen\")\n\nAnd then on the mean value of the 2 persons who do have prior knowledge. Here, a plot doesnt really make sense:\n\ndataset %>% \n  mutate(r_knowledge = case_when(r_knowledge == T ~\"Vorwissen\",\n                                 r_knowledge == F ~\"kein Vorwissen\")) %>% \n  group_by(r_knowledge) %>% \n  summarise(mean=mean(r_knowledge_amount))\n\nOkay now let’s focus on prior knowledge in programming:\n\ndataset %>% \n  select(id,starts_with(\"progr\"),-programming_knowledge) %>% \n  rename_with(., ~ gsub(\"programming_\",\"\",.,fixed = T)) %>% \n  melt(id.vars = c(\"id\")) %>% \n  group_by(variable) %>% \n  summarise(sum=sum(value))\n\nLet’s visualise it. Here, we can skip the summarise.\n\ndataset %>% \n  select(id,starts_with(\"progr\"),-programming_knowledge) %>% \n  rename_with(., ~ gsub(\"programming_\",\"\",.,fixed = T)) %>% \n  melt(id.vars = c(\"id\")) %>% \n  filter(value==T) %>% \n  ggplot(aes(x=variable))+\n  geom_bar(aes(fill=variable),stat = \"count\",color=\"#2b2b2b\")+\n  geom_label(aes(label = paste0(round(after_stat(count)/sum(after_stat(count))*100,1), \"%\")),\n            stat = \"count\",\n            colour = \"black\",\n            position = position_dodge(width=0.9),vjust = 0.5) +\n  scale_fill_manual(values = palette1)+\n  theme_classic()+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  ylab(\"Anzahl\")+xlab(\"Programmiersprache\")+labs(fill=\"Programmiersprache\")\n\nOkay, now we also want to check the expectations:\n\ndataset %>% \n  select(id,starts_with(\"hopes\")) %>% \n  rename_with(., ~ gsub(\"hopes_\",\"\",.,fixed = T)) %>% \n  melt(id.vars = c(\"id\")) %>% \n  filter(value==T) %>% \n  ggplot(aes(x=variable))+\n  geom_bar(aes(fill=variable),stat = \"count\",color=\"#2b2b2b\")+\n  geom_label(aes(label = paste0(round(after_stat(count)/sum(after_stat(count))*100,1), \"%\")),\n            stat = \"count\",\n            colour = \"black\",\n            position = position_dodge(width=0.9),vjust = 0.5) +\n  scale_fill_manual(values = palette1)+\n  theme_classic()+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  ylab(\"Anzahl\")+xlab(\"Erwartungen\")+labs(fill=\"Erwartungen\")"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Kontakt",
    "section": "",
    "text": "Falls ihr Probleme oder Fragen habt, meldet euch gerne.\n\n\nE-Mail: simon.krukowski@uni-due.de\nRaum: Uni Duisburg-Essen, Campus Duisburg, LE 616\nTelefon: +49 203 379 / 4143\n\n\n\nE-Mail: kira.wolff@uni-due.de\nRaum: Uni Duisburg-Essen, Campus Duisburg, LE 623\nTelefon: +49 203 379 / 4761"
  },
  {
    "objectID": "content3.html",
    "href": "content3.html",
    "title": "Inferenzstatistik in R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(rstatix)\nlibrary(ggplot2)\nlibrary(car)\nlibrary(nycflights13)"
  },
  {
    "objectID": "content3.html#die-qual-der-wahl",
    "href": "content3.html#die-qual-der-wahl",
    "title": "Inferenzstatistik in R",
    "section": "Die Qual der Wahl",
    "text": "Die Qual der Wahl\nÄhnlich wie ihr letzte Woche die Unterschiede zwischen base R und dplyr kennengelernt habt, gibt es auch für die inferenzstatistischen Verfahren verschiedene Packages und Funktionen, und damit Vorgehensweisen.\nGrundsätzlich gibt es hier keine richtigen oder falschen Packages, stattdessen hängt es vom Kontext ab. Manche Packages ermöglichen ziemlich komplexe Analysen, und um sich die als Möglichkeit offen zu lassen, kann es sich lohnen, auch direkt die “einfachen” Analysen eines Projekts damit zu rechnen, damit die verschiedenen Analysen kompatibler miteinander sind. Manche Packages sind von der Syntax möglichst eingängig gestaltet, sodass man als Anfänger besser abgeholt wird. Wiederum andere bieten die gleichen Funktionen, verwenden aber leicht unterschiedliche Berechnungsmethoden, da die Methoden für verschiedene Kontexte verschieden robust sind.\nMeistens macht es Sinn mit dem, was man kennt (bzw. was ihr hier kennenlernt), anzufangen. Wenn sich dann eine Datensituation ergibt, die komplexer ist, lässt sich immer noch im Internet recherchieren, welches Package/welche Funktion vielleicht besser geeignet sind. Die Hürde wird dann weniger euer R-Wissen sein, sondern eher euer allgemeines Statistik-Wissen. Unserer Erfahrung nach verbringt man eher mehr Zeit mit der Recherche von statistischen Methoden statt R-Funktionen.\nWie machen wir es hier? Wir zeigen euch die Funktionen von base R und vom Package rstatix, welches bewusst so gestaltet ist, damit es gut mit der Pipe und den dplyr-Funktionen funktioniert."
  },
  {
    "objectID": "content3.html#voraussetzungen",
    "href": "content3.html#voraussetzungen",
    "title": "Inferenzstatistik in R",
    "section": "Voraussetzungen",
    "text": "Voraussetzungen\nWie ihr wisst, gibt es für die sogennannten parametrischen Testverfahren bestimmte Voraussetzungen, die die Daten erfüllen sollten, damit die Tests anwendbar sind und die Auswertung angemessen. Vorbildlich wie wir sind, schauen wir uns diese Tests für die Voraussetzungen zuerst an.\n\nNormalverteilung\n\nplot(dnorm, xlim=c(-3,3))\n\n\n\n\n\n# base\nshapiro.test(Daten$Spalte)\n\n# rstatix\ndata %>% \n  shapiro_test(Spalte)\n\nDer Shapiro-Wilk Test testet, ob die Daten signifikant von einer Normalverteilung abweichen. Wenn er signifikant wird, sind die Daten also nicht normalverteilt.\nJe größer die Stichprobe ist, desto schlechter funktioniert der Shapiro-Wilk Test, da er dann tendenziell zu schnell signifikant wird, obwohl die Daten einigermaßen normalverteilt sind. Daher bietet es sich an, ab ca. n > 50 stattdessen einen QQ-Plot anzuschauen. Ab n > 5000 würde der Test nicht mehr ausgeführt werden.\n\n# normalverteilte Werte erzeugen\nvec.norm <- rnorm(100, mean = 0, sd=1)\n\n# QQ-Plot\nggplot()+\n  geom_qq(aes(sample=vec.norm))+\n  theme_minimal()\n\n\n\n\n\nshapiro.test(vec.norm)\n\n\n    Shapiro-Wilk normality test\n\ndata:  vec.norm\nW = 0.9863, p-value = 0.3925\n\n\nBei einem QQ-Plot werden die Werte, die wir testen, standardisiert und dann gegen die “echte” Standardnormalverteilung geplottet. Wenn unsere Daten perfekt (standard)normalverteilt wären, würde sich eine perfekte 45°-Gerade ergeben. Unsere künstlichen Daten oben sind auf jeden Fall nah genug an der “perfekten Gerade” dran.\nHier noch ein Beispiel, in dem die Daten nicht normalverteilt sind:\n\n# Daten aus nycflights13 package\nggplot(flights)+\n  geom_qq(aes(sample=air_time))+\n  theme_minimal()\n\nWarning: Removed 9430 rows containing non-finite values (`stat_qq()`).\n\n\n\n\nflights\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      544        545      -1    1004    1022     -18 B6     \n 5  2013     1     1      554        600      -6     812     837     -25 DL     \n 6  2013     1     1      554        558      -4     740     728      12 UA     \n 7  2013     1     1      555        600      -5     913     854      19 B6     \n 8  2013     1     1      557        600      -3     709     723     -14 EV     \n 9  2013     1     1      557        600      -3     838     846      -8 B6     \n10  2013     1     1      558        600      -2     753     745       8 AA     \n# … with 336,766 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\nggplot(flights)+\n  geom_qq(aes(sample=dep_delay))+\n  theme_minimal()\n\nWarning: Removed 8255 rows containing non-finite values (`stat_qq()`).\n\n\n\n\nhist(flights$dep_delay)\n\n\n\n\n\n\nVarianzhomogenität\nBei der Varianzhomogenität geht es darum, dass unterschiedliche Gruppen bezüglich einer Variable die in etwa gleiche Varianz haben.\n\n# rstatix\ndata %>% \n  levene_test(abhängigeVariable ~ Gruppe)\n\nflights %>% \n  levene_test(dep_delay ~ factor(month))"
  },
  {
    "objectID": "content3.html#gruppenvergleiche",
    "href": "content3.html#gruppenvergleiche",
    "title": "Inferenzstatistik in R",
    "section": "Gruppenvergleiche",
    "text": "Gruppenvergleiche\n\nt-Test\nZiel: zwei Gruppen vergleichen\nDatenstruktur: kontinuierliche abhängige Variable & dichotome Gruppenvariable\nZur Veranschaulichung verwenden wir hier den Datensatz ToothGrowth: Hier wurde der Zahnwachstum von Meerschweinchen in Abhängigkeit von verabreichtem Vitamin C untersucht. Die Stichprobe besteht aus 60 Meerschweinchen und sowohl die Verabreichungsmethode (Orangensaft vs. Ascorbinsäure) als auch die Vitamin C-Dosis (0.5, 1 oder 2mg pro Tag) wurden variiert. Da wir beim t-Test nur mit zwei Gruppen arbeiten können, schauen wir uns jetzt nur die Unterschiede durch die Verabreichungsmethode an.\n\nsummary(ToothGrowth)\n\n      len        supp         dose      \n Min.   : 4.20   OJ:30   Min.   :0.500  \n 1st Qu.:13.07   VC:30   1st Qu.:0.500  \n Median :19.25           Median :1.000  \n Mean   :18.81           Mean   :1.167  \n 3rd Qu.:25.27           3rd Qu.:2.000  \n Max.   :33.90           Max.   :2.000  \n\nview(ToothGrowth)\n\nToothGrowth %>% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable     n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  <fct>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 len         60   4.2  33.9   19.2  13.1  25.3  12.2 9.04  18.8  7.65  0.988\n2 dose        60   0.5   2      1     0.5   2     1.5 0.741  1.17 0.629 0.081\n# … with 1 more variable: ci <dbl>\n\ntooth <- ToothGrowth %>% \n            mutate(dose = as.factor(dose))\n\n\nggplot(tooth)+\n  geom_boxplot(aes(x=supp, y=len))+\n  theme_minimal()\n\n\n\n\n\nFunktionsaufbau\n\n# Base: Format 1\nt.test(AV ~ group, data)\n\n# Base: Format 2\nt.test(daten$group1, daten$group2)\n\n# rstatix\ndata %>% \n  t_test(AV ~ group)\n\nDie Tilde ~ ist das Zeichen in R, um einen Zusammenhang, eine Abhängigkeit oder ein Modell zu symbolisieren.\nNatürlich gibt es innerhalb von t.test()/t_test() mit Argumenten Möglichkeiten um\n\neinen t-Test für eine Stichprobe durchzuführen: z.B. mu = 100\neinen t-Test für abhängige Stichproben durchzuführen: paired = TRUE\neinen gerichteten t-Test durchzuführen: z.B. alternative = \"less\"\nbei nicht vorliegender Varianzhomogenität stattdessen den Welch-Test anzuwenden: var.equal = FALSE\n\n\n\nFunktionsanwendung\n\n# Base: Format 1\nt.base <- t.test(len ~ supp, tooth, var.equal = TRUE)\nt.base\n\n\n    Two Sample t-test\n\ndata:  len by supp\nt = 1.9153, df = 58, p-value = 0.06039\nalternative hypothesis: true difference in means between group OJ and group VC is not equal to 0\n95 percent confidence interval:\n -0.1670064  7.5670064\nsample estimates:\nmean in group OJ mean in group VC \n        20.66333         16.96333 \n\n# rstatix\nt.rstatix <- tooth %>% \n                t_test(len ~ supp,var.equal = TRUE)\nt.rstatix\n\n# A tibble: 1 × 8\n  .y.   group1 group2    n1    n2 statistic    df      p\n* <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl>  <dbl>\n1 len   OJ     VC        30    30      1.92    58 0.0604\n\n# Normalerweise würde ich die Ergebnisse nicht als Objekte speichern, dient hier nur der Demonstration.\n\nUnterschied zwischen base und rstatix:\n\nListe vs. tibble\nbase: Zusatzinfos - Zuordnung, Verständnis\nrstatix: Formatierung als Tabelle\nrstatix garantiert pipe-Kompatibilität, funktioniert bei Base-Funktionen nicht immer (aber manchmal)\n\nEuch wird im Laufe der kommenden Funktionen auffallen, dass die rstatix Funktionen netterweise fast immer exakt so heißen wie die base Funktionen, nur das statt . ein _ verwendet wird: t.test() vs. t_test()\n\n\nNonparametrisch: Wilcoxon Rank Sum Test\nReminder: Wenn Voraussetzungen nicht erfüllt sein sollten.\n\n# Base\nwilcox.test(len ~ supp, tooth)\n\nWarning in wilcox.test.default(x = c(15.2, 21.5, 17.6, 9.7, 14.5, 10, 8.2, :\ncannot compute exact p-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  len by supp\nW = 575.5, p-value = 0.06449\nalternative hypothesis: true location shift is not equal to 0\n\n# rstatix\ntooth %>% \n  wilcox_test(len ~ supp)\n\n# A tibble: 1 × 7\n  .y.   group1 group2    n1    n2 statistic      p\n* <chr> <chr>  <chr>  <int> <int>     <dbl>  <dbl>\n1 len   OJ     VC        30    30      576. 0.0645\n\n\n\n\n\nANOVA\nZiel: Zwei oder mehr Gruppen vergleichen.\nWir können hier bei den Meerschweinchendaten bleiben, da die Dosis-Variable drei Faktorstufen hat.\n\n#base\ntooth.aov <- aov(len ~ dose, tooth)\nsummary(tooth.aov)\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ndose         2   2426    1213   67.42 9.53e-16 ***\nResiduals   57   1026      18                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#funktioniert nicht:\n\n#tooth %>% \n#  aov(len ~ dose) %>% \n#  summary()\n\n\n#rstatix\n## Schreibweise 1\ntooth %>%\n  anova_test(len ~ dose)\n\nANOVA Table (type II tests)\n\n  Effect DFn DFd      F        p p<.05   ges\n1   dose   2  57 67.416 9.53e-16     * 0.703\n\n## Schreibweise 2\ntooth %>%\n  anova_test(dv = len,\n             between = dose)\n\nANOVA Table (type II tests)\n\n  Effect DFn DFd      F        p p<.05   ges\n1   dose   2  57 67.416 9.53e-16     * 0.703\n\n# Hybrid aus base und rstatix\nanova_summary(tooth.aov)\n\n  Effect DFn DFd      F        p p<.05   ges\n1      1   2  57 67.416 9.53e-16     1 0.703\n\n\nDie Funktion aov() steht natürlich für “analysis of variances”. Die Funktion gibt uns nicht direkt das Ergebnis, was wir von einer ANOVA erwarten, sondern fittet (=“baut”) erst mal nur das Modell. Wenn wir das ANOVA-Modell auswerten wollen, müssen wir uns das aov-Ergebnis über summary() zusammenfassen lassen.\nDie Funktion anova_test() gibt uns direkt das erwartete Ergebnis aus. Innerhalb der Funktion gibt es zwei mögliche Schreibweisen, um das Modell zu spezifizieren: Entweder über die Tilde, oder indem wir Rollen der Variablen separat über die Argumente spezifizieren.\n\n# Hybrid aus base und rstatix\nanova_summary(tooth.aov)\n\n  Effect DFn DFd      F        p p<.05   ges\n1      1   2  57 67.416 9.53e-16     1 0.703\n\n\nanova_summary() kann Outputs von aov() verwerten und verpackt sie in das praktische dataframe-Format, das wir von anova_test() schon kennen.\n\nKomplexere Modelle\nKomplexere Modelle mit mehreren Faktoren lassen sich natürlich auch realisieren. Dafür ein neues Beispiel: Im Datensatz ChickWeight wurde das Gewicht von Küken seit Geburt getrackt. Es sind verschiedene Messzeitpunkte und die Art der Ernährung enthalten.\n\nChickWeight %>% \n  summary()\n\n     weight           Time           Chick     Diet   \n Min.   : 35.0   Min.   : 0.00   13     : 12   1:220  \n 1st Qu.: 63.0   1st Qu.: 4.00   9      : 12   2:120  \n Median :103.0   Median :10.00   20     : 12   3:120  \n Mean   :121.8   Mean   :10.72   10     : 12   4:118  \n 3rd Qu.:163.8   3rd Qu.:16.00   17     : 12          \n Max.   :373.0   Max.   :21.00   19     : 12          \n                                 (Other):506          \n\nview(ChickWeight)\n\nHier schauen wir uns direkt nur rstatix an, da das die eindeutig angenehmere Umsetzung ist.\n\n# rstatix\n# Schreibweise 1\nChickWeight %>% \n  anova_test(weight ~ Diet + Time + Error(Chick/Time))\n\nANOVA Table (type III tests)\n\n$ANOVA\n     Effect DFn DFd       F         p p<.05   ges\n1      Diet   3  41   5.075  4.00e-03     * 0.161\n2      Time  11 451 280.945 6.41e-194     * 0.769\n3 Diet:Time  33 451   3.766  9.34e-11     * 0.118\n\n$`Mauchly's Test for Sphericity`\n     Effect        W         p p<.05\n1      Time 2.68e-17 1.03e-251     *\n2 Diet:Time 2.68e-17 1.03e-251     *\n\n$`Sphericity Corrections`\n     Effect   GGe      DF[GG]    p[GG] p[GG]<.05   HFe      DF[HF]    p[HF]\n1      Time 0.114 1.26, 51.48 2.01e-24         * 0.116 1.28, 52.34 8.63e-25\n2 Diet:Time 0.114 3.77, 51.48 1.00e-02         * 0.116 3.83, 52.34 1.00e-02\n  p[HF]<.05\n1         *\n2         *\n\n# Schreibweise 2\nChickWeight %>% \n  anova_test(dv = weight,\n             between = Diet,\n             within = Time,\n             wid = Chick)\n\nANOVA Table (type III tests)\n\n$ANOVA\n     Effect DFn DFd       F         p p<.05   ges\n1      Diet   3  41   5.075  4.00e-03     * 0.161\n2      Time  11 451 280.945 6.41e-194     * 0.769\n3 Diet:Time  33 451   3.766  9.34e-11     * 0.118\n\n$`Mauchly's Test for Sphericity`\n     Effect        W         p p<.05\n1      Time 2.68e-17 1.03e-251     *\n2 Diet:Time 2.68e-17 1.03e-251     *\n\n$`Sphericity Corrections`\n     Effect   GGe      DF[GG]    p[GG] p[GG]<.05   HFe      DF[HF]    p[HF]\n1      Time 0.114 1.26, 51.48 2.01e-24         * 0.116 1.28, 52.34 8.63e-25\n2 Diet:Time 0.114 3.77, 51.48 1.00e-02         * 0.116 3.83, 52.34 1.00e-02\n  p[HF]<.05\n1         *\n2         *\n\n\nBei Daten mit Messwiederholung müssen wir spezifizieren, welcher Faktor mehrmals gemessen wurde und an welcher Variable erkannt wird, welche Messungen zu welchem “Probanden” gehören. In der Formelschreibweise lässt sich das durch den Term Error(ProbandenID/wiederholterFaktor) ausdrücken. In der Argumentschreibweise spezifizieren wir within = wiederholterFaktor und wid = ProbandenID\nanova_test() testet netterweise die Sphärizität direkt mit. Wenn der Mauchly-Test signifikant wird, müssen wir die messwiederholten Faktoren (hier: Time und die Interaktion Diet:Time) im unteren Abschnitt des Outputs interpretieren. Die Spalten mit “GG” sind dabei korrigiert nach Greenhouse-Geisser, die Spalten mit “HF” nach Huynh-Feldt.\nKovariaten können wir in anova_test() über das Argument covariate = einfügen, so wie bei dv =, between = usw.\nNoch mehr Faktoren/Variablen lassen sich über c() verknüpfen, z.B. between = c(Diet, Species)\n\n\nPost Hoc Test\nWenn eine ANOVA signifikant wird, interessiert uns meistens noch, welche/r der Mittelwertsunterschiede dafür verantwortlich ist. Im Bezug auf unser Meerschweinchen-Beispiel hängt der Zahnwachstum offensichtlich von der Dosis ab, aber bisher wissen wir nicht, ob die höchste Dosis zu mehr Wachstum als die anderen beiden führt, oder ob sich alle signifikant voneinander unterscheiden, oder ob es nur einen Unterschied im Bezug zur niedrigsten Dosis gibt usw.\n\n# base\nTukeyHSD(tooth.aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = len ~ dose, data = tooth)\n\n$dose\n        diff       lwr       upr    p adj\n1-0.5  9.130  5.901805 12.358195 0.00e+00\n2-0.5 15.495 12.266805 18.723195 0.00e+00\n2-1    6.365  3.136805  9.593195 4.25e-05\n\n# rstatix\ntooth %>%\n  tukey_hsd(len ~ dose)\n\n# A tibble: 3 × 9\n  term  group1 group2 null.value estimate conf.low conf.high    p.adj p.adj.si…¹\n* <chr> <chr>  <chr>       <dbl>    <dbl>    <dbl>     <dbl>    <dbl> <chr>     \n1 dose  0.5    1               0     9.13     5.90     12.4  2   e- 8 ****      \n2 dose  0.5    2               0    15.5     12.3      18.7  1.12e-11 ****      \n3 dose  1      2               0     6.36     3.14      9.59 4.25e- 5 ****      \n# … with abbreviated variable name ¹​p.adj.signif\n\n\nDie Funktion tukey_hsd erlaubt als Input entweder eine Formel (wie hier) oder das Ergebnis von aov() oder lm(). Der Output von anova_test() funktioniert hier nicht als Input!\n\n\nNonparametrisch: Kruskal-Wallis Rank Sum Test\n\n# base\nkruskal.test(tooth, len ~ dose)\n\nWarning in kruskal.test.default(tooth, len ~ dose): 'x' is a list, so ignoring\nargument 'g'\n\n\nWarning in kruskal.test.default(tooth, len ~ dose): some elements of 'x' are not\nnumeric and will be coerced to numeric\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  tooth\nKruskal-Wallis chi-squared = 129.49, df = 2, p-value < 2.2e-16\n\n# rstatix\ntooth %>% \n  kruskal_test(len ~ dose)\n\n# A tibble: 1 × 6\n  .y.       n statistic    df             p method        \n* <chr> <int>     <dbl> <int>         <dbl> <chr>         \n1 len      60      40.7     2 0.00000000148 Kruskal-Wallis"
  },
  {
    "objectID": "content3.html#zusammenhänge",
    "href": "content3.html#zusammenhänge",
    "title": "Inferenzstatistik in R",
    "section": "Zusammenhänge",
    "text": "Zusammenhänge\n\nKorrelation\nZiel: Zusammenhang zwischen zwei Variablen feststellen\nDatenstruktur: Zwei kontinuierliche Variablen\nAls Beispiel haben wir den cars Datensatz, der Geschwindigkeit und Bremsweg von Autos enthält.\n\nget_summary_stats(cars)\n\n# A tibble: 2 × 13\n  variable     n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  <fct>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 speed       50     4    25     15    12    19     7  5.93  15.4  5.29 0.748\n2 dist        50     2   120     36    26    56    30 23.7   43.0 25.8  3.64 \n# … with 1 more variable: ci <dbl>\n\nggplot(cars)+\n  geom_point(aes(speed, dist))+\n  theme_minimal()\n\n\n\n\n\n# r base\ncor(cars$speed, cars$dist)\n\n[1] 0.8068949\n\n# rstatix\ncars %>% \n  cor_test(speed, dist)\n\n# A tibble: 1 × 8\n  var1  var2    cor statistic        p conf.low conf.high method \n  <chr> <chr> <dbl>     <dbl>    <dbl>    <dbl>     <dbl> <chr>  \n1 speed dist   0.81      9.46 1.49e-12    0.682     0.886 Pearson\n\n# psych\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following object is masked from 'package:car':\n\n    logit\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\ncorr.test(cars$speed, cars$dist)\n\nCall:corr.test(x = cars$speed, y = cars$dist)\nCorrelation matrix \n[1] 0.81\nSample Size \n[1] 50\nThese are the unadjusted probability values.\n  The probability values  adjusted for multiple tests are in the p.adj object. \n[1] 0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\nBase R kann Korrelationen berechnen, hat aber standardmäßig keinen Test auf Signifikanz enthalten. Hier wird der Nutzen von rstatix besonders deutlich.\nAls Ergänzung noch die Funktion corr.test() aus dem psych Package.\n\nNonparametrisch: Spearman & Kendall\nNonparametrische Korrelationsberechnungen lassen sich über das Argument method spezifizieren.\n\ndata %>% \n  cor_test(method = \"Spearman\") # oder auch Kendall\n\n\n\n\nRegression\nZiele:\n\nGruppenunterschiede feststellen\nWerte vorhersagen\nrelevante Prädiktoren für eine bestimmte abhängige Variable identifizieren\nModell mit bestem Fit finden\n\nDatenstruktur: kontinuierliche abhängige Variable & kategoriale/kontinuierliche Prädiktoren\nHier haben wir als Datenbeispiel den Datensatz swiss, in dem die Fertilität der Population der 47 französisch-sprachigen Provinzen in der Schweiz erfasst wurde. Die weiteren Variablen:\n\nAgriculture: % of males involved in agriculture as occupation\nExamination: % draftees receiving highest mark on army examination\nEducation: % education beyond primary school for draftees\nCatholic: % ‘catholic’ (as opposed to ‘protestant’)\nInfant.Mortality: live births who live less than 1 year\n\n\nswiss\n\n             Fertility Agriculture Examination Education Catholic\nCourtelary        80.2        17.0          15        12     9.96\nDelemont          83.1        45.1           6         9    84.84\nFranches-Mnt      92.5        39.7           5         5    93.40\nMoutier           85.8        36.5          12         7    33.77\nNeuveville        76.9        43.5          17        15     5.16\nPorrentruy        76.1        35.3           9         7    90.57\nBroye             83.8        70.2          16         7    92.85\nGlane             92.4        67.8          14         8    97.16\nGruyere           82.4        53.3          12         7    97.67\nSarine            82.9        45.2          16        13    91.38\nVeveyse           87.1        64.5          14         6    98.61\nAigle             64.1        62.0          21        12     8.52\nAubonne           66.9        67.5          14         7     2.27\nAvenches          68.9        60.7          19        12     4.43\nCossonay          61.7        69.3          22         5     2.82\nEchallens         68.3        72.6          18         2    24.20\nGrandson          71.7        34.0          17         8     3.30\nLausanne          55.7        19.4          26        28    12.11\nLa Vallee         54.3        15.2          31        20     2.15\nLavaux            65.1        73.0          19         9     2.84\nMorges            65.5        59.8          22        10     5.23\nMoudon            65.0        55.1          14         3     4.52\nNyone             56.6        50.9          22        12    15.14\nOrbe              57.4        54.1          20         6     4.20\nOron              72.5        71.2          12         1     2.40\nPayerne           74.2        58.1          14         8     5.23\nPaysd'enhaut      72.0        63.5           6         3     2.56\nRolle             60.5        60.8          16        10     7.72\nVevey             58.3        26.8          25        19    18.46\nYverdon           65.4        49.5          15         8     6.10\nConthey           75.5        85.9           3         2    99.71\nEntremont         69.3        84.9           7         6    99.68\nHerens            77.3        89.7           5         2   100.00\nMartigwy          70.5        78.2          12         6    98.96\nMonthey           79.4        64.9           7         3    98.22\nSt Maurice        65.0        75.9           9         9    99.06\nSierre            92.2        84.6           3         3    99.46\nSion              79.3        63.1          13        13    96.83\nBoudry            70.4        38.4          26        12     5.62\nLa Chauxdfnd      65.7         7.7          29        11    13.79\nLe Locle          72.7        16.7          22        13    11.22\nNeuchatel         64.4        17.6          35        32    16.92\nVal de Ruz        77.6        37.6          15         7     4.97\nValdeTravers      67.6        18.7          25         7     8.65\nV. De Geneve      35.0         1.2          37        53    42.34\nRive Droite       44.7        46.6          16        29    50.43\nRive Gauche       42.8        27.7          22        29    58.33\n             Infant.Mortality\nCourtelary               22.2\nDelemont                 22.2\nFranches-Mnt             20.2\nMoutier                  20.3\nNeuveville               20.6\nPorrentruy               26.6\nBroye                    23.6\nGlane                    24.9\nGruyere                  21.0\nSarine                   24.4\nVeveyse                  24.5\nAigle                    16.5\nAubonne                  19.1\nAvenches                 22.7\nCossonay                 18.7\nEchallens                21.2\nGrandson                 20.0\nLausanne                 20.2\nLa Vallee                10.8\nLavaux                   20.0\nMorges                   18.0\nMoudon                   22.4\nNyone                    16.7\nOrbe                     15.3\nOron                     21.0\nPayerne                  23.8\nPaysd'enhaut             18.0\nRolle                    16.3\nVevey                    20.9\nYverdon                  22.5\nConthey                  15.1\nEntremont                19.8\nHerens                   18.3\nMartigwy                 19.4\nMonthey                  20.2\nSt Maurice               17.8\nSierre                   16.3\nSion                     18.1\nBoudry                   20.3\nLa Chauxdfnd             20.5\nLe Locle                 18.9\nNeuchatel                23.0\nVal de Ruz               20.0\nValdeTravers             19.5\nV. De Geneve             18.0\nRive Droite              18.2\nRive Gauche              19.3\n\n\n\nswiss.lm <- lm(Fertility ~ Agriculture + Examination + Education + Catholic, swiss)\nsummary(swiss.lm)\n\n\nCall:\nlm(formula = Fertility ~ Agriculture + Examination + Education + \n    Catholic, data = swiss)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7813  -6.3308   0.8113   5.7205  15.5569 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 91.05542    6.94881  13.104  < 2e-16 ***\nAgriculture -0.22065    0.07360  -2.998  0.00455 ** \nExamination -0.26058    0.27411  -0.951  0.34722    \nEducation   -0.96161    0.19455  -4.943 1.28e-05 ***\nCatholic     0.12442    0.03727   3.339  0.00177 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.736 on 42 degrees of freedom\nMultiple R-squared:  0.6498,    Adjusted R-squared:  0.6164 \nF-statistic: 19.48 on 4 and 42 DF,  p-value: 3.95e-09\n\n#funktioniert nicht:\n# swiss %>% \n#   lm(Fertility ~ Agriculture)\n\nMit diesem Output können wir feststellen, welche Prädiktoren signifikant mit der Fertilität zusammenhängen, kontrolliert für die anderen Prädiktoren. Auch hier sehen wir wieder: lm() baut nur das Modell, erst summary() wertet es aus. In dem Fall gibt es kein Äquivalent von rstatix.\nWie oben erwähnt, kann ein anderes mögliches Ziel einer Regression sein, das Regressionsmodell mit dem besten Fit zu den Daten herauszufinden, also nur Prädiktoren nach dem Prinzip “so viel wie nötig, so wenig wie möglich” im finalen Modell zu behalten. Dafür werden mehrere Regressionsgleichungen bezüglich ihrer Varianzaufklärung miteinander verglichen.\n\n# Modelle bauen\nswiss.lm1 <- lm(Fertility ~ 1, swiss)\nswiss.lm2 <- update(swiss.lm1, ~. + Education)\nswiss.lm3 <- update(swiss.lm2, ~. + Catholic)\n\n# Modelle vergleichen\nanova(swiss.lm1, swiss.lm2, swiss.lm3)\n\nAnalysis of Variance Table\n\nModel 1: Fertility ~ 1\nModel 2: Fertility ~ Education\nModel 3: Fertility ~ Education + Catholic\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1     46 7178.0                                  \n2     45 4015.2  1    3162.7 45.564  2.66e-08 ***\n3     44 3054.2  1     961.1 13.846 0.0005598 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMithilfe der Funktion update() können wir uns sparen, das bisherige Modell noch mal komplett einzutippen. Sie funktioniert nach dem Prinzip update(altes Modell, neues Modell). Mit ~. kürzen wir das alte Modell ab, danach ergänzen wir neue Prädiktoren und/oder Interaktionen.\nDie Funktion anova() ist potentiell verwirrend: Wir rechnen hier offensichtlich keine ANOVA, wie wir sie weiter oben kennengelernt haben. Wir führen allerdings schon eine “Analyse der Varianzen” durch - nur beziehen sich die Varianzen auf jeweils die aufgeklärte Varianz der Regressionsmodelle. anova() kann als Input nur “fitted model objects” verwerten, also bereits erstellte Modelle, und gibt als Output einen Vergleich dieser Modelle.\nAnsonsten lassen sich Messwiederholungen genauso wie in der “Formelschreibweise” der ANOVA über + Error(ProbandenID/messwiederholteVariable) spezifizieren.\nInteraktionen können wir in die Formel durch * oder : einbauen: Fertility ~ Education*Catholic"
  },
  {
    "objectID": "content3.html#andere-ressourcen",
    "href": "content3.html#andere-ressourcen",
    "title": "Inferenzstatistik in R",
    "section": "Andere Ressourcen:",
    "text": "Andere Ressourcen:\n\nÜberblick über rstatx-Funktionen: https://rpkgs.datanovia.com/rstatix/\nListe von Datasets, die direkt in base R abrufbar sind: https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html"
  },
  {
    "objectID": "dataviz_ggplot2.html",
    "href": "dataviz_ggplot2.html",
    "title": "Visualisierung mit ggplot2",
    "section": "",
    "text": "Show code\nlibrary(tidyverse)\nlibrary(reshape2)\nlibrary(reactable)\n\npalette1 <- c(\"#648fff\",\"#785ef0\",\"#dc267f\",\"#fe6100\",\"#ffb000\",\"#000000\",\"#ffffff\")\npalette2 <- c(\"#CC79A7\",\"#D55E00\",\"#0072B2\",\"#F0E442\",\"#009E73\",\"#56B4E9\",\"#E69F00\",\"#000000\")\n\ndataset <- read.csv(\"assets/datasets/iris.csv\")\ndataset$Species <- as.factor(dataset$Species)"
  },
  {
    "objectID": "dataviz_ggplot2.html#hintergrund",
    "href": "dataviz_ggplot2.html#hintergrund",
    "title": "Visualisierung mit ggplot2",
    "section": "Hintergrund",
    "text": "Hintergrund\nBase-R bietet natürlich auch Funktionen zur Visualisierung von Daten. Jedoch ist es auch hier hilfreich, gleich den “state-of-the-art” zu lernen und sich ggplot2 anzueignen, da wir somit viel mehr Anpassungsmöglichkeiten, sowie eine vereinfachte Grammatik haben. Um dennoch etwas Kontext zu bieten, schauen wir uns illustrativ die base-R hist() Funktion an:\n\nhist(dataset$Sepal.Length)\n\n\n\n\nWir sehen, mit ganz wenig Code bietet base-R bereits eine Funktion zum erstellen von Histogrammen an. Das kann manchmal nützlich sein, bspw. wenn wir schnell einen Überblick über die Verteilung einer Variablen bekommen möchten. Aus den oben genannten Gründen wollen wir an dieser Stelle aber den Fokus vor allem auf ggplot2 setzen. Hinweis: ggplot2 ist der offzielle Name und auch das richtige Package, der Einfachheit halber werden wir aber von ggplot sprechen."
  },
  {
    "objectID": "dataviz_ggplot2.html#syntax",
    "href": "dataviz_ggplot2.html#syntax",
    "title": "Visualisierung mit ggplot2",
    "section": "Syntax",
    "text": "Syntax\nEbenso wie der Pipe-Operator %>% bei dplyr eine besondere Rolle spielt, tut es das + bei ggplot. Aber dazu gleich mehr. Im Folgenden sehen wir den grundsätzlichen Aufbau eines Befehls bzw. einer Befehlskette, die ein Plot erzeugt:\n\nggplot(data=object)+\n  geom_function(aes(x=variable),parameters=\"xyz\")+\n  additional_functions(parameters=\"additionals\")\n\nDas sieht auf den ersten Blick erstmal komplizierter aus, als es ist. Aber wir werden den Code Schritt für Schritt aufschlüsseln.\n\n+ Operator\nWie bereits erwähnt, spielt der + Operator eine wichtige Rolle bei ggplot2. Durch + können wir Befehle miteinander verketten. Wichtig zu verstehen ist, dass - ähnlich wie bei dplyr - das Verketten von Befehlen dem “Stapeln” verschiedener Visualisierungen bzw. Eigenschaften entspricht, und jeder einzelne Schritt etwas zum finalen Plot beiträgt. Wie genau das abläuft, werden wir gleich am Beispiel des iris Datensatzes sehen.\n\n\ngeom_function()\ngeom_function() (geom_ für geometrical) ist hier nur ein Platzhalter für eine Vielzahl verschiedener Funktionen, die wir in Abhängigkeit davon benutzen, was wir visualisieren wollen. So gibt es etwa geom_bar() für Bar-Charts, geom_point() für Punktwolken oder geom_line() für Liniendiagramme. Weiter unten werden wir einige davon beispielhaft vorstellen.\n\n\naes()\naes() steht für aesthetics und sorgt dafür, Datenpunkte in visuelle Darstellung zu übertragen bzw. zu mappen. Das klingt erstmal abstrakter, als es eigentlich ist. Meistens werden hier die Variablen für die entsprechenden Achsen angebenen (bspw. x=Sepal.Width) oder aber auch für die Farben bzw. Füllungen von Balken/Punkten (bspw. color=Species). Warum das wichtig ist und was der Unterschied zu Parametern außerhalb der aes() Funktion ist, werden wir gleich sehen.\n\n\nParameters\nWie bei allen Funktionen können wir hier auch diverse Dinge als Parameter übergeben. Beispiele sind etwa color oder size.\n\n\nadditional_functions()\nAuch additional_functions() sind hier nur ein Platzhalter für eine Vielzahl weiterer Funktionen, die wir mit in unsere ggplot Kette nehmen können, bspw. theme_classic() oder xlab(). Mithilfe dieser können wir das Plot im Nachhinein weiter anpassen."
  },
  {
    "objectID": "dataviz_ggplot2.html#beispiel",
    "href": "dataviz_ggplot2.html#beispiel",
    "title": "Visualisierung mit ggplot2",
    "section": "Beispiel",
    "text": "Beispiel\nSoweit die Syntax. Am besten lässt sich ggplot aber anhand eines Beispiels verstehen. Machen wir also mit unserem iris Datensatz weiter. Wir wollen die Verteilung der Blattgrößen besser verstehen, und entscheiden uns dazu, ein Plot mit ggplot zu erstellen. Wie bereits beschrieben, hilft der + Operator dabei, verschiedene Funktionen für das Plot miteinander zu verketten. Fangen wir also mit der ersten Ebene an:\n\nggplot(data = dataset)\n\n\n\n\nWir sehen, mit dem Befehl ggplot() haben wir ein leeres Plot erzeugt. Fügen wir nun also ein Diagramm unserer Wahl hinzu. Wir fangen mit geom_bar() an.\n\nggplot(data = dataset)+\n  geom_bar()\n\nWie wir sehen können, wirft R den Fehler :\n`stat_count()` requires an x or y aesthetic.\nDas liegt daran, dass wir die aes() Funktion vergessen haben. Ohne das mapping von Datenpunkten weiß die geom_bar() Funktion nicht, wo welcher Datenpunkt hinsoll. Versuchen wir es also nochmal, und spezifizieren wir Sepal.Width als x-Variable:\n\nggplot(data=dataset)+\n  geom_bar(aes(x=Sepal.Width))\n\n\n\n\nWie wir sehen, haben wir nun die Variable Sepal.Width erfolgreich visualisiert und entdecken eine Normalverteilung.\nDa es sich bei geom_bar() um ein Histogramm handelt, müssen wir lediglich die x-Variable übergeben, da für die y-Variable automatisch gezählt wird.\nAngenommen, wir möchten die Farbe verändern, sodass die Balken die Farbe rot haben. Dann müssen wir dies als fill Parameter der geom_bar() Funktion übergeben:\n\nggplot(data=dataset)+\n  geom_bar(aes(x=Sepal.Width), fill=\"red\")\n\n\n\n\nNun sind alle Balken rot. Was ist aber, wenn wir wollen, dass die Balken in Abhängigkeit einer Variable verschiedenfarbig sind? Dann müssen wir die fill Variable in der aes() Funktion übergeben:\n\nggplot(data=dataset)+\n  geom_bar(aes(x=Sepal.Width, fill=Species))\n\n\n\n\nPerfekt! Auf einen Blick können wir sehen, dass Pflanzen der Spezies setosaeher größere Sepal.Width haben. Bis jetzt haben wir allerdings nur einen Befehl mithilfe des + Operators verknüpft. Angenommen, uns stören die Achsenbeschriftungen und wir wollen einen Titel haben. Auch das ist einfach möglich:\n\nggplot(data=dataset)+\n  geom_bar(aes(x=Sepal.Width, fill=Species))+\n  ylab(\"Prevalence\")+\n  xlab(\"Width of Sepal\")+\n  ggtitle(\"Sepal width of Iris Flower for each Species\")\n\n\n\n\nDie Farben sehen noch etwas langweilig aus. Ein kurzer Blick auf colorhunt.co inspiriert uns und wir wollen nun die Farben #9A208C, #E11299 und #F5C6EC als Farben haben. Das können wir über scale_fill_manual() machen. Dazu speichern wir die Farben einfach in einen Vektor namens palette und übergeben diesen als Parameter.\n\npalette <- c(\"#9A208C\",\"#E11299\",\"#F5C6EC\")\n\nggplot(data=dataset)+\n  geom_bar(aes(x=Sepal.Width, fill=Species))+\n  ylab(\"Prevalence\")+\n  xlab(\"Width of Sepal\")+\n  ggtitle(\"Sepal width of Iris Flower for each Species\")+\n  scale_fill_manual(values = palette)\n\n\n\n\nWir hätten nun gerne noch einen schwarzen Rand um die Balken. Dabei hilft uns der Parameter colour in geom_bar():\n\nggplot(data=dataset)+\n  geom_bar(aes(x=Sepal.Width, fill=Species), color=\"#2b2b2b\")+\n  ylab(\"Prevalence\")+\n  xlab(\"Width of Sepal\")+\n  ggtitle(\"Sepal width of Iris Flower for each Species\")+\n  scale_fill_manual(values = palette)\n\n\n\n\nJetzt stört uns nur noch der Hintergrund, und dann sind wir zufrieden. ggplot bietet verschiedene themes, die wir einfach über den + Operator verwenden können. Wir entscheiden uns für theme_classic():\n\nggplot(data=dataset)+\n  geom_bar(aes(x=Sepal.Width, fill=Species), color=\"#2b2b2b\")+\n  ylab(\"Prevalence\")+\n  xlab(\"Width of Sepal\")+\n  ggtitle(\"Sepal width of Iris Flower for each Species\")+\n  scale_fill_manual(values = palette)+\n  theme_classic()\n\n\n\n\nSuper! Ein publizierfähiges Plot in 7 Zeilen wiederverwendbarem Code. An diesem Beispiel haben wir gesehen, wie einfach das mit ggplot und das verketten von Befehlen geht. Im Folgenden wollen wir euch noch eine Auswahl an Visualisierungsfunktionen vorstellen, die wir häufig benötigen."
  },
  {
    "objectID": "dataviz_ggplot2.html#visualisierungsfunktionen",
    "href": "dataviz_ggplot2.html#visualisierungsfunktionen",
    "title": "Visualisierung mit ggplot2",
    "section": "Visualisierungsfunktionen",
    "text": "Visualisierungsfunktionen\nWie bereits beschrieben, sind geom_ die Visualisierungsfunktionen. Jenachdem, wie unsere Daten strukturiert sind, bietet sich einige davon mehr, andere weniger an. Eine sinnvolle Einteilung geht von den Achsen bzw. zu visualisierenden Variablen aus. Dabei sollten wir uns immer die Frage stellen:\n\nWieviele Variablen möchte ich darstellen?\n\nIn einem 2-dimensionalen Koordinatensystem können wir natürlich erstmal nur zwei Variablen darstellen. Allerdings erlaubt uns ggplot auch die Visualisierung von mehr Variablen. So können wir etwa über fill bzw. colour weitere Variablen visualieren. Eine weitere Frage, die wir uns in diese Richtung stellen sollten, ist:\n\nWas für einen Datentyp haben die zu visualisierenden Variablen?\n\nJe nachdem, wie die wir diese Fragen beantworten, bieten sich dann verschiedene Funktionen an. Im obigen Beispiel haben wir eine Variable (Sepal.Width), welche intervallskaliert ist. Dafür bietet sich entsprechend ein Histogramm bzw. geom_bar()an. Doch was ist, wenn wir zwei intervallskalierte Variablen haben? Hier könnten wir bspw. die geom_point() Funktion benutzen. Für solche Fragestellungen kann das ggplot Cheatsheet sehr hilfreich sein:\n\n\n\n\n\nNeben geom_bar wollen wir zwei weitere, häufig benötigte Funktionen, sowie deren Kombination zeigen: geom_point() und geom_smooth().\n\ngeom_point()\nAngenommen, wir haben zwei intervallskalierte Variablen, die wir visualisieren wollen. Dann bietet sich geom_point() an. Versuchen wir also mal, die Verteilung von Sepal.Width und Sepal.Length zu visualisieren.\n\nggplot(data=dataset)+\n  geom_point(aes(x=Sepal.Width,y=Sepal.Length))\n\n\n\n\nSuper! Nun wäre es praktisch, wenn wir auch noch die Variable Species einbringen könnten. Dies können wir durch colour in aes() tun.\n\nggplot(data=dataset)+\n  geom_point(aes(x=Sepal.Width,y=Sepal.Length,color=Species))\n\n\n\n\nAuch hier sehen wir direkt, dass es einen Unterschied zwischen den verschiedenen Spezies zu geben scheint. Visuell können wir auch den (offensichtlichen) Zusammenhang zwischen Sepal.Width und Sepal.Length erkennen. Um das aber noch etwas besser zu verstehen, legen wir eine Regressionsgrade mithilfe von geom_smooth() rein. Das können wir über den + Operator tun - denn in ggplot können wir alle Visualisierungsfunktionen nach Belieben “stapeln”.\n\nggplot(data=dataset)+\n  geom_point(aes(x=Sepal.Width,y=Sepal.Length,color=Species))+\n  geom_smooth(aes(x=Sepal.Width,y=Sepal.Length), method=\"lm\") # lm for linear model\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nDie Regressionsgrade zeigt einen negativen Zusammenhang, obwohl wir offensichtlich einen positiven Zusammenhang vorliegen haben. Das liegt daran, dass geom_smooth() unseren Faktor Species nicht berücksichtigt. Dies können wir ändern, indem wir in dessen aes() Funktion ebenfalls colour übergeben.\n\nggplot(data=dataset)+\n  geom_point(aes(x=Sepal.Width,y=Sepal.Length,colour=Species))+\n  geom_smooth(aes(x=Sepal.Width,y=Sepal.Length, colour=Species), method=\"lm\") # lm for linear model\n\n\n\n\nVoila! Wir sehen unsere drei kleinen Regressionsgraden und den erwarteten positiven Zusammenhang."
  },
  {
    "objectID": "dataviz_ggplot2.html#fazit-weitere-resourcen",
    "href": "dataviz_ggplot2.html#fazit-weitere-resourcen",
    "title": "Visualisierung mit ggplot2",
    "section": "Fazit & Weitere Resourcen",
    "text": "Fazit & Weitere Resourcen\nWir haben nun gelernt, wie wir mithilfe von ggplot Daten visualisieren können. Natürlich war das nur die Spitze des Eisbergs, und noch viel mehr ist möglich. Einige dieser Dinge werden wir im weiteren Verlauf des Seminars kennenlernen, wenn es um das Arbeiten (und Visualisieren) von konkreten Datensätzen geht. Im Laufe des Seminars werden wir auch eine eigene “nice to know” Seite erstellen, auf der wir zusätzliches Wissen sammeln.\nNeben dieser Website hier (🥳) gibt es natürlich noch viele weitere, tolle Resourcen im Internet. Neben den offziellen Dokumentationen bspw. auf tidyverse oder CRAN wollen wir euch vor allem mit Blick auf dplyr und ggplot noch einige ans ❤️ legen:\n\nsthda.com: Be Awesome in ggplot2: A Practical Guide to be Highly Effective - R software and data visualization\nr-graph-gallery.com: A collection of charts made with the R programming language\nr-bloggers.com: DPLYR: A beginner’s guide"
  },
  {
    "objectID": "datawrangling_base.html",
    "href": "datawrangling_base.html",
    "title": "Base R",
    "section": "",
    "text": "Show code\nlibrary(tidyverse)\nlibrary(reshape2)\nlibrary(reactable)\n\npalette1 <- c(\"#648fff\",\"#785ef0\",\"#dc267f\",\"#fe6100\",\"#ffb000\",\"#000000\",\"#ffffff\")\npalette2 <- c(\"#CC79A7\",\"#D55E00\",\"#0072B2\",\"#F0E442\",\"#009E73\",\"#56B4E9\",\"#E69F00\",\"#000000\")"
  },
  {
    "objectID": "datawrangling_base.html#iris-datensatz",
    "href": "datawrangling_base.html#iris-datensatz",
    "title": "Base R",
    "section": "Iris Datensatz",
    "text": "Iris Datensatz\nBeginnen wir also zunächst damit, Daten in R einzulesen. Wir werden heute mit dem iris Datensatz arbeiten, einem berühmten Datensatz von R.A. Fisher aus dem Jahr 1936, in dem verschiedene Eigenschaften in Bezug auf die Blütengröße verschiedener Spezies der Blume Iris enthalten sind (siehe hier).\n\n\n\nSources: Wikipedia; Danielle Langlois, Денис Анисимов & Eric Hunt\n\n\nIm Zentrum des Datensatzes steht die Größe der verschiedenen Blatttypen für die jeweilige Spezies. Dabei gibt es die Blatttypen Sepal und Petal. Die folgende Grafik verdeutlicht das etwas:\n\n\n\n\n\nDer Datensatz ist direkt im Package datasets integriert. Um allerdings zu lernen, wie wir Daten bspw. aus .csv einlesen, haben wir den Datensatz nochmal als einzelne Datei vorbereitet. Diese können wir hier herunterladen:\n Iris Datensatz \n\nExkurs: Trennzeichengetrennte Textdateien & CSV\n.csv ist ein typisches Datenformat, aus dem wir mit R Daten einlesen können. Wenn wir die Datei mit einem Textbearbeitungsprogramm öffnen, sehen wir, wie die Daten dort strukturiert sind:\n\n\n\n.csv in einem Texteditor (hier Sublime Text)\n\n\nWir sehen, dass die Daten einfach hintereinander per Text in der Datei gespeichert sind, und mit einem Trennzeichen (in diesem Fall das ,) voneinander getrennt sind. Strings werden mit \" abgespeichert. Die erste Zeile beschreibt die Spaltennamen."
  },
  {
    "objectID": "datawrangling_base.html#daten-einlesen",
    "href": "datawrangling_base.html#daten-einlesen",
    "title": "Base R",
    "section": "Daten einlesen",
    "text": "Daten einlesen\nNun wissen wir also, wie .csv Dateien funktionieren. Fangen wir also an, die Daten in R-einzulesen:\n\ndataset <- read.csv(\"assets/datasets/iris.csv\")\n\nDie Funktion read.csv liest die Dateien ein. Als Parameter müssen wir lediglich den Pfad zur Datei angeben. In unserem Fall handelt es sich um einen relativen Pfad - absolute Pfade würden auch funktionieren. In unserem Fall ist die Datei wir wir wissen mit Komma (,) getrennt, daher brauchen wir nichts weiter als Parameter zu übergeben. Achtung: Das deutsche Excel speichert Dateien bei .csv oft mit Semikolon (;) als Trennzeichen ab (aufrund des Kommas als Dezimaltrennzeichen).\n\nMac User:innen aufgepasst: Wenn ihr im Finder mit der rechten Maustaste auf eine Datei klickt, und dann alt bzw. option drückt, könnt ihr direkt den Pfad zur Datei in eure Zwischenablage kopieren. Das eignet sich hier bspw. besonders gut.\n\nWenn alles geklappt hat, sollte dataset rechts in unserem Environment auftauchen. Schauen wir es uns also an. Hierzu können wir entweder rechts oben auf das Objekt klicken, damit es sich im Viewer öffnet, oder wir schauen es uns hier an:\n\ndataset\n\n\n\n  \n\n\n\nWie wir sehen gibt es mehrere Spalten für die Länge und Breite der Blätter (e.g., Sepal.Length), sowie eine Spalte mit der Spezies (Species).\nWerfen wir nun einen genaueren Blick in die Daten."
  },
  {
    "objectID": "datawrangling_base.html#daten-anschauen",
    "href": "datawrangling_base.html#daten-anschauen",
    "title": "Base R",
    "section": "Daten anschauen",
    "text": "Daten anschauen\nUm schnell einen Überblick über die Daten zu bekommen, eignet sich die summary() Funktion:\n\nsummary(dataset)\n\n       X           Sepal.Length    Sepal.Width     Petal.Length  \n Min.   :  1.00   Min.   :4.300   Min.   :2.000   Min.   :1.000  \n 1st Qu.: 38.25   1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600  \n Median : 75.50   Median :5.800   Median :3.000   Median :4.350  \n Mean   : 75.50   Mean   :5.843   Mean   :3.057   Mean   :3.758  \n 3rd Qu.:112.75   3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100  \n Max.   :150.00   Max.   :7.900   Max.   :4.400   Max.   :6.900  \n  Petal.Width      Species         \n Min.   :0.100   Length:150        \n 1st Qu.:0.300   Class :character  \n Median :1.300   Mode  :character  \n Mean   :1.199                     \n 3rd Qu.:1.800                     \n Max.   :2.500                     \n\n\nHier sehen wir direkt deskriptive Werte wie Minimum, Maximum oder Median für jede einzelne Spalte. Für die Spalte Species sehen wir allerdings lediglich, dass diese als character gespeichert ist, wenngleich es sich eigentlich um einen Faktor handelt, da wir ja bereits wissen, dass es drei Spezies gibt. Das können wir ändern:\n\ndataset$Species <- as.factor(dataset$Species)\n\nFühren wir nun den Code erneut aus, sehen wir die entsprechenden Faktorlevel bzw. Ausprägungen und die jeweiligen Datenpunkte pro Ausprägung:\n\nsummary(dataset)\n\n       X           Sepal.Length    Sepal.Width     Petal.Length  \n Min.   :  1.00   Min.   :4.300   Min.   :2.000   Min.   :1.000  \n 1st Qu.: 38.25   1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600  \n Median : 75.50   Median :5.800   Median :3.000   Median :4.350  \n Mean   : 75.50   Mean   :5.843   Mean   :3.057   Mean   :3.758  \n 3rd Qu.:112.75   3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100  \n Max.   :150.00   Max.   :7.900   Max.   :4.400   Max.   :6.900  \n  Petal.Width          Species  \n Min.   :0.100   setosa    :50  \n 1st Qu.:0.300   versicolor:50  \n Median :1.300   virginica :50  \n Mean   :1.199                  \n 3rd Qu.:1.800                  \n Max.   :2.500                  \n\n\nDas Gleiche würden wir sehen, wenn wir mit der Funktion levels die Ausprägungen für die entsprechende Spalte abfragen:\n\nlevels(dataset$Species)\n\n[1] \"setosa\"     \"versicolor\" \"virginica\""
  },
  {
    "objectID": "datawrangling_base.html#deskriptive-analysen",
    "href": "datawrangling_base.html#deskriptive-analysen",
    "title": "Base R",
    "section": "Deskriptive Analysen",
    "text": "Deskriptive Analysen\nStrenggenommen hat uns die summary() Funktion bereits viele interessante Werte für alle Spalten gegeben. Falls wir dies aber für eine einzelne Spalte machen wollen, können wir dies natürlich auch tun:\n\nmean(dataset$Sepal.Length)\n\n[1] 5.843333\n\n\nWie wir sehen, kann die mean() Funktion mit einer Spalte eines Dataframes (über den $-Operator ausgewählt) arbeiten, und gibt uns den entsprechenden Mittelwert aus. Gleich verhält es sich mit der Standardabweichung:\n\nsd(dataset$Sepal.Length)\n\n[1] 0.8280661\n\n\nWie wir in der ersten Seminarstunde bereits gesagt haben, bietet R als Statistiksoftware natürlich eine Vielzahl eingebauter Standard-Funktionen. So können wir etwa auch gleich die Korrelation zwischen Sepal.Width und Sepal.Length berechnen…\n\ncor(dataset$Sepal.Length,dataset$Sepal.Width, method = \"pearson\")\n\n[1] -0.1175698\n\n\n… oder die Quantile ausgeben lassen:\n\nquantile(dataset$Sepal.Length)\n\n  0%  25%  50%  75% 100% \n 4.3  5.1  5.8  6.4  7.9 \n\n\nDas sind natürlich alles nur Beispiele der im base Package enthaltenen Funktionen - alle weiteren können wir bspw. in der Dokumentation oder über help(base) finden."
  },
  {
    "objectID": "datawrangling_base.html#datensatz-aufteilen",
    "href": "datawrangling_base.html#datensatz-aufteilen",
    "title": "Base R",
    "section": "Datensatz aufteilen",
    "text": "Datensatz aufteilen\nAngenommen, wir wollen die Spalte Species von den anderen Spalten trennen, und in einem separaten Aufgabenblatt abspeichern. Dies können wir über die $ und <- Operatoren machen:\n\ndataset_species <- dataset$Species\n\nDies speichert die entsprechende Spalte als Objekt dataset_species in unserem Environment. Dies geschieht als Vektor des Typs factor. Die Länge des Vektors entspricht natürlich der Anzahl an Observations:\n\nlength(dataset_species)\n\n[1] 150\n\n\nNun wollen wir einen Schritt weitergehen, und nicht nur einzelne Spalten extrahieren, sondern unseren Datensatz auf Basis gewisser Kriterien filtern, bzw. ein Subset bilden. Base R bietet dafür die subset() Funktion. Deren Syntax ist folgendermaßen:\n\ndataset_filtered <- subset(dataset, Sepal.Width > 3)\n\nWir definieren das betreffende Objekt dataset, sowie die Spalte, nach der wir filtern wollen (Sepal.Width) und unsere Bedingung (> 3).\nWir wollen aber noch weiter spezifizieren. Angenommen, wir wollen, dass zusätzlich nur Blumen der Spezies setosa im Datensatz enthalten sind, und wir nur die Spalten Sepal.Width und Sepal.Length betrachten wollen:\n\ndataset_filtered2 <- subset(dataset, Sepal.Width > 3 & Species == \"setosa\",select = c(Sepal.Width,Sepal.Length,Species))\n\nDer obige Befehl hat das Objekt dataset_filtered2 erzeugt, welches die entsprechenden Filterungen beinhaltet.\nWir haben nun also gelernt, wie wir sogenannte Subsets von Dataframes auf Basis von Variablenausprägungen und Spalten erzeugen können, und diese enstprechend in unserem Environment speichern können.\nWenn wir nun allerdings einen Blick in unser Environment werfen, und uns vorstellen, wie dieses nach einer R-Session aussehen könnte, stellt sich vielleicht eine Frage: Wie können wir den Überblick behalten?\n\n\n\nHow the author imagines Environment overload\n\n\nDabei, sowie bei vielen anderen Dingen die mit Data Wrangling zu tun haben, hilft uns das dplyr package. Dazu auf der nächsten Seite mehr."
  },
  {
    "objectID": "datawrangling_dplyr.html",
    "href": "datawrangling_dplyr.html",
    "title": "Wrangling mit dplyr",
    "section": "",
    "text": "Show code\nlibrary(tidyverse)\nlibrary(reshape2)\nlibrary(reactable)\n\npalette1 <- c(\"#648fff\",\"#785ef0\",\"#dc267f\",\"#fe6100\",\"#ffb000\",\"#000000\",\"#ffffff\")\npalette2 <- c(\"#CC79A7\",\"#D55E00\",\"#0072B2\",\"#F0E442\",\"#009E73\",\"#56B4E9\",\"#E69F00\",\"#000000\")\n\ndataset <- read.csv(\"assets/datasets/iris.csv\")\ndataset$Species <- as.factor(dataset$Species)"
  },
  {
    "objectID": "datawrangling_dplyr.html#hintergrund",
    "href": "datawrangling_dplyr.html#hintergrund",
    "title": "Wrangling mit dplyr",
    "section": "Hintergrund",
    "text": "Hintergrund\nViele verschiedene Objekte im Environment sind nur eine Motivation, dplyr zu benutzen. Vielmehr bietet es “eine einfache Grammatik der Datenmanipulation, welche einfach zu lernen & anzuwenden ist” (vgl. tidyverse.com) und viel Flexibilität bietet - daher gehört es auch zu den beliebtesten R-Packages, besonders wenn es um Datenanalyse geht."
  },
  {
    "objectID": "datawrangling_dplyr.html#der-pipe-operator",
    "href": "datawrangling_dplyr.html#der-pipe-operator",
    "title": "Wrangling mit dplyr",
    "section": "Der Pipe-Operator",
    "text": "Der Pipe-Operator\nIm Zentrum von dplyr steht der sogenannte Pipe-Operator %>% (ursprünglich aus dem Magrittr Package). Dieser schaut zunächst etwas ungewohnt aus, daran gewöhnt man sich aber schnell:\n\nobject %>% \n  do_something(parameters = \"xyz\")\n\nMithilfe des Pipe Operators können wir verschiedene Befehler aneinanderketten. Der Output des jeweiligen Befehls wird sozusagen als Input in die nächste Zeile “gepiped”. Das heißt, wenn der Input ein Dataframe ist, mit dem dann entsprechend Anpassungen vorgenommen werden, so ist der Output für die nächste Zeile wieder ein Dataframe - welches diese dann weiterverarbeitet.\nFür den Operator gibt es natürlich eine Tastenkombination, mithilfe derer wir ihn schnell eingeben können. Diese lautet COMMAND + SHIFT + M für Mac-User, und STRG + SHIFT + M für Windows User.\nWir erinnern uns, dass der obige Befehl üblicherweise so lauten würde:\n\ndo_something(object, parameters = \"xyz\")\n\nHier müssen wir das betreffende Objekt als Parameter übergeben, und anschließend den Output entweder direkt verwerten, oder über <- speichern. Wenn wir nun eine zweite Funktion do_something_else für das gleiche Objekt verwenden wollten, müssten wir dies erneut speichern, oder überspeichern:\n\nresult <- do_something(object, parameters = \"xyz\")\n\ndo_something_else(result, parameters = \"abc\")\n\nMithilfe des Pipe Operators %>% können wir uns das sparen. Wir starten mit dem ursprünglichen Objekt, und geben dann die bearbeiteten Versionen in die jeweilige Zeile weiter:\n\nobject %>% \n  do_something(parameters = \"xyz\") %>% \n  do_something_else(parameters = \"abc\")\n\nDer Output von do_something() wird also zum Input von do_something_else() ."
  },
  {
    "objectID": "datawrangling_dplyr.html#funktionen",
    "href": "datawrangling_dplyr.html#funktionen",
    "title": "Wrangling mit dplyr",
    "section": "Funktionen",
    "text": "Funktionen\ndplyr ist aber natürlich nicht nur wegen des Pipe-Operators so praktisch. Es bringt auch viele Funktionen mit sich, die bei typischen Datenanalyse-Aufgaben relevant sind. Diese Funktionen ermöglichen ähnliche Dinge wie bspw. das auf der vorigen Seite gezeigte subset(), sind dabei jedoch etwas weniger umständlich und intuitiver. Im Folgenden wollen wir euch die wichtigsten davon vorstellen:\n\nfilter()\nBeschreibung von tidyverse.org:\n\nThe filter() function is used to subset a data frame, retaining all rows that satisfy your conditions. To be retained, the row must produce a value of TRUE for all conditions. Note that when a condition evaluates to NA the row will be dropped, unlike base subsetting with [.\n\n\nMit der filter() Funktion können wir, wie der Name schon sagt, Datensätze auf Basis gewisser Kriterien filtern. Angenommen, wir wollen wieder nach der Spezies setosa filtern. Dies geschieht mit dplyr wie folgt:\n\ndataset %>% \n  filter(Species == \"setosa\")\n\n\n\n  \n\n\n\nEbenso können wir mehrere Bedingungen kombinieren:\n\ndataset %>% \n  filter(Species == \"setosa\" & Sepal.Length < 5)\n\n\n\n  \n\n\n\nDas besondere an dplyr ist, dass wir dies nun aber auch in zwei Schritten machen könnten:\n\ndataset %>% \n  filter(Species == \"setosa\") %>% \n  filter(Sepal.Length < 5)\n\n\n\n  \n\n\n\nWie zu sehen ist, sind die resultierenden Dataframes dieselben.\n\n\nselect()\nBeschreibung von tidyverse.org:\n\nSelect (and optionally rename) variables in a data frame, using a concise mini-language that makes it easy to refer to variables based on their name (e.g. a:f selects all columns from a on the left to f on the right) or type (e.g. where(is.numeric) selects all numeric columns).\n\n\nEbenso wie mit select() in subset() können wir hiermit verschiedene Spalten auswählen. Die Syntax ist wie folgt:\n\ndataset %>% \n  select(Sepal.Length,Sepal.Width)\n\n\n\n  \n\n\n\nDamit wählen wir die Spalten Sepal.Length und Sepal.Width aus. Wir könnten auch sagen, wir wollen alle Spalten außer Species:\n\ndataset %>% \n  select(-Species)\n\n\n\n  \n\n\n\nOft kommt es vor, dass wir bspw. durch Limesurvey wissen, dass alle Variablen eines Fragebogens mit “SQ..” anfangen. Sollten wir diese alle schnell auswählen wollen (bspw. zum Berechnen von Summenscores), kann uns dplyr auch dabei helfen, mithilfe von starts_with().\n\ndataset %>% \n  select(starts_with(\"Se\"))\n\n\n\n  \n\n\n\n\n\nmutate()\nBeschreibung von tidyverse.org:\n\nmutate() creates new columns that are functions of existing variables. It can also modify (if the name is the same as an existing column) and delete columns (by setting their value to NULL).\n\n\nApropos Summenscores - mutate() klingt gruseliger, als es ist. Mit mutate() können wir neue Variablen kreieren, oder bestehende modifizieren. Angenommen, die Blätter der Iris-Blume wären rechteckig, und wir könnten die Fläche Petal.Square einfach in Quadratzentimeter berechnen:\n\ndataset %>% \n  mutate(Petal.Square = Petal.Length*Petal.Width)\n\n\n\n  \n\n\n\nWir sehen, die Syntax ist mutate(neue_variable = berechnungen). Wir können auch mehrere Variablen in einem Zug erzeugen:\n\ndataset %>% \n  mutate(Petal.Square = Petal.Length*Petal.Width,\n         Sepal.Square = Sepal.Length*Sepal.Width)\n\n\n\n  \n\n\n\nWenn der neue_variable Name gleich wie der alte ist, überschreiben wir die Spalte:\n\ndataset %>% \n  mutate(Sepal.Length = Sepal.Length/10)\n\n\n\n  \n\n\n\n\n\narrange()\nBeschreibung von tidyverse.org:\n\narrange() orders the rows of a data frame by the values of selected columns. Unlike other dplyr verbs, arrange() largely ignores grouping; you need to explicitly mention grouping variables (or use  .by_group = TRUE) in order to group by them, and functions of variables are evaluated once per data frame, not once per group.\n\n\nManchmal kommt es vor, dass wir einen Datensatz sortieren wollen, um einen besseren Überblick über die Daten zu bekommen. Dazu bietet sich arrange() an:\n\ndataset %>% \n  arrange(Sepal.Length)\n\n\n\n  \n\n\n\nWir sehen, die Spalte Sepal.Length wird in aufsteigender Reihenfole sortiert. Wollen wir es in absteigender Reihenfolge haben, müssen wir die Funktion desc() dazunehmen:\n\ndataset %>% \n  arrange(desc(Sepal.Length))\n\n\n\n  \n\n\n\n\n\ngroup_by()\nBeschreibung von tidyverse.org:\n\nMost data operations are done on groups defined by variables. group_by() takes an existing tbl and converts it into a grouped tbl where operations are performed “by group”. ungroup() removes grouping.\n\n\nEine der hilfreichsten Funktionen von dplyr ist die group_by() Funktion. Hier ist allerdings eine etwas genauere Erklärung erforderlich. Die generelle Syntax ist die folgende:\n\nobject %>% \n  group_by(variable) %>% \n  do_something()\n\nWir übergeben der group_by() Funktion als Parameter eine Variable, nach der diese den Datensatz gruppieren soll. Hier ist es wichtig zu verstehen, dass diese Gruppierung für alle nachfolgenden Schritte gilt, selbst aber keinen Effekt hat. Was heißt das genau? Betrachten wir folgenden Code:\n\ndataset %>% \n  group_by(Species)\n\n\n\n  \n\n\n\nAußer der Info Groups: Species [3] hat sich nichts verändert. Nehmen wir allerdings in der nächsten Zeile eine Funktion hinzu (bspw. mutate()), so verändert das, wie diese sich verhält. Normalerweise würde folgender Code die Variable Sepal.Length_mean erzeugen, welche den Mittelwert von Sepal.Length enthält:\n\ndataset %>% \n  mutate(Sepal.Length_mean = mean(Sepal.Length))\n\n\n\n  \n\n\n\nDa dieser natürlich für den gesamten Datensatz berechnet wird, beträgt er für jede Messung 5.84. Wenn wir vorher allerdings group_by() eingeben, sieht das anders aus:\n\ndataset %>% \n  group_by(Species) %>% \n  mutate(Sepal.Length_mean = mean(Sepal.Length))\n\n\n\n  \n\n\n\nWir sehen, dass mutate() den Wert pro Gruppe berechnet hat. Dies kann sehr hilfreich für verschiedenste Anwendungen sein. Ebenso können wir nach mehreren Variablen gruppieren. Dazu aber gleich mehr.\n\n\nsummarise()\nBeschreibung von tidyverse.org:\n\nsummarise() creates a new data frame. It returns one row for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarising all observations in the input. It will contain one column for each grouping variable and one column for each of the summary statistics that you have specified. summarise() and summarize() are synonyms.\n\n\nVielleicht wirkt es etwas merkwürdig, dass wir oben die Variable Sepal.Length_mean für jeden Datenpunkt einzeln berechnet haben, obwohl diese natürlich immer gleich ist. Meistens wollen wir solche Werte für die gesamte Stichprobe haben. Dabei hilft uns summarise():\n\ndataset %>% \n  summarise(Sepal.Length_mean = mean(Sepal.Length))\n\n\n\n  \n\n\n\nWir sehen, dass die Syntax dieselbe wie die von mutate() ist, nur dass der Output eben nur eine Zahl (bzw. eine Zeile ist) - eben die Summary unseres Datensatzes unter den gegebenen Bedingungen. Besonders hilfreich ist summarise() in Kombination mit group_by():\n\ndataset %>% \n  group_by(Species) %>% \n  summarise(Sepal.Length_mean = mean(Sepal.Length))\n\n\n\n  \n\n\n\nSo können wir mit drei Zeilen die Mittelwerte für die jeweilige Gruppe sehen. Wir können natürlich auch gleich die Standardabweichung mitberechnen:\n\ndataset %>% \n  group_by(Species) %>% \n  summarise(Sepal.Length_mean = mean(Sepal.Length),\n            Sepal.Length_sd = sd(Sepal.Length))\n\n\n\n  \n\n\n\n\nFür Expert:innen:\nOben wurde kurz erwähnt, dass wir auch nach mehreren Variablen gruppieren können. Angenommen, es gäbe noch zusätzlich die Variable color in den Ausprägungen purple, blue und white für jeden Datenpunkt, d.h., jede Blüte kann auch eine dieser verschiedenen Farben haben.\n\n\nShow code\ndataset %>% \n  mutate(color = as.factor(sample(rep(c(\"purple\",\"blue\",\"white\"),50)))) -> dataset\n\n\nWenn wir nun nach den Variablen Species und color gruppieren wollen, müssen wir diese beiden Variablen als Parameter übergeben:\n\ndataset %>% \n  group_by(Species, color) %>% \n  summarise(Sepal.Length_mean = mean(Sepal.Length),\n            Sepal.Length_sd = sd(Sepal.Length))\n\n\n\n  \n\n\n\nNun sehen wir die Mittelwerte für die verschiedenen Kombinationen der Variablen (also bspw. setosa & blue). Hier kann es hilfreich sein, die Anzahl an Zeilen pro Gruppe zu bekommen. Hierfür eignet sich die n() Funktion:\n\ndataset %>% \n  group_by(Species, color) %>% \n  summarise(Sepal.Length_mean = mean(Sepal.Length),\n            Sepal.Length_sd = sd(Sepal.Length),\n            n = n())\n\n\n\n  \n\n\n\nNun haben wir die Mittelwerte für jede Kombination aus Species und color, sowie die Anzahl n der jeweiligen Zeilen bzw. Messungen."
  },
  {
    "objectID": "datawrangling_dplyr.html#temporär-oder-speichern",
    "href": "datawrangling_dplyr.html#temporär-oder-speichern",
    "title": "Wrangling mit dplyr",
    "section": "Temporär oder Speichern?",
    "text": "Temporär oder Speichern?\nSicherlich ist aufgefallen, dass alle unsere gezeigten “Pipes” nur zu einem Output im Chunk geführt haben, wir diese allerdings nicht in unserem Environment gespeichert haben. Natürlich können wir dies tun, indem wir unsere Pipe einem Objekt über den Zuweisungsoperator <- zuweisen:\n\nobject <- object %>% \n            select(variableA,variableB,variableC) %>% \n            mutate(variableA*100)\n\nNatürlich ist uns selbst überlassen, wie wir mit den Daten umgehen. Manchmal wird es Fälle geben, in denen Speichern Sinn ergibt (bspw. wenn sonst immer wieder die gleiche Transformation anstünde), oft wird es aber auch ausreichend sein, die Daten einfach im Output des jeweiligen Chunks anzuschauen, bzw. etwaige Transformationen unmittelbar vor weiteren Schritten zu machen (bspw. beim Erzeugen von Plots).\nAchtung: Wenn wir mit summarise() einen Output bekommen, so ist dieser natürlich nicht mehr das “ursprüngliche” Dataframe. Diesen würden wir uns also üblicherweise im Chunk-Output anschauen."
  },
  {
    "objectID": "datawrangling_dplyr.html#fazit",
    "href": "datawrangling_dplyr.html#fazit",
    "title": "Wrangling mit dplyr",
    "section": "Fazit",
    "text": "Fazit\ndplyr bringt eine einfach verständliche Grammatik mit sich, die uns hilft, uns auf das wesentliche zu konzentrieren: die Daten. Im nächsten Kapitel werden wir lernen, wie wir diese Daten visualiseren können, um sie noch besser zu verstehen."
  },
  {
    "objectID": "Deskriptives.html",
    "href": "Deskriptives.html",
    "title": "Deskriptives",
    "section": "",
    "text": "Dataframes\n\neinlesen\ndamit arbeiten\n\nDeskriptive Analyse\n\nmean/sd/sum für einzelne Spalten\nDatensatz aufteilen (i.e., subset)\nAufzeigen, was das Problem ist ⇒ Überleitung zu Dplyr\n\nDplyr\n\nEinleitung\nSyntax (i.e., Konzept von Pipes)\nBeispiele\nAufteilen des Datensatzes\n\nggplot\n\nEinleitung\nSyntax (i.e., Konzept von Pipes)\nBeispiele\nBar Plot\nLine Plot\n\n\n\n\nShow code\nlibrary(tidyverse)\nlibrary(reshape2)\n\npalette1 <- c(\"#648fff\",\"#785ef0\",\"#dc267f\",\"#fe6100\",\"#ffb000\",\"#000000\",\"#ffffff\")\npalette2 <- c(\"#CC79A7\",\"#D55E00\",\"#0072B2\",\"#F0E442\",\"#009E73\",\"#56B4E9\",\"#E69F00\",\"#000000\")"
  },
  {
    "objectID": "Deskriptives.html#einlesen-von-daten",
    "href": "Deskriptives.html#einlesen-von-daten",
    "title": "Deskriptives",
    "section": "Einlesen von Daten",
    "text": "Einlesen von Daten\nBeginnen wir also zunächst damit, Daten in R einzulesen. Wir werden heute mit dem iris Datensatz arbeiten, einem berühmten Datensatz von R.A. Fisher aus dem Jahr 1936, in dem verschiedene Eigenschaften in Bezug auf die Blütengröße verschiedener Spezies der Blume Iris enthalten sind (siehe hier).\n\n\n\nSources: Wikipedia ([Iris Versicolor](https://de.wikipedia.org/wiki/Verschiedenfarbige_Schwertlilie#/media/Datei:Iris_versicolor_1.jpg), [Iris Setosa](https://en.wikipedia.org/wiki/Iris_setosa#/media/File:Irissetosa1.jpg), [Iris Virginica](https://en.wikipedia.org/wiki/Iris_virginica#/media/File:Iris_virginica_2.jpg))\n\n\nIm Zentrum des Datensatzes steht die Größe der verschiedenen Blatttypen für die jeweilige Spezies. Dabei gibt es die Blatttypen Sepal und Petal. Die folgende Grafik verdeutlicht das etwas:\n\n\n\n\n\n\n\nShow code\ndatasets::iri"
  },
  {
    "objectID": "folien/120423 - Website.html",
    "href": "folien/120423 - Website.html",
    "title": "Introduction & R-Basics",
    "section": "",
    "text": "Verteilung Vorwissen"
  },
  {
    "objectID": "folien/120423 - Website.html#vorstellungsrunde-1",
    "href": "folien/120423 - Website.html#vorstellungsrunde-1",
    "title": "Introduction & R-Basics",
    "section": "Vorstellungsrunde",
    "text": "Vorstellungsrunde\nVorwissen Programmiersprachen"
  },
  {
    "objectID": "folien/120423 - Website.html#vorstellungsrunde-2",
    "href": "folien/120423 - Website.html#vorstellungsrunde-2",
    "title": "Introduction & R-Basics",
    "section": "Vorstellungsrunde",
    "text": "Vorstellungsrunde\nErwartungen ans Seminar"
  },
  {
    "objectID": "folien/120423 - Website.html#ziele-des-seminars",
    "href": "folien/120423 - Website.html#ziele-des-seminars",
    "title": "Introduction & R-Basics",
    "section": "Ziele des Seminars",
    "text": "Ziele des Seminars\n\nDaten einlesen, bearbeiten, grafisch darstellen und statistisch auswerten\nEinblick in Breite und Tiefe, die R bietet\nGrundlagen schaffen, auf die ihr selbstständig aufbauen könnt (z.B. für Masterarbeit oder eigene Projekte)"
  },
  {
    "objectID": "folien/120423 - Website.html#zeitplan",
    "href": "folien/120423 - Website.html#zeitplan",
    "title": "Introduction & R-Basics",
    "section": "Zeitplan",
    "text": "Zeitplan\n\n7-8 Sitzungen, die jeweils 3h dauern\n\nMittagspause?\n\n31.05 vrsl. letzte, kürzere Sitzung\nInhaltlich: Basics, Deskriptives, Inferenzstatistik, Auswertung von Forschungsdaten\ninnerhalb der Sitzung: abwechselnd Blöcke von Theorie + Praxis\nWebsite für Präsentationsinhalte, Lösungen, Cheatsheets usw. (selbst Notizen machen lohnt sich auch!)"
  },
  {
    "objectID": "folien/120423 - Website.html#prüfungsleistung",
    "href": "folien/120423 - Website.html#prüfungsleistung",
    "title": "Introduction & R-Basics",
    "section": "Prüfungsleistung",
    "text": "Prüfungsleistung\n\nWöchentliche “Hausaufgaben”: Wiederholen und erweitern das im Seminar Gelernte\n\nAbgabe per Moodle spätestens am Sonntag vor der nächsten Sitzung\nwerden benotet, am Ende per Durchschnitt zusammengefasst und machen gemeinsam 50% der Note aus\nAbgabe mit der schlechtesten Benotung wird aus der Durchschnittsberechnung entfernt\nEinzelarbeit (Plagiatscheck)\nWir laden nach Abgabe “Musterlösung” hoch\n\n1x Hausaufgabe vorstellen (unbenotet)\nBericht zu (neuem) Datensatz einreichen (50% der Note) - mehr Infos folgen\n\nEs gibt keine Anwesenheitspflicht, wir empfehlen aber immer da zu sein, da euch die Prüfungsleistungen dann leichter fallen werden."
  },
  {
    "objectID": "folien/120423 - Website.html#was-ist-r",
    "href": "folien/120423 - Website.html#was-ist-r",
    "title": "Introduction & R-Basics",
    "section": "Was ist R?",
    "text": "Was ist R?\n\nProgrammiersprache (und -oberfläche) für statistische Berechnungen und Grafiken\nopen-source & kostenlos\ngroße Community, die ständig Funktionen erweitert und bei Problemen hilft\nR Studio ist die meistgenutzte Benutzeroberfläche und macht die Handhabung einfacher"
  },
  {
    "objectID": "folien/120423 - Website.html#user-interface",
    "href": "folien/120423 - Website.html#user-interface",
    "title": "Introduction & R-Basics",
    "section": "User Interface",
    "text": "User Interface\n\n\noben links: Code\nunten links: Konsole - ausgeführter Code und Output\n\nhier lässt sich auch Code eintippen, dieser kann allerdings nicht gespeichert werden\n\noben rechts: u.a. Environment - welche Objekte und Daten exisitieren\nunten rechts: Dateizugriff, Plotanzeige, Hilfeanzeige, Package-Übersicht"
  },
  {
    "objectID": "folien/120423 - Website.html#code-erstellen-ausführen",
    "href": "folien/120423 - Website.html#code-erstellen-ausführen",
    "title": "Introduction & R-Basics",
    "section": "Code erstellen + ausführen",
    "text": "Code erstellen + ausführen\n\nÖffnet das File das ihr auf der Seite mit unseren Installationstips heruntergeladen habt (vielleicht ist es auch noch offen)\nIn dem File ist nur die Zeile ohne #ausführbarer Code. Wie hier geschehen, lassen sich mit # Kommentare in den Code hinzufügen, die dem Code Struktur geben oder erklären, was hier passieren soll\nFührt die Zeile Code aus, in dem ihr euren Cursor in die Zeile stellt und STRG+ENTER drückt\nÜber das Markieren von mehreren/allen Zeilen werden die entsprechenden Zeilen nacheinander ausgeführt\n\n\n# Erstes Skript zum Seminar Medienbasierte Wissenskonstruktion: R\n\n# Satz in Konsole printen:\nprint(paste0(\"Ich habe \",R.version$version.string,\" und RStudio v\",rstudioapi::versionInfo()$version))\n\n\n\n[1] \"Ich habe R version 4.2.1 (2022-06-23 ucrt) und RStudio v2023.3.0.386\""
  },
  {
    "objectID": "folien/120423 - Website.html#r-vs.-quarto",
    "href": "folien/120423 - Website.html#r-vs.-quarto",
    "title": "Introduction & R-Basics",
    "section": ".R vs. Quarto",
    "text": ".R vs. Quarto\n\nR’s Standardformat sind .R-Dateien\nFür “echte Arbeit” mit R sind Quarto-Dokumente meistens nützlicher:\n\nermöglichen Export/Rendern in andere Dateiformate (html, pdf, docx, pptx,…)\nermöglichen neben Code-Abschnitte auch Text-Abschnitte, die formatiert werden können (wie LaTeX)\nermöglichen einzustellen, ob nur Code, nur Output, oder Code+Output angezeigt werden soll\nAuswertung & Bericht können somit im gleichen Dokument geschehen\nDiese Präsentation ist auch in R entstanden!"
  },
  {
    "objectID": "folien/120423 - Website.html#r-vs.-quarto-1",
    "href": "folien/120423 - Website.html#r-vs.-quarto-1",
    "title": "Introduction & R-Basics",
    "section": ".R vs. Quarto",
    "text": ".R vs. Quarto"
  },
  {
    "objectID": "folien/120423 - Website.html#quarto---dokument-aufsetzen",
    "href": "folien/120423 - Website.html#quarto---dokument-aufsetzen",
    "title": "Introduction & R-Basics",
    "section": "Quarto - Dokument aufsetzen",
    "text": "Quarto - Dokument aufsetzen\n\nErstellt ein Quarto-Dokument über File > New File > Quarto Document\nIm auftauchenden Fenster könnt ihr einstellen:\n\nTitel & AutorIn des Dokuments\ngewünschtes Output-Format (hier: html)\nwomit das Quarto-Dokument gerendert werden soll (hier: knitr)"
  },
  {
    "objectID": "folien/120423 - Website.html#quarto---im-dokument",
    "href": "folien/120423 - Website.html#quarto---im-dokument",
    "title": "Introduction & R-Basics",
    "section": "Quarto - im Dokument",
    "text": "Quarto - im Dokument\n\nDer Kasten oben heißt “YAML-header” - dort sind die Rendereinstellungen (Titel, Speicherort, Format, Schriftgröße usw.)\nIm Feld darunter kann Text geschrieben und formatiert werden\n\nSource: Format lässt sich über bestimmte Zeichen einstellen\nVisual: Format lässt sich über Markieren + Auswählen einstellen\n\nCode kann man nur innerhalb von Chunks einfügen (oben rechts auf das grün unterlegte C klicken oder STRG+ALT+I)\nIm grau unterlegten Kästchen lässt sich “normaler” R-Code schreiben und ausführen\n\n“Vorschau” über den Play-Button\nÜber #| am Zeilenbeginn lässt sich einstellen, was mit Code + Output beim Rendern passieren soll\n\nÜber “Render” wird das Dokument in das Zielformat umgewandelt\nIn den meisten Fällen rendert man nur gegen Ende und nutzt während der Analyse die Chunk-Vorschau"
  },
  {
    "objectID": "folien/120423 - Website.html#quarto",
    "href": "folien/120423 - Website.html#quarto",
    "title": "Introduction & R-Basics",
    "section": "Quarto",
    "text": "Quarto\n\nErstellt zwei Code-Chunks\nFügt bei einem der Chunks in die erste Zeile #| echo: TRUE\nFügt beim anderen Chunk in die erste Zeile #| echo: FALSE\nSchreibt in den Text- und in die Code-Bereiche 4+5\nRendert das Dokument\n\nOutput ohne Code:\n\n4+5\n\n[1] 9\n\n\nOutput mit Code:\n\n4+5\n\n[1] 9\n\n\nReiner Text:\n4+5"
  },
  {
    "objectID": "folien/120423 - Website.html#rechenoperatoren",
    "href": "folien/120423 - Website.html#rechenoperatoren",
    "title": "Introduction & R-Basics",
    "section": "(Rechen)Operatoren",
    "text": "(Rechen)Operatoren\n\nDezimaltrennzeichen: Punkt . , nicht Komma ,\n+ - * /\nExponent: ^\nlogisch gleich: ==\nlogisch ungleich: !=\nlogisches und: &\nlogisches oder: |\nlogisches exklusives oder: xor\nnicht: !"
  },
  {
    "objectID": "folien/120423 - Website.html#objekte",
    "href": "folien/120423 - Website.html#objekte",
    "title": "Introduction & R-Basics",
    "section": "Objekte",
    "text": "Objekte\n\nAnstatt nur Code durchzuführen und das Ergebnis in der Konsole abzulesen, können wir auch alles mögliche als Objekte speichern\nKonzept: links mit Objektname beginnen, dann den Zuweisungsoperator und anschließend das, was als Objekt gespeichert werden soll\n\n\nergebnis <- 3+5\n\n\nR führt den Code rechts vom Pfeil aus - das Objekt ist “8”, nicht “3+5”!\nObjekte lassen sich abrufen, indem man ihren Namen schreibt und ausführt\n\n\nergebnis\n\n[1] 8\n\n\n\nR ist case-sensitive, d.h. Groß-und Kleinschreibung muss übereinstimmen!\nObjekte lassen sich auch oben rechts im Environment einsehen (wird aber irgendwann unübersichtlich…)\nErstellt selbst eine Rechenaufgabe und speichert sie als Objekt!\nWas passiert, wenn man den gleichen Objektnamen erneut verwendet?"
  },
  {
    "objectID": "folien/120423 - Website.html#objekte-1",
    "href": "folien/120423 - Website.html#objekte-1",
    "title": "Introduction & R-Basics",
    "section": "Objekte",
    "text": "Objekte\n\nWarum ist Objekte erstellen wichtig?\n\nFür R ist jeder Output sonst “flüchtig”: Es wird nicht wirklich etwas verändert\nZwischenergebnisse überprüfen\nParameter festlegen\nFlüchtigkeitsfehler vermeiden\nuvm.\n\nShortcut für <- : ALT+-\nPfeil in die andere Richtung -> und Gleichheitszeichen = funktionieren nicht bzw. sollten nicht verwendet werden!"
  },
  {
    "objectID": "folien/120423 - Website.html#funktionen",
    "href": "folien/120423 - Website.html#funktionen",
    "title": "Introduction & R-Basics",
    "section": "Funktionen",
    "text": "Funktionen\n\nMithilfe von Funktionen lassen sich Objekte/Daten bearbeiten\nBeispiel: round(3.45, digits = 1) rundet 3.45 auf die erste Nachkommastelle\n\nam Anfang steht der Name der Funktion (auch hier: case-sensitive)\ndie Argumente der Funktion werden von runden Klammern eingerahmt\nArgumente sind bspw. die Daten, auf die die Funktion angewendet werden soll, aber auch weitere Einstellungen der Funktion, z.B. auf wie viele Nachkommastellen gerundet werden soll\nsie folgen der Syntax Argument = Angabe\nArgumente werden durch Komma getrennt\nmanche Argumente sind zwingend notwendig, viele aber optional\ndie Argumente einer Funktion haben eine bestimmte Reihenfolge, daher kann man den Argumentnamen häufig weglassen, z.B. round(3.45, 1) (solange man die Funktion noch nicht gut kennt, oder auch um den Überblick zu behalten, besser die Argumente explizit nennen)"
  },
  {
    "objectID": "folien/120423 - Website.html#hilfe-in-r",
    "href": "folien/120423 - Website.html#hilfe-in-r",
    "title": "Introduction & R-Basics",
    "section": "Hilfe in R",
    "text": "Hilfe in R\n\nÜber help(Funktion) wird die Hilfeseite/Dokumentation der Funktion “Funktion” aufgerufen\n\ngenauso funktioniert ?Funktion\nauf der Hilfeseite stehen mögliche Argumente, ihre Reihenfolge und in welchem Format das Argument angegeben wird (wird eine Zahl oder ein Wort erwartet?)\n\nFindet mit der Hilfe-Funktion heraus, was mean() macht und welche Argumente es hat\n\n\n?mean\n#oder\nhelp(mean)"
  },
  {
    "objectID": "folien/120423 - Website.html#hilfe-außerhalb-von-r",
    "href": "folien/120423 - Website.html#hilfe-außerhalb-von-r",
    "title": "Introduction & R-Basics",
    "section": "Hilfe außerhalb von R",
    "text": "Hilfe außerhalb von R\n\nFehlermeldungen lesen und versuchen, Problem zu verstehen\ndas Internet!\n\nFehlermeldung bei Google einfügen und Links durchsuchen\nStack Overflow\nStatt selbst eine Frage einreichen zu müssen findet sich fast immer ein Thread mit dem gleichen/ähnlichen Problem und möglichen Lösungen\n\nDanach: Lösung für sich dokumentieren"
  },
  {
    "objectID": "folien/120423 - Website.html#packages",
    "href": "folien/120423 - Website.html#packages",
    "title": "Introduction & R-Basics",
    "section": "Packages",
    "text": "Packages\n\nFrisch nach der Installation besteht R aus Base R und einigen wenigen Erweiterungen\nDurch die Installation von weiteren Paketen/Packages lassen sich die Funktionen fast beliebig erweitern: Jedes Package enthält Funktionen und/oder Datensätze, meistens zu bestimmten Analysen, mehr oder weniger nischig\nIm Fenster unten rechts unter “Packages” könnt ihr sehen, welche Packages schon installiert und/oder aktiviert sind\nNeue Packages lassen sich mit install.packages(\"package-name\") installieren und mit library(package-name) aktivieren (auf die Anführungszeichen achten!)\nMan kann neue Funktionen auch selbst schreiben, das ist aber meistens nur in Sonderfällen notwendig"
  },
  {
    "objectID": "folien/120423 - Website.html#packages-1",
    "href": "folien/120423 - Website.html#packages-1",
    "title": "Introduction & R-Basics",
    "section": "Packages",
    "text": "Packages\n\nInstalliert und aktiviert das Package psych (Erinnerung: install.packages()& library())\nWendet die Hilfefunktion help() auf sich selbst an, um herauszufinden, wie ihr mit ihr Informationen über Packages bekommt\nSchaut euch mit dem neuen Wissen an, welche Funktionen das Package psych beinhaltet\n\n\n# Installieren\ninstall.packages(\"psych\")\n# Aktivieren\nlibrary(psych)\n\n# Mehr über help() herausfinden\nhelp(help) # Inception!\n# Aha, das Argument \"package\" kann mir weiterhelfen!\n\n# Über psych informieren\nhelp(package=psych)"
  },
  {
    "objectID": "folien/120423 - Website.html#datentypen",
    "href": "folien/120423 - Website.html#datentypen",
    "title": "Introduction & R-Basics",
    "section": "Datentypen",
    "text": "Datentypen\n\nBasic:\n\nnumeric: Zahlen (integer, double)\ncharacter: Buchstabenfolgen (strings)\nfactor: z.B. Faktorstufen eines Faktors\nlogical: TRUE, FALSE (vgl. 1 & 0)\n\nVektor: eindimensionale “Aufzählung” von Elementen\nMatrix: ein- bis zweidimensionale Aufzählung von Elementen\nArray: beliebig-dimensionale Aufzählung von Elementen\nListe: kann verschiedene Datentypen und Strukturen enthalten (z.B. Ergebnis einer ANOVA)\nDataframe, Tibble: Spezifische Formate, um Datensätze darzustellen\n\nKönnen pro Spalte unterschiedliche Datentypen beinhalten\nStandardformat, wenn man von extern Daten in R reinlädt"
  },
  {
    "objectID": "folien/120423 - Website.html#datentypen-1",
    "href": "folien/120423 - Website.html#datentypen-1",
    "title": "Introduction & R-Basics",
    "section": "Datentypen",
    "text": "Datentypen\nDer Datensatz aus der Umfrage sieht als Vorschau in der Konsole so aus:\n\n\nWarning: Paket 'tidyverse' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'ggplot2' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'tibble' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'tidyr' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'readr' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'purrr' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'dplyr' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'stringr' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'forcats' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'lubridate' wurde unter R Version 4.2.3 erstellt\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nNew names:\nRows: 4 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): sex, r_problems\ndbl  (5): ...1, id, lastpage, age, hopes_perc\nlgl  (3): r_knowledge, r_knowledge_amount, programming_knowledge\ndttm (1): submitdate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n  \n\n\n\n\n\nZeile: Ein Tibble mit diesen Dimensionen wird angezeigt\n\n\nZeile: Spaltennamen\n\n\nZeile: Datentypen\n\nfolgende Zeilen: Daten\nin der Konsole werden Daten meistens nur gekürzt angezeigt\nÜber view(daten) öffnet sich eine Tabellenübersicht"
  },
  {
    "objectID": "folien/120423 - Website.html#bestimmte-daten-abrufen",
    "href": "folien/120423 - Website.html#bestimmte-daten-abrufen",
    "title": "Introduction & R-Basics",
    "section": "Bestimmte Daten abrufen",
    "text": "Bestimmte Daten abrufen\n\nIn eckigen Klammern nach dem Objektnamen lassen sich die “Koordinaten” angeben\n\n\numfrage[2,3] # zeigt den Wert der zweiten Zeile und dritten Spalte\n\n\n\n  \n\n\n\n\nStatt Zahlen können auch Spaltennamen benutzt werden\n\n\numfrage[3, \"geschlecht\"]\n\n\n\n  \n\n\n\n\nSpalten lassen sich auch direkt über $ ansprechen.\n\n\numfrage$geschlecht\n\n[1] <NA>   female female male  \nLevels: female male"
  },
  {
    "objectID": "folien/120423 - Website.html#bestimmte-daten-abrufen-1",
    "href": "folien/120423 - Website.html#bestimmte-daten-abrufen-1",
    "title": "Introduction & R-Basics",
    "section": "Bestimmte Daten abrufen",
    "text": "Bestimmte Daten abrufen\n\nUm alles anzeigen zu lassen, kann man die Koordinate “leer” lassen\n\n\numfrage[ , \"geschlecht\"]\n\n\n\n  \n\n\n\n\numfrage[ , ]"
  },
  {
    "objectID": "folien/120423 - Website.html#datentypen-2",
    "href": "folien/120423 - Website.html#datentypen-2",
    "title": "Introduction & R-Basics",
    "section": "Datentypen",
    "text": "Datentypen\n\nR erkennt häufig von selbst, welcher Datentyp gemeint ist\nHäufig ist es aber sinnvoll, noch mal “von Hand” zu überprüfen und/oder den richtigen Datentyp festzulegen\ntypeof() um den Datentyp zu erfragen\nis.numeric() / is.factor() / is.logical() / is.character() um einen bestimmten Datentyp zu testen\nas.numeric() usw. nutzen, um den Datentyp zu verändern\n\n\numfrage$r.probleme <- as.factor(umfrage$r.probleme)\n# Die umgewandelte Spalte überschreibt die alte Spalte!"
  },
  {
    "objectID": "folien/120423 - Website.html#neue-daten-erstellen",
    "href": "folien/120423 - Website.html#neue-daten-erstellen",
    "title": "Introduction & R-Basics",
    "section": "Neue Daten erstellen",
    "text": "Neue Daten erstellen\n\nVektoren: c()\nKombiniert alle aufgezählten Objekte\n\n\nvektor <- c(1,3,5)\nvektor\n\n[1] 1 3 5\n\n\n\nStrings müssen in Anführungszeichen gesetzt werden, damit R sie nicht mit Objekten verwechselt.\nIm Output erkennt man Strings auch an den Anführungszeichen\n\n\nstring <- \"hallo\"\nstring\n\n[1] \"hallo\""
  },
  {
    "objectID": "folien/120423 - Website.html#unterschiede-zu-anderen-programmiersprachen",
    "href": "folien/120423 - Website.html#unterschiede-zu-anderen-programmiersprachen",
    "title": "Introduction & R-Basics",
    "section": "Unterschiede zu anderen Programmiersprachen",
    "text": "Unterschiede zu anderen Programmiersprachen\n\nkeine Unterscheidung zwischen integer und double\nverwendet keine Pointer\ntendenziell: Viele Angelegenheiten, bei denen andere Sprachen empfindlich sind, sind in R simpler aufgebaut"
  },
  {
    "objectID": "folien/120423.html#vorstellungsrunde",
    "href": "folien/120423.html#vorstellungsrunde",
    "title": "R Basics - 12.04.23",
    "section": "Vorstellungsrunde",
    "text": "Vorstellungsrunde\n\nWer sind wir?"
  },
  {
    "objectID": "folien/120423.html#vorstellungsrunde-1",
    "href": "folien/120423.html#vorstellungsrunde-1",
    "title": "R Basics - 12.04.23",
    "section": "Vorstellungsrunde",
    "text": "Vorstellungsrunde\nVerteilung Vorwissen"
  },
  {
    "objectID": "folien/120423.html#vorstellungsrunde-2",
    "href": "folien/120423.html#vorstellungsrunde-2",
    "title": "R Basics - 12.04.23",
    "section": "Vorstellungsrunde",
    "text": "Vorstellungsrunde\nVorwissen Programmiersprachen"
  },
  {
    "objectID": "folien/120423.html#vorstellungsrunde-3",
    "href": "folien/120423.html#vorstellungsrunde-3",
    "title": "R Basics - 12.04.23",
    "section": "Vorstellungsrunde",
    "text": "Vorstellungsrunde\nErwartungen ans Seminar"
  },
  {
    "objectID": "folien/120423.html#vorstellungsrunde-4",
    "href": "folien/120423.html#vorstellungsrunde-4",
    "title": "R Basics - 12.04.23",
    "section": "Vorstellungsrunde",
    "text": "Vorstellungsrunde\n\nWer seid ihr?\n\nSemester\nWarum Interesse an R?\nFun Fact"
  },
  {
    "objectID": "folien/120423.html#ziele-des-seminars",
    "href": "folien/120423.html#ziele-des-seminars",
    "title": "R Basics - 12.04.23",
    "section": "Ziele des Seminars",
    "text": "Ziele des Seminars\n\nDaten einlesen, bearbeiten, grafisch darstellen und statistisch auswerten\nEinblick in Breite und Tiefe, die R bietet\nGrundlagen schaffen, auf die ihr selbstständig aufbauen könnt (z.B. für Masterarbeit oder eigene Projekte)"
  },
  {
    "objectID": "folien/120423.html#zeitplan",
    "href": "folien/120423.html#zeitplan",
    "title": "R Basics - 12.04.23",
    "section": "Zeitplan",
    "text": "Zeitplan\n\n7-8 Sitzungen, die jeweils 3h dauern\n\nMittagspause?\n\n31.05 vrsl. letzte, kürzere Sitzung\nInhaltlich: Basics, Deskriptives, Inferenzstatistik, Auswertung von Forschungsdaten\ninnerhalb der Sitzung: abwechselnd Blöcke von Theorie + Praxis\nWebsite für Präsentationsinhalte, Lösungen, Cheatsheets usw. (selbst Notizen machen lohnt sich auch!)\n\nLink einfügen"
  },
  {
    "objectID": "folien/120423.html#prüfungsleistung",
    "href": "folien/120423.html#prüfungsleistung",
    "title": "R Basics - 12.04.23",
    "section": "Prüfungsleistung",
    "text": "Prüfungsleistung\n\nWöchentliche “Hausaufgaben”: Wiederholen und erweitern das im Seminar Gelernte\n\nAbgabe per Moodle spätestens am Sonntag vor der nächsten Sitzung\nwerden benotet, am Ende per Durchschnitt zusammengefasst und machen gemeinsam 50% der Note aus\nAbgabe mit der schlechtesten Benotung wird aus der Durchschnittsberechnung entfernt\nEinzelarbeit (Plagiatscheck)\nWir laden nach Abgabe “Musterlösung” hoch\n\n1x Hausaufgabe vorstellen (unbenotet)\nBericht zu (neuem) Datensatz einreichen (50% der Note) - mehr Infos folgen\n\n\n\nEs gibt keine Anwesenheitspflicht, wir empfehlen aber immer da zu sein, da euch die Prüfungsleistungen dann leichter fallen werden."
  },
  {
    "objectID": "folien/120423.html#was-ist-r",
    "href": "folien/120423.html#was-ist-r",
    "title": "R Basics - 12.04.23",
    "section": "Was ist R?",
    "text": "Was ist R?\n\nProgrammiersprache (und -oberfläche) für statistische Berechnungen und Grafiken\nopen-source & kostenlos\ngroße Community, die ständig Funktionen erweitert und bei Problemen hilft\nR Studio ist die meistgenutzte Benutzeroberfläche und macht die Handhabung einfacher"
  },
  {
    "objectID": "folien/120423.html#user-interface",
    "href": "folien/120423.html#user-interface",
    "title": "R Basics - 12.04.23",
    "section": "User Interface",
    "text": "User Interface\n\n\n\n\noben links: Code\nunten links: Konsole - ausgeführter Code und Output\n\nhier lässt sich auch Code eintippen, dieser kann allerdings nicht gespeichert werden\n\noben rechts: u.a. Environment - welche Objekte und Daten exisitieren\nunten rechts: Dateizugriff, Plotanzeige, Hilfeanzeige, Package-Übersicht"
  },
  {
    "objectID": "folien/120423.html#code-erstellen-ausführen",
    "href": "folien/120423.html#code-erstellen-ausführen",
    "title": "R Basics - 12.04.23",
    "section": "Code erstellen + ausführen",
    "text": "Code erstellen + ausführen\n\n\nÖffnet das File das ihr auf der Seite mit unseren Installationstips heruntergeladen habt (vielleicht ist es auch noch offen)\nIn dem File ist nur die Zeile ohne #ausführbarer Code. Wie hier geschehen, lassen sich mit # Kommentare in den Code hinzufügen, die dem Code Struktur geben oder erklären, was hier passieren soll\nFührt die Zeile Code aus, in dem ihr euren Cursor in die Zeile stellt und STRG+ENTER drückt\nÜber das Markieren von mehreren/allen Zeilen werden die entsprechenden Zeilen nacheinander ausgeführt\n\n\n\n\n# Erstes Skript zum Seminar Medienbasierte Wissenskonstruktion: R\n\n# Satz in Konsole printen:\nprint(paste0(\"Ich habe \",R.version$version.string,\" und RStudio v\",rstudioapi::versionInfo()$version))\n\n\n\n[1] \"Ich habe R version 4.2.1 (2022-06-23 ucrt) und RStudio v2023.3.0.386\""
  },
  {
    "objectID": "folien/120423.html#r-vs.-quarto",
    "href": "folien/120423.html#r-vs.-quarto",
    "title": "R Basics - 12.04.23",
    "section": ".R vs. Quarto",
    "text": ".R vs. Quarto\n\n\nR’s Standardformat sind .R-Dateien\nFür “echte Arbeit” mit R sind Quarto-Dokumente meistens nützlicher:\n\nermöglichen Export/Rendern in andere Dateiformate (html, pdf, docx, pptx,…)\nermöglichen neben Code-Abschnitte auch Text-Abschnitte, die formatiert werden können (wie LaTeX)\nermöglichen einzustellen, ob nur Code, nur Output, oder Code+Output angezeigt werden soll\nAuswertung & Bericht können somit im gleichen Dokument geschehen\nDiese Präsentation ist auch in R entstanden!"
  },
  {
    "objectID": "folien/120423.html#r-vs.-quarto-1",
    "href": "folien/120423.html#r-vs.-quarto-1",
    "title": "R Basics - 12.04.23",
    "section": ".R vs. Quarto",
    "text": ".R vs. Quarto"
  },
  {
    "objectID": "folien/120423.html#quarto---dokument-aufsetzen",
    "href": "folien/120423.html#quarto---dokument-aufsetzen",
    "title": "R Basics - 12.04.23",
    "section": "Quarto - Dokument aufsetzen",
    "text": "Quarto - Dokument aufsetzen\n\nErstellt ein Quarto-Dokument über File > New File > Quarto Document\nIm auftauchenden Fenster könnt ihr einstellen:\n\nTitel & AutorIn des Dokuments\ngewünschtes Output-Format (hier: html)\nwomit das Quarto-Dokument gerendert werden soll (hier: knitr)"
  },
  {
    "objectID": "folien/120423.html#quarto---im-dokument",
    "href": "folien/120423.html#quarto---im-dokument",
    "title": "R Basics - 12.04.23",
    "section": "Quarto - im Dokument",
    "text": "Quarto - im Dokument\n\n\nDer Kasten oben heißt “YAML-header” - dort sind die Rendereinstellungen (Titel, Speicherort, Format, Schriftgröße usw.)\nIm Feld darunter kann Text geschrieben und formatiert werden\n\nSource: Format lässt sich über bestimmte Zeichen einstellen\nVisual: Format lässt sich über Markieren + Auswählen einstellen\n\nCode kann man nur innerhalb von Chunks einfügen (oben rechts auf das grün unterlegte C klicken oder STRG+ALT+I)\nIm grau unterlegten Kästchen lässt sich “normaler” R-Code schreiben und ausführen\n\n“Vorschau” über den Play-Button\nÜber #| am Zeilenbeginn lässt sich einstellen, was mit Code + Output beim Rendern passieren soll\n\nÜber “Render” wird das Dokument in das Zielformat umgewandelt\nIn den meisten Fällen rendert man nur gegen Ende und nutzt während der Analyse die Chunk-Vorschau"
  },
  {
    "objectID": "folien/120423.html#quarto",
    "href": "folien/120423.html#quarto",
    "title": "R Basics - 12.04.23",
    "section": "Quarto",
    "text": "Quarto\n\nErstellt zwei Code-Chunks\nFügt bei einem der Chunks in die erste Zeile #| echo: TRUE\nSchreibt in den Text- und in die Code-Bereiche 4+5\nRendert das Dokument\n\n\nOutput ohne Code:\n\n\n[1] 9\n\n\n\n\nOutput mit Code:\n\n4+5\n\n[1] 9\n\n\n\n\nReiner Text: 4+5"
  },
  {
    "objectID": "folien/120423.html#rechenoperatoren",
    "href": "folien/120423.html#rechenoperatoren",
    "title": "R Basics - 12.04.23",
    "section": "(Rechen)Operatoren",
    "text": "(Rechen)Operatoren\n\nDezimaltrennzeichen: Punkt . , nicht Komma ,\n+ - * /\nExponent: ^\nlogisch gleich: ==\nlogisch ungleich: !=\nlogisches und: &\nlogisches oder: |\nlogisches exklusives oder: xor\nnicht: !"
  },
  {
    "objectID": "folien/120423.html#objekte",
    "href": "folien/120423.html#objekte",
    "title": "R Basics - 12.04.23",
    "section": "Objekte",
    "text": "Objekte\n\n\nAnstatt nur Code durchzuführen und das Ergebnis in der Konsole abzulesen, können wir auch alles mögliche als Objekte speichern\nKonzept: links mit Objektname beginnen, dann den Zuweisungsoperator und anschließend das, was als Objekt gespeichert werden soll\n\n\n\n\nergebnis <- 3+5\n\n\n\nR führt den Code rechts vom Pfeil aus - das Objekt ist “8”, nicht “3+5”!\nObjekte lassen sich abrufen, indem man ihren Namen schreibt und ausführt\n\n\n\n\n\nergebnis\n\n[1] 8\n\n\n\n\nR ist case-sensitive, d.h. Groß-und Kleinschreibung muss übereinstimmen!\nObjekte lassen sich auch oben rechts im Environment einsehen (wird aber irgendwann unübersichtlich…)\nErstellt selbst eine Rechenaufgabe und speichert sie als Objekt!\nWas passiert, wenn man den gleichen Objektnamen erneut verwendet?"
  },
  {
    "objectID": "folien/120423.html#objekte-1",
    "href": "folien/120423.html#objekte-1",
    "title": "R Basics - 12.04.23",
    "section": "Objekte",
    "text": "Objekte\n\n\nWarum ist Objekte erstellen wichtig?\n\nFür R ist jeder Output sonst “flüchtig”: Es wird nicht wirklich etwas verändert\nZwischenergebnisse überprüfen\nParameter festlegen\nFlüchtigkeitsfehler vermeiden\nuvm.\n\nShortcut für <- : ALT+-\nPfeil in die andere Richtung -> und Gleichheitszeichen = funktionieren nicht bzw. sollten nicht verwendet werden!"
  },
  {
    "objectID": "folien/120423.html#funktionen",
    "href": "folien/120423.html#funktionen",
    "title": "R Basics - 12.04.23",
    "section": "Funktionen",
    "text": "Funktionen\n\n\nMithilfe von Funktionen lassen sich Objekte/Daten bearbeiten\nBeispiel: round(3.45, digits = 1) rundet 3.45 auf die erste Nachkommastelle\n\nam Anfang steht der Name der Funktion (auch hier: case-sensitive)\ndie Argumente der Funktion werden von runden Klammern eingerahmt\nArgumente sind bspw. die Daten, auf die die Funktion angewendet werden soll, aber auch weitere Einstellungen der Funktion, z.B. auf wie viele Nachkommastellen gerundet werden soll\nsie folgen der Syntax Argument = Angabe\nArgumente werden durch Komma getrennt\nmanche Argumente sind zwingend notwendig, viele aber optional\ndie Argumente einer Funktion haben eine bestimmte Reihenfolge, daher kann man den Argumentnamen häufig weglassen, z.B. round(3.45, 1) (solange man die Funktion noch nicht gut kennt, oder auch um den Überblick zu behalten, besser die Argumente explizit nennen)"
  },
  {
    "objectID": "folien/120423.html#hilfe-in-r",
    "href": "folien/120423.html#hilfe-in-r",
    "title": "R Basics - 12.04.23",
    "section": "Hilfe in R",
    "text": "Hilfe in R\n\n\nÜber help(Funktion) wird die Hilfeseite/Dokumentation der Funktion “Funktion” aufgerufen\n\ngenauso funktioniert ?Funktion\nauf der Hilfeseite stehen mögliche Argumente, ihre Reihenfolge und in welchem Format das Argument angegeben wird (wird eine Zahl oder ein Wort erwartet?)\n\nFindet mit der Hilfe-Funktion heraus, was mean() macht und welche Argumente es hat\n\n\n\n\n?mean\n#oder\nhelp(mean)"
  },
  {
    "objectID": "folien/120423.html#hilfe-außerhalb-von-r",
    "href": "folien/120423.html#hilfe-außerhalb-von-r",
    "title": "R Basics - 12.04.23",
    "section": "Hilfe außerhalb von R",
    "text": "Hilfe außerhalb von R\n\n\nFehlermeldungen lesen und versuchen, Problem zu verstehen\ndas Internet!\n\nFehlermeldung bei Google einfügen und Links durchsuchen\nStack Overflow\nStatt selbst eine Frage einreichen zu müssen findet sich fast immer ein Thread mit dem gleichen/ähnlichen Problem und möglichen Lösungen\n\nDanach: Lösung für sich dokumentieren"
  },
  {
    "objectID": "folien/120423.html#packages",
    "href": "folien/120423.html#packages",
    "title": "R Basics - 12.04.23",
    "section": "Packages",
    "text": "Packages\n\n\nFrisch nach der Installation besteht R aus Base R und einigen wenigen Erweiterungen\nDurch die Installation von weiteren Paketen/Packages lassen sich die Funktionen fast beliebig erweitern: Jedes Package enthält Funktionen und/oder Datensätze, meistens zu bestimmten Analysen, mehr oder weniger nischig\nIm Fenster unten rechts unter “Packages” könnt ihr sehen, welche Packages schon installiert und/oder aktiviert sind\nNeue Packages lassen sich mit install.packages(\"package-name\") installieren und mit library(package-name) aktivieren (auf die Anführungszeichen achten!)\nMan kann neue Funktionen auch selbst schreiben, das ist aber meistens nur in Sonderfällen notwendig"
  },
  {
    "objectID": "folien/120423.html#packages-1",
    "href": "folien/120423.html#packages-1",
    "title": "R Basics - 12.04.23",
    "section": "Packages",
    "text": "Packages\n\n\nInstalliert und aktiviert das Package psych (Erinnerung: install.packages()& library())\nWendet die Hilfefunktion help() auf sich selbst an, um herauszufinden, wie ihr mit ihr Informationen über Packages bekommt\nSchaut euch mit dem neuen Wissen an, welche Funktionen das Package psych beinhaltet\n\n\n\n\n# Installieren\ninstall.packages(\"psych\")\n# Aktivieren\nlibrary(psych)\n\n# Mehr über help() herausfinden\nhelp(help) # Inception!\n# Aha, das Argument \"package\" kann mir weiterhelfen!\n\n# Über psych informieren\nhelp(package=psych)"
  },
  {
    "objectID": "folien/120423.html#datentypen",
    "href": "folien/120423.html#datentypen",
    "title": "R Basics - 12.04.23",
    "section": "Datentypen",
    "text": "Datentypen\n\nBasic:\n\nnumeric: Zahlen (integer, double)\ncharacter: Buchstabenfolgen (strings)\nfactor: z.B. Faktorstufen eines Faktors\nlogical: TRUE, FALSE (vgl. 1 & 0)\n\nVektor: eindimensionale “Aufzählung” von Elementen des gleichen Datentyps\nMatrix: ein- bis zweidimensionale Aufzählung von Elementen des gleichen Datentyps\nArray: beliebig-dimensionale Aufzählung von Elementen des gleichen Datentyps\nListe: kann verschiedene Datentypen und Strukturen enthalten (z.B. Ergebnis einer ANOVA)\nDataframe, Tibble: Spezifische Formate, um Datensätze darzustellen\n\nKönnen pro Spalte unterschiedliche Datentypen beinhalten\nStandardformat, wenn man von extern Daten in R reinlädt"
  },
  {
    "objectID": "folien/120423.html#datentypen-1",
    "href": "folien/120423.html#datentypen-1",
    "title": "R Basics - 12.04.23",
    "section": "Datentypen",
    "text": "Datentypen\nDer Datensatz aus der Umfrage sieht als Vorschau in der Konsole so aus:\n\n\n# A tibble: 4 × 7\n     id alter geschlecht r.vorwissen r.wieviel.vorwissen r.probleme\n  <dbl> <dbl> <fct>      <lgl>       <lgl>               <chr>     \n1     1    NA <NA>       NA          NA                  <NA>      \n2     2    12 female     TRUE        TRUE                Test      \n3     3    19 female     TRUE        TRUE                kp        \n4     4    20 male       TRUE        TRUE                problemz  \n# ℹ 1 more variable: programmier.vorwissen <lgl>\n\n\n\n\nZeile: Ein Tibble mit diesen Dimensionen wird angezeigt\n\n\nZeile: Spaltennamen\n\n\nZeile: Datentypen\n\nfolgende Zeilen: Daten\nin der Konsole werden Daten meistens nur gekürzt angezeigt\nÜber view(daten) öffnet sich eine Tabellenübersicht"
  },
  {
    "objectID": "folien/120423.html#bestimmte-daten-abrufen",
    "href": "folien/120423.html#bestimmte-daten-abrufen",
    "title": "R Basics - 12.04.23",
    "section": "Bestimmte Daten abrufen",
    "text": "Bestimmte Daten abrufen\n\nIn eckigen Klammern nach dem Objektnamen lassen sich die “Koordinaten” angeben\n\n\numfrage[2,3] # zeigt den Wert der zweiten Spalte und dritten Zeile\n\n# A tibble: 1 × 1\n  geschlecht\n  <fct>     \n1 female    \n\n\n\nStatt Zahlen können auch Spaltennamen benutzt werden\n\n\numfrage[3, \"geschlecht\"]\n\n# A tibble: 1 × 1\n  geschlecht\n  <fct>     \n1 female    \n\n\n\nSpalten lassen sich auch direkt über $ ansprechen.\n\n\numfrage$geschlecht\n\n[1] <NA>   female female male  \nLevels: female male"
  },
  {
    "objectID": "folien/120423.html#bestimmte-daten-abrufen-1",
    "href": "folien/120423.html#bestimmte-daten-abrufen-1",
    "title": "R Basics - 12.04.23",
    "section": "Bestimmte Daten abrufen",
    "text": "Bestimmte Daten abrufen\n\nUm alles anzeigen zu lassen, kann man die Koordinate “leer” lassen\n\n\numfrage[ , \"geschlecht\"]\n\n# A tibble: 4 × 1\n  geschlecht\n  <fct>     \n1 <NA>      \n2 female    \n3 female    \n4 male      \n\n\n\n\numfrage[ , ]\n\n# A tibble: 4 × 7\n     id alter geschlecht r.vorwissen r.wieviel.vorwissen r.probleme\n  <dbl> <dbl> <fct>      <lgl>       <lgl>               <chr>     \n1     1    NA <NA>       NA          NA                  <NA>      \n2     2    12 female     TRUE        TRUE                Test      \n3     3    19 female     TRUE        TRUE                kp        \n4     4    20 male       TRUE        TRUE                problemz  \n# ℹ 1 more variable: programmier.vorwissen <lgl>"
  },
  {
    "objectID": "folien/120423.html#datentypen-2",
    "href": "folien/120423.html#datentypen-2",
    "title": "R Basics - 12.04.23",
    "section": "Datentypen",
    "text": "Datentypen\n\n\nR erkennt häufig von selbst, welcher Datentyp gemeint ist\nHäufig ist es aber sinnvoll, noch mal “von Hand” zu überprüfen und/oder den richtigen Datentyp festzulegen\ntypeof() um den Datentyp zu erfragen\nis.numeric() / is.factor() / is.logical() / is.character() um einen bestimmten Datentyp zu testen\nas.numeric() usw. nutzen, um den Datentyp zu verändern\n\n\n\n\numfrage$r.probleme <- as.factor(umfrage$r.probleme)\n# Die umgewandelte Spalte überschreibt die alte Spalte!"
  },
  {
    "objectID": "folien/120423.html#neue-daten-erstellen",
    "href": "folien/120423.html#neue-daten-erstellen",
    "title": "R Basics - 12.04.23",
    "section": "Neue Daten erstellen",
    "text": "Neue Daten erstellen\n\n\nVektoren: c()\nKombiniert alle aufgezählten Objekte\n\n\n\n\nvektor <- c(1,3,5)\nvektor\n\n[1] 1 3 5\n\n\n\n\nStrings müssen in Anführungszeichen gesetzt werden, damit R sie nicht mit Objekten verwechselt.\nIm Output erkennt man Strings auch an den Anführungszeichen\n\n\n\n\n\nstring <- \"hallo\"\nstring\n\n[1] \"hallo\""
  },
  {
    "objectID": "folien/120423.html#unterschiede-zu-anderen-programmiersprachen",
    "href": "folien/120423.html#unterschiede-zu-anderen-programmiersprachen",
    "title": "R Basics - 12.04.23",
    "section": "Unterschiede zu anderen Programmiersprachen",
    "text": "Unterschiede zu anderen Programmiersprachen\n\n\nkeine Unterscheidung zwischen integer und double\nverwendet keine Pointer\ntendenziell: Viele Angelegenheiten, bei denen andere Sprachen empfindlich sind, sind in R simpler aufgebaut"
  },
  {
    "objectID": "folien/120423.html#fragen",
    "href": "folien/120423.html#fragen",
    "title": "R Basics - 12.04.23",
    "section": "Fragen?",
    "text": "Fragen?"
  },
  {
    "objectID": "folien/test fuer fehler.html",
    "href": "folien/test fuer fehler.html",
    "title": "Untitled",
    "section": "",
    "text": "print(paste0(\"Ich habe \",R.version$version.string,\" und RStudio v\",rstudioapi::versionInfo()$version))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Seminar Medienbasierte Wissenskonstruktion: R - Website",
    "section": "",
    "text": "Herzlich willkommen auf unserer seminarbegleitenden Website. Hier findet ihr die Seminarskripte (i.e., im Seminar verwendete .qmd Dateien), Lösungen der Übungsblätter sowie weitere, hilfreiche Informationen.\n\n\n\n\n\n\n\nDie Folien sind hier zu finden.\n\n\n\n\nBase R\nData Wrangling mit dplyr\nData visualisation mit ggplot2\n\n\n\n\n\nInferenzstatistik in R\nBerichte in R\n\n\n\n\n\nSkalen erstellen mit mutate() und across()\nDeskr. Auswertung & Wide/Long-Format\nInferenzstatistische Analyse"
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Download & Installation von R",
    "section": "",
    "text": "In Folgenden erklären wir, wie ihr euch R und RStudio herunterladen könnt. Zwar gibt es im Web einige How-Tos und Tutorials, wir wollen euch an dieser Stelle der Vollständigkeit halber aber nochmal vermitteln, wie genau ihr euch die Software installiert und was es dabei zu beachten gibt. Falls während der Installation Fragen aufkommen, meldet euch gerne per Mail bei uns (simon.krukowski@stud.uni-due.de) oder schaut einfach im Web nach How-Tos/Troubleshooting. Das ist generell auch eine gute Idee beim Thema R, Programmieren & Datenanalyse - mehr dazu erfahrt ihr dann im Seminar.\n\nÜberblick\nDa dein Computer R als Programmiersprache nicht von selbst versteht, musst du R zunächst installieren. Sobald du R installiert hast, kannst du es theoretisch benutzen, und Daten damit auswerten. Hierbei ist wichtig zu differenzieren, dass du R zwar in der Kommandozeile/Terminal verwenden kannst, dies aber wenig intuitiv ist. Links in Figure 1 ist bspw. zu sehen, wir wir R in der Kommandozeile nutzen.\n\n\n\nFigure 1: R & Rstudio\n\n\nTrotzdem ist das wenig intuitiv. Daher bietet es sich an, eine IDE (Entwicklungsumgebung, siehe hier) zu benutzen. Die populärste IDE dafür ist RStudio. Im Rahmen dieses Seminars werden wir mit RStudio arbeiten. Wie du beides installierst, erfährst du in diesem Notebook.\n\n\nDownload & Installation\nStarten wir also mit dem Download. Besuche folgende Website:\n\n\n\nFigure 2: posit.co\n\n\nWenn du dem Link folgst, wirst du auf die offzielle Seite von R (CRAN) weitergeleitet, und kannst dir dort die für dein System richtige Version auswählen:\n\n\n\nFigure 3: CRAN\n\n\nIn unserem Fall haben wir einen Mac, und wählen den entsprechenden Link aus. Auf der folgenden Seite gibt es viele verschiedene Links, hier ist es wichtig den bei latest release und für euer System richtigen zu nehmen (in unserem Fall Intel Mac).\n\n\n\nFigure 4: CRAN\n\n\nNach dem Download könnt ihr den Installer ausführen und voilà - ihr habt R installiert! Da bei uns R schon installiert ist gibt es hier leider keinen Screenshot.\nTheoretisch könntet ihr R nun wie oben beschrieben in der Kommandozeile verwenden. Wir wollen allerdings RStudio benutzen, also gehen wir wieder zurück zu posit.co und laden dort die richtige Version für unser System herunter (die richtige Version wird hier automatisch vorgeschlagen):\n\n\n\nFigure 5: posit.co\n\n\nJe nach System unterscheidet sich hier die Installation etwas, aber am Ende sollte RStudio bei dir installiert sein. Öffnet also R, und es sollte etwa wie folgt aussehen:\n\n\n\nFigure 6: So sollte RStudio aussehen\n\n\n\n\nR-Skripte ausführen\nAls erste Übung wollen wir ein Test-Skript ausführen. Ladet euch das Skript zunächst herunter:\n R Skript \nÖffnet das Skript durch einen Doppelklick mit RStudio und führt es aus. Wenn ihr alles richtig gemacht habt, sollte etwas in der Konsole erscheinen. Diesen Output werden wir in der ersten Seminarstunde besprechen.\n\n\nTroubleshooting\nHaben sich bei der Installation Probleme ergeben? Meldet euch gerne per Mail an simon.krukowski@stud.uni-due.de."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction & Basics",
    "section": "",
    "text": "Verteilung Vorwissen"
  },
  {
    "objectID": "intro.html#vorstellungsrunde-1",
    "href": "intro.html#vorstellungsrunde-1",
    "title": "Introduction & Basics",
    "section": "Vorstellungsrunde",
    "text": "Vorstellungsrunde\nVorwissen Programmiersprachen"
  },
  {
    "objectID": "intro.html#vorstellungsrunde-2",
    "href": "intro.html#vorstellungsrunde-2",
    "title": "Introduction & Basics",
    "section": "Vorstellungsrunde",
    "text": "Vorstellungsrunde\nErwartungen ans Seminar"
  },
  {
    "objectID": "intro.html#ziele-des-seminars",
    "href": "intro.html#ziele-des-seminars",
    "title": "Introduction & Basics",
    "section": "Ziele des Seminars",
    "text": "Ziele des Seminars\n\nDaten einlesen, bearbeiten, grafisch darstellen und statistisch auswerten\nEinblick in Breite und Tiefe, die R bietet\nGrundlagen schaffen, auf die ihr selbstständig aufbauen könnt (z.B. für Masterarbeit oder eigene Projekte)"
  },
  {
    "objectID": "intro.html#zeitplan",
    "href": "intro.html#zeitplan",
    "title": "Introduction & Basics",
    "section": "Zeitplan",
    "text": "Zeitplan\n\n7-8 Sitzungen, die jeweils 3h dauern\n\nMittagspause?\n\n31.05 vrsl. letzte, kürzere Sitzung\nInhaltlich: Basics, Deskriptives, Inferenzstatistik, Auswertung von Forschungsdaten\ninnerhalb der Sitzung: abwechselnd Blöcke von Theorie + Praxis\nWebsite für Präsentationsinhalte, Lösungen, Cheatsheets usw. (selbst Notizen machen lohnt sich auch!)"
  },
  {
    "objectID": "intro.html#prüfungsleistung",
    "href": "intro.html#prüfungsleistung",
    "title": "Introduction & Basics",
    "section": "Prüfungsleistung",
    "text": "Prüfungsleistung\n\nWöchentliche “Hausaufgaben”: Wiederholen und erweitern das im Seminar Gelernte\n\nAbgabe per Moodle spätestens am Sonntag vor der nächsten Sitzung\nwerden benotet, am Ende per Durchschnitt zusammengefasst und machen gemeinsam 50% der Note aus\nAbgabe mit der schlechtesten Benotung wird aus der Durchschnittsberechnung entfernt\nEinzelarbeit (Plagiatscheck)\nWir laden nach Abgabe “Musterlösung” hoch\n\n1x Hausaufgabe vorstellen (unbenotet)\nBericht zu (neuem) Datensatz einreichen (50% der Note) - mehr Infos folgen\n\nEs gibt keine Anwesenheitspflicht, wir empfehlen aber immer da zu sein, da euch die Prüfungsleistungen dann leichter fallen werden."
  },
  {
    "objectID": "intro.html#was-ist-r",
    "href": "intro.html#was-ist-r",
    "title": "Introduction & Basics",
    "section": "Was ist R?",
    "text": "Was ist R?\n\nProgrammiersprache (und -oberfläche) für statistische Berechnungen und Grafiken\nopen-source & kostenlos\ngroße Community, die ständig Funktionen erweitert und bei Problemen hilft\nR Studio ist die meistgenutzte Benutzeroberfläche und macht die Handhabung einfacher"
  },
  {
    "objectID": "intro.html#user-interface",
    "href": "intro.html#user-interface",
    "title": "Introduction & Basics",
    "section": "User Interface",
    "text": "User Interface\n\n\noben links: Code\nunten links: Konsole - ausgeführter Code und Output\n\nhier lässt sich auch Code eintippen, dieser kann allerdings nicht gespeichert werden\n\noben rechts: u.a. Environment - welche Objekte und Daten exisitieren\nunten rechts: Dateizugriff, Plotanzeige, Hilfeanzeige, Package-Übersicht"
  },
  {
    "objectID": "intro.html#code-erstellen-ausführen",
    "href": "intro.html#code-erstellen-ausführen",
    "title": "Introduction & Basics",
    "section": "Code erstellen + ausführen",
    "text": "Code erstellen + ausführen\n\nÖffnet das File das ihr auf der Seite mit unseren Installationstips heruntergeladen habt (vielleicht ist es auch noch offen)\nIn dem File ist nur die Zeile ohne #ausführbarer Code. Wie hier geschehen, lassen sich mit # Kommentare in den Code hinzufügen, die dem Code Struktur geben oder erklären, was hier passieren soll\nFührt die Zeile Code aus, in dem ihr euren Cursor in die Zeile stellt und STRG+ENTER drückt\nÜber das Markieren von mehreren/allen Zeilen werden die entsprechenden Zeilen nacheinander ausgeführt\n\n\n# Erstes Skript zum Seminar Medienbasierte Wissenskonstruktion: R\n\n# Satz in Konsole printen:\nprint(paste0(\"Ich habe \",R.version$version.string,\" und RStudio v\",rstudioapi::versionInfo()$version))\n\n\n\n[1] \"Ich habe R version 4.2.1 (2022-06-23 ucrt) und RStudio v2023.3.0.386\""
  },
  {
    "objectID": "intro.html#r-vs.-quarto",
    "href": "intro.html#r-vs.-quarto",
    "title": "Introduction & Basics",
    "section": ".R vs. Quarto",
    "text": ".R vs. Quarto\n\nR’s Standardformat sind .R-Dateien\nFür “echte Arbeit” mit R sind Quarto-Dokumente meistens nützlicher:\n\nermöglichen Export/Rendern in andere Dateiformate (html, pdf, docx, pptx,…)\nermöglichen neben Code-Abschnitte auch Text-Abschnitte, die formatiert werden können (wie LaTeX)\nermöglichen einzustellen, ob nur Code, nur Output, oder Code+Output angezeigt werden soll\nAuswertung & Bericht können somit im gleichen Dokument geschehen\nDiese Präsentation ist auch in R entstanden!"
  },
  {
    "objectID": "intro.html#r-vs.-quarto-1",
    "href": "intro.html#r-vs.-quarto-1",
    "title": "Introduction & Basics",
    "section": ".R vs. Quarto",
    "text": ".R vs. Quarto"
  },
  {
    "objectID": "intro.html#quarto---dokument-aufsetzen",
    "href": "intro.html#quarto---dokument-aufsetzen",
    "title": "Introduction & Basics",
    "section": "Quarto - Dokument aufsetzen",
    "text": "Quarto - Dokument aufsetzen\n\nErstellt ein Quarto-Dokument über File > New File > Quarto Document\nIm auftauchenden Fenster könnt ihr einstellen:\n\nTitel & AutorIn des Dokuments\ngewünschtes Output-Format (hier: html)\nwomit das Quarto-Dokument gerendert werden soll (hier: knitr)"
  },
  {
    "objectID": "intro.html#quarto---im-dokument",
    "href": "intro.html#quarto---im-dokument",
    "title": "Introduction & Basics",
    "section": "Quarto - im Dokument",
    "text": "Quarto - im Dokument\n\nDer Kasten oben heißt “YAML-header” - dort sind die Rendereinstellungen (Titel, Speicherort, Format, Schriftgröße usw.)\nIm Feld darunter kann Text geschrieben und formatiert werden\n\nSource: Format lässt sich über bestimmte Zeichen einstellen\nVisual: Format lässt sich über Markieren + Auswählen einstellen\n\nCode kann man nur innerhalb von Chunks einfügen (oben rechts auf das grün unterlegte C klicken oder STRG+ALT+I)\nIm grau unterlegten Kästchen lässt sich “normaler” R-Code schreiben und ausführen\n\n“Vorschau” über den Play-Button\nÜber #| am Zeilenbeginn lässt sich einstellen, was mit Code + Output beim Rendern passieren soll\n\nÜber “Render” wird das Dokument in das Zielformat umgewandelt\nIn den meisten Fällen rendert man nur gegen Ende und nutzt während der Analyse die Chunk-Vorschau"
  },
  {
    "objectID": "intro.html#quarto",
    "href": "intro.html#quarto",
    "title": "Introduction & Basics",
    "section": "Quarto",
    "text": "Quarto\n\nErstellt zwei Code-Chunks\nFügt bei einem der Chunks in die erste Zeile #| echo: TRUE\nFügt beim anderen Chunk in die erste Zeile #| echo: FALSE\nSchreibt in den Text- und in die Code-Bereiche 4+5\nRendert das Dokument\n\nOutput ohne Code:\n\n4+5\n\n[1] 9\n\n\nOutput mit Code:\n\n4+5\n\n[1] 9\n\n\nReiner Text:\n4+5"
  },
  {
    "objectID": "intro.html#rechenoperatoren",
    "href": "intro.html#rechenoperatoren",
    "title": "Introduction & Basics",
    "section": "(Rechen)Operatoren",
    "text": "(Rechen)Operatoren\n\nDezimaltrennzeichen: Punkt . , nicht Komma ,\n+ - * /\nExponent: ^\nlogisch gleich: ==\nlogisch ungleich: !=\nlogisches und: &\nlogisches oder: |\nlogisches exklusives oder: xor\nnicht: !"
  },
  {
    "objectID": "intro.html#objekte",
    "href": "intro.html#objekte",
    "title": "Introduction & Basics",
    "section": "Objekte",
    "text": "Objekte\n\nAnstatt nur Code durchzuführen und das Ergebnis in der Konsole abzulesen, können wir auch alles mögliche als Objekte speichern\nKonzept: links mit Objektname beginnen, dann den Zuweisungsoperator und anschließend das, was als Objekt gespeichert werden soll\n\n\nergebnis <- 3+5\n\n\nR führt den Code rechts vom Pfeil aus - das Objekt ist “8”, nicht “3+5”!\nObjekte lassen sich abrufen, indem man ihren Namen schreibt und ausführt\n\n\nergebnis\n\n[1] 8\n\n\n\nR ist case-sensitive, d.h. Groß-und Kleinschreibung muss übereinstimmen!\nObjekte lassen sich auch oben rechts im Environment einsehen (wird aber irgendwann unübersichtlich…)\nErstellt selbst eine Rechenaufgabe und speichert sie als Objekt!\nWas passiert, wenn man den gleichen Objektnamen erneut verwendet?"
  },
  {
    "objectID": "intro.html#objekte-1",
    "href": "intro.html#objekte-1",
    "title": "Introduction & Basics",
    "section": "Objekte",
    "text": "Objekte\n\nWarum ist Objekte erstellen wichtig?\n\nFür R ist jeder Output sonst “flüchtig”: Es wird nicht wirklich etwas verändert\nZwischenergebnisse überprüfen\nParameter festlegen\nFlüchtigkeitsfehler vermeiden\nuvm.\n\nShortcut für <- : ALT+-\nPfeil in die andere Richtung -> und Gleichheitszeichen = funktionieren nicht bzw. sollten nicht verwendet werden!"
  },
  {
    "objectID": "intro.html#funktionen",
    "href": "intro.html#funktionen",
    "title": "Introduction & Basics",
    "section": "Funktionen",
    "text": "Funktionen\n\nMithilfe von Funktionen lassen sich Objekte/Daten bearbeiten\nBeispiel: round(3.45, digits = 1) rundet 3.45 auf die erste Nachkommastelle\n\nam Anfang steht der Name der Funktion (auch hier: case-sensitive)\ndie Argumente der Funktion werden von runden Klammern eingerahmt\nArgumente sind bspw. die Daten, auf die die Funktion angewendet werden soll, aber auch weitere Einstellungen der Funktion, z.B. auf wie viele Nachkommastellen gerundet werden soll\nsie folgen der Syntax Argument = Angabe\nArgumente werden durch Komma getrennt\nmanche Argumente sind zwingend notwendig, viele aber optional\ndie Argumente einer Funktion haben eine bestimmte Reihenfolge, daher kann man den Argumentnamen häufig weglassen, z.B. round(3.45, 1) (solange man die Funktion noch nicht gut kennt, oder auch um den Überblick zu behalten, besser die Argumente explizit nennen)"
  },
  {
    "objectID": "intro.html#hilfe-in-r",
    "href": "intro.html#hilfe-in-r",
    "title": "Introduction & Basics",
    "section": "Hilfe in R",
    "text": "Hilfe in R\n\nÜber help(Funktion) wird die Hilfeseite/Dokumentation der Funktion “Funktion” aufgerufen\n\ngenauso funktioniert ?Funktion\nauf der Hilfeseite stehen mögliche Argumente, ihre Reihenfolge und in welchem Format das Argument angegeben wird (wird eine Zahl oder ein Wort erwartet?)\n\nFindet mit der Hilfe-Funktion heraus, was mean() macht und welche Argumente es hat\n\n\n?mean\n#oder\nhelp(mean)"
  },
  {
    "objectID": "intro.html#hilfe-außerhalb-von-r",
    "href": "intro.html#hilfe-außerhalb-von-r",
    "title": "Introduction & Basics",
    "section": "Hilfe außerhalb von R",
    "text": "Hilfe außerhalb von R\n\nFehlermeldungen lesen und versuchen, Problem zu verstehen\ndas Internet!\n\nFehlermeldung bei Google einfügen und Links durchsuchen\nStack Overflow\nStatt selbst eine Frage einreichen zu müssen findet sich fast immer ein Thread mit dem gleichen/ähnlichen Problem und möglichen Lösungen\n\nDanach: Lösung für sich dokumentieren"
  },
  {
    "objectID": "intro.html#packages",
    "href": "intro.html#packages",
    "title": "Introduction & Basics",
    "section": "Packages",
    "text": "Packages\n\nFrisch nach der Installation besteht R aus Base R und einigen wenigen Erweiterungen\nDurch die Installation von weiteren Paketen/Packages lassen sich die Funktionen fast beliebig erweitern: Jedes Package enthält Funktionen und/oder Datensätze, meistens zu bestimmten Analysen, mehr oder weniger nischig\nIm Fenster unten rechts unter “Packages” könnt ihr sehen, welche Packages schon installiert und/oder aktiviert sind\nNeue Packages lassen sich mit install.packages(\"package-name\") installieren und mit library(package-name) aktivieren (auf die Anführungszeichen achten!)\nMan kann neue Funktionen auch selbst schreiben, das ist aber meistens nur in Sonderfällen notwendig"
  },
  {
    "objectID": "intro.html#packages-1",
    "href": "intro.html#packages-1",
    "title": "Introduction & Basics",
    "section": "Packages",
    "text": "Packages\n\nInstalliert und aktiviert das Package psych (Erinnerung: install.packages()& library())\nWendet die Hilfefunktion help() auf sich selbst an, um herauszufinden, wie ihr mit ihr Informationen über Packages bekommt\nSchaut euch mit dem neuen Wissen an, welche Funktionen das Package psych beinhaltet\n\n\n# Installieren\ninstall.packages(\"psych\")\n# Aktivieren\nlibrary(psych)\n\n# Mehr über help() herausfinden\nhelp(help) # Inception!\n# Aha, das Argument \"package\" kann mir weiterhelfen!\n\n# Über psych informieren\nhelp(package=psych)"
  },
  {
    "objectID": "intro.html#datentypen",
    "href": "intro.html#datentypen",
    "title": "Introduction & Basics",
    "section": "Datentypen",
    "text": "Datentypen\n\nBasic:\n\nnumeric: Zahlen (integer, double)\ncharacter: Buchstabenfolgen (strings)\nfactor: z.B. Faktorstufen eines Faktors\nlogical: TRUE, FALSE (vgl. 1 & 0)\n\nVektor: eindimensionale “Aufzählung” von Elementen\nMatrix: ein- bis zweidimensionale Aufzählung von Elementen\nArray: beliebig-dimensionale Aufzählung von Elementen\nListe: kann verschiedene Datentypen und Strukturen enthalten (z.B. Ergebnis einer ANOVA)\nDataframe, Tibble: Spezifische Formate, um Datensätze darzustellen\n\nKönnen pro Spalte unterschiedliche Datentypen beinhalten\nStandardformat, wenn man von extern Daten in R reinlädt"
  },
  {
    "objectID": "intro.html#datentypen-1",
    "href": "intro.html#datentypen-1",
    "title": "Introduction & Basics",
    "section": "Datentypen",
    "text": "Datentypen\nDer Datensatz aus der Umfrage sieht als Vorschau in der Konsole so aus:\n\n\nWarning: Paket 'tidyverse' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'ggplot2' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'tibble' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'tidyr' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'readr' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'purrr' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'dplyr' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'stringr' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'forcats' wurde unter R Version 4.2.3 erstellt\n\n\nWarning: Paket 'lubridate' wurde unter R Version 4.2.3 erstellt\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nNew names:\nRows: 4 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): sex, r_problems\ndbl  (5): ...1, id, lastpage, age, hopes_perc\nlgl  (3): r_knowledge, r_knowledge_amount, programming_knowledge\ndttm (1): submitdate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 4 × 7\n     id alter geschlecht r.vorwissen r.wieviel.vorwissen r.probleme\n  <dbl> <dbl> <fct>      <lgl>       <lgl>               <chr>     \n1     1    NA <NA>       NA          NA                  <NA>      \n2     2    12 female     TRUE        TRUE                Test      \n3     3    19 female     TRUE        TRUE                kp        \n4     4    20 male       TRUE        TRUE                problemz  \n# ℹ 1 more variable: programmier.vorwissen <lgl>\n\n\n\n\nZeile: Ein Tibble mit diesen Dimensionen wird angezeigt\n\n\nZeile: Spaltennamen\n\n\nZeile: Datentypen\n\nfolgende Zeilen: Daten\nin der Konsole werden Daten meistens nur gekürzt angezeigt\nÜber view(daten) öffnet sich eine Tabellenübersicht"
  },
  {
    "objectID": "intro.html#bestimmte-daten-abrufen",
    "href": "intro.html#bestimmte-daten-abrufen",
    "title": "Introduction & Basics",
    "section": "Bestimmte Daten abrufen",
    "text": "Bestimmte Daten abrufen\n\nIn eckigen Klammern nach dem Objektnamen lassen sich die “Koordinaten” angeben\n\n\numfrage[2,3] # zeigt den Wert der zweiten Zeile und dritten Spalte\n\n# A tibble: 1 × 1\n  geschlecht\n  <fct>     \n1 female    \n\n\n\nStatt Zahlen können auch Spaltennamen benutzt werden\n\n\numfrage[3, \"geschlecht\"]\n\n# A tibble: 1 × 1\n  geschlecht\n  <fct>     \n1 female    \n\n\n\nSpalten lassen sich auch direkt über $ ansprechen.\n\n\numfrage$geschlecht\n\n[1] <NA>   female female male  \nLevels: female male"
  },
  {
    "objectID": "intro.html#bestimmte-daten-abrufen-1",
    "href": "intro.html#bestimmte-daten-abrufen-1",
    "title": "Introduction & Basics",
    "section": "Bestimmte Daten abrufen",
    "text": "Bestimmte Daten abrufen\n\nUm alles anzeigen zu lassen, kann man die Koordinate “leer” lassen\n\n\numfrage[ , \"geschlecht\"]\n\n# A tibble: 4 × 1\n  geschlecht\n  <fct>     \n1 <NA>      \n2 female    \n3 female    \n4 male      \n\n\n\numfrage[ , ]\n\n# A tibble: 4 × 7\n     id alter geschlecht r.vorwissen r.wieviel.vorwissen r.probleme\n  <dbl> <dbl> <fct>      <lgl>       <lgl>               <chr>     \n1     1    NA <NA>       NA          NA                  <NA>      \n2     2    12 female     TRUE        TRUE                Test      \n3     3    19 female     TRUE        TRUE                kp        \n4     4    20 male       TRUE        TRUE                problemz  \n# ℹ 1 more variable: programmier.vorwissen <lgl>"
  },
  {
    "objectID": "intro.html#datentypen-2",
    "href": "intro.html#datentypen-2",
    "title": "Introduction & Basics",
    "section": "Datentypen",
    "text": "Datentypen\n\nR erkennt häufig von selbst, welcher Datentyp gemeint ist\nHäufig ist es aber sinnvoll, noch mal “von Hand” zu überprüfen und/oder den richtigen Datentyp festzulegen\ntypeof() um den Datentyp zu erfragen\nis.numeric() / is.factor() / is.logical() / is.character() um einen bestimmten Datentyp zu testen\nas.numeric() usw. nutzen, um den Datentyp zu verändern\n\n\numfrage$r.probleme <- as.factor(umfrage$r.probleme)\n# Die umgewandelte Spalte überschreibt die alte Spalte!"
  },
  {
    "objectID": "intro.html#neue-daten-erstellen",
    "href": "intro.html#neue-daten-erstellen",
    "title": "Introduction & Basics",
    "section": "Neue Daten erstellen",
    "text": "Neue Daten erstellen\n\nVektoren: c()\nKombiniert alle aufgezählten Objekte\n\n\nvektor <- c(1,3,5)\nvektor\n\n[1] 1 3 5\n\n\n\nStrings müssen in Anführungszeichen gesetzt werden, damit R sie nicht mit Objekten verwechselt.\nIm Output erkennt man Strings auch an den Anführungszeichen\n\n\nstring <- \"hallo\"\nstring\n\n[1] \"hallo\""
  },
  {
    "objectID": "intro.html#unterschiede-zu-anderen-programmiersprachen",
    "href": "intro.html#unterschiede-zu-anderen-programmiersprachen",
    "title": "Introduction & Basics",
    "section": "Unterschiede zu anderen Programmiersprachen",
    "text": "Unterschiede zu anderen Programmiersprachen\n\nkeine Unterscheidung zwischen integer und double\nverwendet keine Pointer\ntendenziell: Viele Angelegenheiten, bei denen andere Sprachen empfindlich sind, sind in R simpler aufgebaut"
  },
  {
    "objectID": "ntn.html",
    "href": "ntn.html",
    "title": "Nice to Know",
    "section": "",
    "text": "Nice to Know\nHier werden wir bald alles auflisten, was wir beim Umgang mit R wichtig finden, was aber vielleicht im Seminar selbst schwierig unterzubringen ist. Stay tuned!"
  },
  {
    "objectID": "quarto_reports.html",
    "href": "quarto_reports.html",
    "title": "Berichte in Quarto",
    "section": "",
    "text": "Wir haben nun kennengelernt, wie wir mit Hilfe statistischer Analysen Daten mit R auswerten können. Dabei haben wir die Ergebnisse direkt in R ausgewertet - im Output der jeweiligen Chunks. Doch was ist, wenn dir unsere Ergbenisse mit anderen Teilen wollen? Nicht jede:r hat R & R-Studio installiert, und kann ganz einfach unsere .qmd Dateien öffnen. Daher ist es wichtig, sich mit dem Bereitstellen von wissenschaftlichen Ergebnissen zu beschäftigen.\nZwar haben wir in der ersten Sitzung kurz kennengelernt, wie Quarto mit seinen Chunks & HTML Output funktioniert, diese Sitzung soll allerdings dazu dienen, dieses Wissen zu festigen. Dabei wollen wir verschiedene Möglichkeiten zeigen, wie wir Ergebnisse in verschiedener Art und Weise vermitteln können. Dabei ist die Art der Darbietung, auf die wir uns fokussieren html. Zwar ist der Export in .docx und .pdf ebenfalls möglich, diesen wollen wir aber euch aber nur zum Schluss kurz zeigen."
  },
  {
    "objectID": "quarto_reports.html#packages-quarto-rendering",
    "href": "quarto_reports.html#packages-quarto-rendering",
    "title": "Berichte in Quarto",
    "section": "Packages & Quarto-Rendering",
    "text": "Packages & Quarto-Rendering\nIhr solltet mittlerweise wissen, wir ihr Packages sowohl installiert als auch aktiviert. Doch was ist, wenn ihr euer Quarto Dokument für die Übergabe bspw. an eine Dozentin/Vorgesetzte rendern wollt?\n\nFrage: Wir würdet ihr dann vorgehen?\nAntwort: Das Dokument rendern!\n\nHierbei kann es passieren, dass Quarto beim Rendern bspw. folgende Fehlermeldung schmeißt:\n\nError: could not find function “%>%”\n\nEtwaige Fehlermeldungen können immer mal passieren, und zu Anfang ganz schön für Verwirrung sorgen.\n\nFrage: Was ist hier passiert?\nAntwort: WIr haben vergessen, alle Packages zu aktivieren\n\nDa wir das package dplyr, in dem der Pipe-Operator %>% vorhanden ist, nicht aktiviert haben, weiß Quarto nicht, wie es diesen interpretieren muss. Dabei ist es auch egal, ob ihr das entsprechende Package in euer laufenden R-Session bereits aktiviert habt: Wenn es um das Rendern geht, könnt ihr jedes Quarto-Dokument als in-sich geschlossenes System betrachten - daher ist es nötig, alle notwendigen Packages zu aktivieren.\n\nFrage: Was für eine Vorgehensweise bietet sich dafür an?\nAntwort: Zu Beginn ein “Info-Chunk” definieren.\n\nDaher bietet es sich an, gleich zu Beginn des Dokuments einen Basis “Konfigurations-/Packagechunk” zu erstellen, in dem ihr alle wichtigen Dinge definiert. Die Position ist relevant, da Quarto das Dokument ja von oben nach unten rendert, und somit die Packages zunächst aktiviert werden müssen, bevor deren Funktionen verwendet werden können.\nDieser könnte dann etwa so aussehen:\n\nlibrary(tidyverse)\nlibrary(rstatix)\n\npalette1 <- c(\"#648fff\",\"#785ef0\",\"#dc267f\",\"#fe6100\",\"#ffb000\",\"#000000\",\"#ffffff\")\npalette2 <- c(\"#CC79A7\",\"#D55E00\",\"#0072B2\",\"#F0E442\",\"#009E73\",\"#56B4E9\",\"#E69F00\",\"#000000\")\n\nNatürlich wollen wir nicht immer, dass das dann aber gleich auch das erste ist, was eine potenzielle Rezipientin unseres Berichts sieht.\n\nFrage: Wie können wir sicherstellen, dass dieser Chunk im finalen Bericht nicht angezeigt wird?\nAntwort: Wir können zu Beginn in den Chunk Optionen über #| einstellen, dass er nicht angezeigt (aber ausgeführt) wird.\n\nPerfekt! Wir haben nun alles was wir brauchen, ohne dass man sieht, was wir gemacht haben."
  },
  {
    "objectID": "quarto_reports.html#kurzer-exkurs-packages-explizit-machen",
    "href": "quarto_reports.html#kurzer-exkurs-packages-explizit-machen",
    "title": "Berichte in Quarto",
    "section": "Kurzer Exkurs: Packages : : explizit machen",
    "text": "Kurzer Exkurs: Packages : : explizit machen\nEs soll vorkommen, dass ein wissenschaftlicher Quarto-Bericht ganz schön lang wird, und über mehrere Wochen/Monate hinweg erstellt wird, bevor er dann gerendert wird. Da kann es sein, dass wir uns gar nicht mehr so gut erinnern können, was wir alles für Packages verwendet haben. Dagegen gibt es zwei Möglichkeiten:\nEntweder, ihr aktiviert ganz bewusst jedes mal, wenn ihr ein besonderes Package verwendet (bspw. wesanderson), dieses im jeweiligen Chunk - denn doppeltes Aktivieren von Packages (im Gegensatz zu install.packages() ist erstmal nicht weiter problematisch:\n\nlibrary(wesanderson)\n\nWarning: Paket 'wesanderson' wurde unter R Version 4.2.3 erstellt\n\niris %>% \n  ggplot()+\n  geom_density(aes(x=Sepal.Width,fill=Species)) +\n  scale_fill_manual(values=wes_palette(\"GrandBudapest1\",type=\"discrete\"))+\n  theme_classic()\n\n\n\n\nIhr könnt Packages aber auch explizit nutzen über den :: Operator. Gleicher Chunk Output wie oben kann dann ohne library() erreicht werden:\n\niris %>% \n  ggplot()+\n  geom_density(aes(x=Sepal.Width,fill=Species)) +\n  scale_fill_manual(values=wesanderson::wes_palette(\"GrandBudapest1\",type=\"discrete\"))+\n  theme_classic()"
  },
  {
    "objectID": "quarto_reports.html#chunk-options",
    "href": "quarto_reports.html#chunk-options",
    "title": "Berichte in Quarto",
    "section": "Chunk Options",
    "text": "Chunk Options\n\n#| eval\nWir wollen nun die verschiedenen Chunk-Options etwas beleuchten. Fangen wir mit dem relevantesten an, eval. Angenommen, wir wollen uns für den eben benutzten ToothGrowth Datensatz kurz die Summary anschauen, wollen diese aber nicht in unserem Bericht integrieren. Dann können wir das so tun über #| eval: false in der ersten Zeile des Chunks tun. Achtet auf die Leerstelle nach dem | ! RStudio sollte euch hier die versch. Options vorschlagen (bspw. true):\n\nsummary(ToothGrowth)\n\nIm final gerenderten Dokument sollte der obige Chunk nicht angezeigt sein.\n\n\n#| include\nJe nach Art des Berichts (simples Zeigen der Ergebnisse vs. zeigen der Ergebnisse + Code) kann es sinnvoll sein, auch den Code zu zeigen. Dabei ist es wichtig zu verstehen, dass der Code per default angezeigt wird. Wenn wir nun wollen, dass dieser für einzelne Chunks ausgeblendet wird, können wir #| include: false verwenden:\nNatürlich können wir mehere dieser Optionen kombineren (als einzelne Zeilen).\n\n\n#| output\nManchmal wollen wir auch nur den Code ausführen, ohne die Ergebnisse zu zeigen. Dabei hilft die #| output Option:\n\nsummary(ToothGrowth)\n\n\n\n#| warning\nWenn wir bspw. Packages aktivieren, kann es oft zu Warnungen kommen, welche im finalen Bericht unschön sind. Dabei kann #| warning helfen:\n\nlibrary(dplyr)\n\nManchmal kann es auch sein, dass wir Probleme dabei haben, einen Chunk auszuführen und nicht gleich wissen, wo das Problem ist. Um Dokumente trotzdem zu rendern, gibt es zwei Möglichkeiten: #| error und #| eval :\n\n\n#| error\nSollte ein error entstehen ohne den Code zu unterbrechen ist das eine gute Wahl:\n\n2+\"2\"\n\n\n\n#| eval\n\n2+\"2\"\n\nAußerdem gibt es eine Vielzahl weiterer Optionen für bspw. das Anzeigen von Figures etc. Diese könnt ihr auf posit.co einsehen."
  },
  {
    "objectID": "quarto_reports.html#results",
    "href": "quarto_reports.html#results",
    "title": "Berichte in Quarto",
    "section": "#| results",
    "text": "#| results\nIhr könnt auch entscheiden, wie die results aussehen sollen. Manchmal kann es bspw. hilfreich sein, diese so wie sie sind darzustellen. Das könnt ihr über die Option machen:\n#| results: asis"
  },
  {
    "objectID": "quarto_reports.html#tabellen",
    "href": "quarto_reports.html#tabellen",
    "title": "Berichte in Quarto",
    "section": "Tabellen",
    "text": "Tabellen\nIm Rahmen der Regression habt ihr das swiss Package kennengelernt. Üblicherweise würden wir uns die Daten zunächst anschauen bzw. wollen, dass unsere Rezipientinnen diese anschauen können. Standardmäßig rendert Quarto Tabellen allerdings etwas unschön:\n\nswiss\n\n\n\n  \n\n\n\nEine Lösung dafür sind die Optionen df-print im YAML Header. Hierfür gibt es die Optionen:\n\ndefault\nkable\ntibble\npaged\n\nDiese könnt ihr dann entsprechend oben definieren:\n\n---\ntitle: \"Berichte in Quarto\"\nauthor: \"Simon Krukowski\"\ndf-print: paged\n---\n\nAngenommen, wir wollen das gesamte Ergebnis eines t-Tests direkt berichten. Hierfür bietet sich das rstatix package an, da dieses den Output direkt als tibble speichert. Zeigen wir uns also den t-Test für ToothGrowth an:\n\nToothGrowth %>% \n  mutate(dose = as.factor(dose)) %>% \n  t_test(len ~ supp,var.equal = TRUE)\n\n\n\n  \n\n\n\n\nFrage: Probiert die verschiedenen Funktionen aus. Wie unterscheiden sich die Outputs?\n\nJe nach Option solltet ihr nun verschieden-gestylte Tabellen Output sehen. Am besten ihr benutzt die, die euch am besten gefällt."
  },
  {
    "objectID": "quarto_reports.html#in-text-variablen",
    "href": "quarto_reports.html#in-text-variablen",
    "title": "Berichte in Quarto",
    "section": "In-Text Variablen",
    "text": "In-Text Variablen\nManchmal wollt ihr im Fließtext Ergebnisse beschreiben, dabei aber flexibel bleiben und diese nicht “fest” in den Text schreiben. Auch das ist mit Quarto möglich.\nAngenommen, wir wollen den Koeffizient für Agriculture unseres eben gerechneten Regressionsmodells im Text beschreiben:\n\n# Erneut rechnen just to be safe\nswiss.lm <- lm(Fertility ~ Agriculture + Examination + Education + Catholic, swiss)\n\nÜber `r können wir (im Source Editor) in-text ausführbaren Code integrieren. Um den Wert zu bekommen, müssen wir folgendes machen:\n\nswiss.lm$coefficients['Agriculture']\n\nAgriculture \n -0.2206455 \n\n\nUnd der finale Satz könnte dann so aussehen:\n\nFür unser Regressionsmodeel ergeben sich die Koeffizienten -0.221 für Agriculture, (…)\n\nWir sehen also, dass wir auch im Text selbst Variablen anzeigen können."
  },
  {
    "objectID": "quarto_reports.html#und-latex",
    "href": "quarto_reports.html#und-latex",
    "title": "Berichte in Quarto",
    "section": "Und LaTeX?",
    "text": "Und LaTeX?\nSolltet ihr mit LaTeX bspw. über Overleaf arbeiten, so ist stargazer sogar noch besser geeignet. Die default Einstellung ist, euch den entsprechenden LaTeX geben:\n\nswiss.lm %>% stargazer::stargazer()\n\n\n% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com\n% Date and time: Do, Mai 11, 2023 - 11:09:37\n\\begin{table}[!htbp] \\centering \n  \\caption{} \n  \\label{} \n\\begin{tabular}{@{\\extracolsep{5pt}}lc} \n\\\\[-1.8ex]\\hline \n\\hline \\\\[-1.8ex] \n & \\multicolumn{1}{c}{\\textit{Dependent variable:}} \\\\ \n\\cline{2-2} \n\\\\[-1.8ex] & Fertility \\\\ \n\\hline \\\\[-1.8ex] \n Agriculture & $-$0.221$^{***}$ \\\\ \n  & (0.074) \\\\ \n  & \\\\ \n Examination & $-$0.261 \\\\ \n  & (0.274) \\\\ \n  & \\\\ \n Education & $-$0.962$^{***}$ \\\\ \n  & (0.195) \\\\ \n  & \\\\ \n Catholic & 0.124$^{***}$ \\\\ \n  & (0.037) \\\\ \n  & \\\\ \n Constant & 91.055$^{***}$ \\\\ \n  & (6.949) \\\\ \n  & \\\\ \n\\hline \\\\[-1.8ex] \nObservations & 47 \\\\ \nR$^{2}$ & 0.650 \\\\ \nAdjusted R$^{2}$ & 0.616 \\\\ \nResidual Std. Error & 7.736 (df = 42) \\\\ \nF Statistic & 19.482$^{***}$ (df = 4; 42) \\\\ \n\\hline \n\\hline \\\\[-1.8ex] \n\\textit{Note:}  & \\multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\ \n\\end{tabular} \n\\end{table}"
  }
]